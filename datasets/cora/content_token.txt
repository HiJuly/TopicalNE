several computer algorithms discovering patterns groups protein sequences use based fitting parameters statistical model group related sequences these include hidden markov model hmm algorithms multiple sequence alignment meme gibbs sampler algorithms discovering motifs these algorithms sometimes prone producing models incorrect two patterns combined the statistical model produced situation convex combination weighted average two different models this paper presents solution problem convex combinations form heuristic based using extremely low variance dirichlet mixture priors part statistical model this heuristic call megaprior heuristic increases strength ie decreases variance prior proportion size sequence dataset this causes column final model strongly resemble mean single component prior regardless size dataset we describe cause convex combination problem analyze mathematically motivate describe implementation megaprior heuristic show effectively eliminate problem convex combinations protein sequence pattern discovery
this paper describes preliminary work aims apply learning strategies medical followup study an investigation application three machine learning algorithmsr foil inducth identify risk factors govern colposuspension cure rate made the goal study induce generalised description explanation classification attribute colposuspension cure rate completely cured improved unchanged worse examples questionnaires we looked set rules described risk factors result differences cure rate the results encouraging indicate machine learning play useful role large scale medical problem solving
in cellular telephone systems important problem dynamically allocate communication resource channels maximize service stochastic caller environment this problem naturally formulated dynamic programming problem use reinforcement learning rl method find dynamic channel allocation policies better previous heuristic solutions the policies obtained perform well broad variety call traffic patterns we present results large cellular system in cellular communication systems important problem allocate communication resource bandwidth maximize service provided set mobile callers whose demand service changes stochastically a given geographical area divided mutually disjoint cells cell serves calls within boundaries see figure the total system bandwidth divided channels channel centered around frequency each channel used simultaneously different cells provided cells sufficiently separated spatially interference the minimum separation distance simultaneous reuse channel called channel reuse constraint when call requests service given cell either free channel one violate channel reuse constraint may assigned call else call blocked system happen free channel found also mobile caller crosses one cell another call handed cell entry new free channel provided call new cell if channel available call must droppeddisconnected system one objective channel allocation policy allocate available channels calls number blocked calls minimized an additional objective minimize number calls dropped handed busy cell these two objectives must weighted appropriately reflect relative importance since dropping existing calls generally undesirable blocking new calls approximately states
in paper bring techniques operations research bear problem choosing optimal actions partially observable stochastic domains we begin introducing theory markov decision processes mdps partially observable mdps pomdps we outline novel algorithm solving pomdps line show cases finitememory controller extracted solution pomdp we conclude discussion approach relates previous work complexity finding exact solutions pomdps possibilities finding approximate solutions
graphical models enhance representational power probability models qualitative characterization properties this also leads greater efficiency terms computational algorithms empower representations the increasing complexity models however quickly renders exact probabilistic calculations infeasible we propose principled framework approximating graphical models based variational methods we develop variational techniques perspective unifies expands applicability graphical models these methods allow recursive computation upper lower bounds quantities interest such bounds yield considerably information mere approximations provide inherent error metric tailoring approximations individually cases considered these desirable properties concomitant variational methods unlikely arise result deterministic stochastic approximations
realtime decision algorithms class incremental resourcebounded horvitz anytime dean algorithms evaluating influence diagrams we present test domain realtime decision algorithms results experiments several realtime decision algorithms domain the results demonstrate high performance two algorithms decisionevaluation variant incremental probabilisitic inference dambrosio variant algorithm suggested goldszmidt goldszmidt pkreduced we discuss implications experimental results explore broader applicability algorithms
speedup learning seeks improve computational efficiency problem solving experience in paper develop formal framework learning efficient problem solving random problems solutions we apply framework two different representations learned knowledge namely control rules macrooperators prove theorems identify sufficient conditions learning representation our proofs constructive accompanied learning algorithms our framework captures empirical explanationbased speedup learning unified fashion we illustrate framework implementations two domains symbolic integration eight puzzle this work integrates many strands experimental theoretical work machine learning including empirical learning control rules macrooperator learning
in previous paper sm showed finite automata could used define objective functions assessing quality alignment two sequences in paper show results using cost functions we also show extend hischbergs linear space algorithm hir setting thus generalizing result myers miller mmb
we present offline variant mistakebound model learning just like well studied online model learner offline model learn unknown concept sequence elements instance space makes guess test trials in models aim learner make mistakes possible the difference models online model set possible elements known offline model sequence elements ie identity elements well order presented known learner advance we give combinatorial characterization number mistakes offline model we apply characterization solve several natural questions arise new model first compare mistake bounds offline learner learner learning concept classes online scenario we show number mistakes online learning log n factor offline learning n length sequence in addition show offline algorithm make constant number mistakes sequence online algorithm also make constant number mistakes the second issue address effect ordering elements number mistakes offline learner it turns sequences offline learner guarantee one mistake yet permutation sequence forces err many elements we prove however gap offline mistake bounds permutations sequence nmany elements larger multiplicative factor log n present examples obtain gap
wahba wang gu klein klein introduced smoothing spline analysis variance ss anova method data exponential families based rkpack fits ss anova models gaussian data introduce grkpack collection fortran subroutines binary binomial poisson gamma data we also show calculate bayesian confidence intervals ss anova estimates
this paper presents evolutionary approach incremental approach find learning rules several supervised learning tasks in evolutionary approach potential solutions represented variable length mathematical lisp s expressions thus similar genetic programming gp employs fixed set nonproblem specific functions solve variety problems the model tested three monks parity problems the results indicate usefulness encoding schema discovering learning rules simple supervised learning problems however hard learning problems require special attention terms need larger size codings potential solutions ability generalisation testing set in order find better solutions issues hill climbing strategy incremental coding potential solutions used discovering learning rules problems it found strategy larger solutions easily coded less computational effort although better performance achieved training hard learning problems ability generalisation testing cases observed poor
the key quantity needed bayesian hypothesis testing model selection marginal likelihood model also known integrated likelihood marginal probability data in paper describe way use posterior simulation output estimate marginal likelihoods we describe basic laplacemetropolis estimator models without random effects for models random effects compound laplacemetropolis estimator introduced this estimator applied data world fertility survey shown give accurate results batching simulation output used assess uncertainty involved using compound laplacemetropolis estimator the method allows us test effects independent variables random effects model also test presence random effects
the overfit problem empirical learning utility problem explanationbased learning describe similar phenomenon degradation performance due increase amount learned knowledge plotting performance learned knowledge course learning performance response reveals common trend several learning methods modeling trend allows control system constrain amount learned knowledge achieve peak performance avoid general utility problem experiments evaluate particular empirical model trend analysis learners derive several formal models if evidence suggests general utility problem modeled using mechanisms different learning paradigms model serves unify paradigms one framework capable comparing selecting different learning methods based predicted achievable performance
hidden markov models hmms applied problems statistical modeling database searching multiple sequence alignment protein families protein domains these methods demonstrated globin family protein kinase catalytic domain efhand calcium binding motif in case parameters hmm estimated training set unaligned sequences after hmm built used obtain multiple alignment training sequences it also used search swissprot database sequences members given protein family contain given domain the hmm produces multiple alignments good quality agree closely alignments produced programs incorporate threedimensional structural information when employed discrimination tests examining closely sequences database fit globin kinase efhand hmms hmm able distinguish members families nonmembers high degree accuracy both hmm profilesearch technique used search relationships protein sequence multiply aligned sequences perform better tests prosite dictionary sites patterns proteins the hmm appears slight advantage
this paper explores effect initial weight selection feedforward networks learning simple functions backpropagation technique we first demonstrate use monte carlo techniques magnitude initial condition vector weight space significant parameter convergence time variability in order understand result additional deterministic experiments performed the results experiments demonstrate extreme sensitivity back propagation initial weight configuration
some forms memory rely temporarily system brain structures located medial temporal lobe includes hippocampus the recall recent events one task relies crucially proper functioning system as event becomes less recent medial temporal lobe becomes less critical recall event recollection appears rely upon neocortex it proposed process called consolidation responsible transfer memory medial temporal lobe neocortex we examine network model proposed p alvarez l squire designed incorporate known features consolidation propose several possible experiments intended help evaluate performance model realistic conditions finally implement extended version model accommodate varying assumptions number areas connections within brain memory capacity examine performance model alvarez squires original task
the map eye brain vertebrates topographic ie neighbouring points eye map neighbouring points brain in addition two eyes innervate target structure two sets fibres segregate form ocular dominance stripes experimental evidence frog goldfish suggests two phenomena may subserved mechanisms we present computational model addresses formation topography ocular dominance the model based form competitive learning subtractive enforcement weight normalization rule inputs model distributed patterns activity presented simultaneously eyes an important aspect model ocular dominance segregation occur two eyes positively correlated whereas previous models tended assume zero negative correlations eyes this allows investigation dependence pattern stripes degree correlation eyes find increasing correlation leads narrower stripes experiments suggested test prediction
we examine methods estimate average variance test error rates set classifiers we begin process drawing classifier random example given validation data average test error rate estimated validating single classifier given test example inputs variance computed exactly next consider process drawing classifier random using examples once expected test error rate validated validating single classifier however variance must estimated validating classifers yields loose uncertain bounds
we consider formal models learning noisy data specifically focus learning probability approximately correct model defined valiant two widely studied models noise setting classification noise malicious errors however realistic model combining two types noise formalized we define learning environment based natural combination two noise models we first show hypothesis testing possible model we next describe simple technique learning model describe powerful technique based statistical query learning we show noise tolerance improved technique roughly optimal respect desired learning accuracy provides smooth tradeoff tolerable amounts two types noise finally show statistical query simulation yields learning algorithms combinations noise models thus demonstrating statistical query specification truly an important goal research machine learning determine tasks automated determine information computation requirements one way answer questions development investigation formal models machine learning capture task learning plausible assumptions in work consider formal model learning examples called probably approximately correct pac learning defined valiant val in setting learner attempts approximate unknown target concept simply viewing positive negative examples concept an adversary chooses specified function class hidden f gvalued target function defined specified domain examples chooses probability distribution domain the goal learner output polynomial time high probability hypothesis close target function respect distribution examples the learner gains information target function distribution interacting example oracle at request learner oracle draws example randomly according hidden distribution labels according hidden target function returns labelled example learner a class functions f said pac learnable captures generic fault tolerance learning algorithm
we present decision tree based approach function approximation reinforcement learning we compare approach table lookup neural network function approximator three problems well known mountain car pole balance problems well simulated automobile race car we find decision tree provide better learning performance neural network function approximation solve large problems infeasible using table lookup
an approach develop new game playing strategies based artificial evolution neural networks presented evolution directed discover strategies othello randommoving opponent later fffi search program the networks discovered first standard positional strategy subsequently mobility strategy advanced strategy rarely seen outside tournaments the latter discovery demonstrates evolutionary neural networks develop novel solutions turning initial disadvantage advantage changed environment
we derive general bounds complexity learning statistical query model pac model classification noise we considering problem boosting accuracy weak learning algorithms fall within statistical query model this new model introduced kearns provide general framework efficient pac learning presence classification noise we first show general scheme boosting accuracy weak sq learning algorithms proving weak sq learning equivalent strong sq learning the boosting efficient used show main result first general upper bounds complexity strong sq learning specifically derive simultaneous upper bounds respect number queries olog vapnikchervonenkis dimension query space olog inverse minimum tolerance o log in addition show general upper bounds nearly optimal describing class learning problems simultaneously lower bound number queries log we apply boosting results sq model learning pac model classification noise since nearly pac learning algorithms cast sq model apply boosting techniques convert pac algorithms highly efficient sq algorithms by simulating efficient sq algorithms pac model classification noise show nearly pac algorithms converted highly efficient pac algorithms tolerate classification noise we give upper bound sample complexity noisetolerant pac algorithms nearly optimal respect noise rate we also give upper bounds space complexity hypothesis size show two measures fact independent noise rate we note running times noisetolerant pac algorithms efficient this sequence simulations also demonstrates possible boost accuracy nearly pac algorithms even presence noise this provides partial answer open problem schapire first theoretical evidence empirical result drucker schapire simard
the tremendous current effort propose neurally inspired methods computation forces closer scrutiny real world application potential models this paper categorizes applications classes particularly discusses features applications make efficiently amenable neural network methods computational machines deterministic mappings inputs outputs many computational mechanisms proposed problem solutions neural network features include parallel execution adaptive learning generalization fault tolerance often much effort given model applications already implemented much efficient way alternate technology neural networks potentially powerful devices many classes applications however proposed class applications neural networks efficient large commonly occurring nature comparison supervised unsupervised generalizing systems also included
subjectivism become dominant philosophical foundation bayesian inference yet practice bayesian analyses performed socalled noninformative priors priors constructed formal rule we review plethora techniques constructing priors discuss practical philosophical issues arise used we give special emphasis jeffreyss rules discuss evolution point view interpretation priors away unique representation ignorance toward notion chosen convention we conclude problems raised research priors chosen formal rules serious may dismissed lightly sample sizes small relative number parameters estimated dangerous put faith default solution asymptotics take jeffreyss rules variants remain reasonable choices we also provide annotated bibliography fl robert e kass professor larry wasserman associate professor department statistics carnegie mellon university pittsburgh pennsylvania the work authors supported nsf grant dms nih grant rca the authors thank nick polson helping annotations jim berger teddy seidenfeld arnold zellner useful comments discussion
recurrent neural networks become popular models system identification time series prediction narx nonlinear autoregressive models exogenous inputs neural network models popular subclass recurrent networks used many applications though embedded memory found recurrent network models particularly prominent narx models we show using intelligent memory order selection pruning good initial heuristics significantly improves generalization predictive performance nonlinear systems problems diverse grammatical inference time series prediction
this paper presents incremental concept learning approach identiflcation concepts high overall accuracy the main idea address concept overlap central problem learning multiple descriptions many traditional inductive algorithms disjunctive version space family considered face problem the approach focuses combinations confldent possibly overlapping concepts original stochastic complexity formula the focusing ecient organized simulated annealingbased beam search the experiments show approach especially suitable developing incremental learning algorithms following advantages flrst generates highly accurate concepts second overcomes certain degree sensitivity order examples third handles noisy examples
casebased reasoning cbr great deal offer supporting creative design particularly processes rely heavily previous design experience framing problem evaluating design alternatives however existing cbr systems living potential they tend adapt reuse old solutions routine ways producing robust uninspired results little research effort directed towards kinds situation assessment evaluation assimilation processes facilitate exploration ideas elaboration redefinition problems crucial creative design also typically rigid control structures facilitate kinds strategic control opportunism inherent creative reasoning in paper describe types behavior would like casebased design systems support based study designers working mechanical engineering problem we show standard cbr framework extended describe architecture developing experiment ideas
in paper present framework building probabilistic automata parameterized contextdependent probabilities gibbs distributions used model state transitions output generation parameter estimation carried using em algorithm mstep uses generalized iterative scaling procedure we discuss relations certain classes stochastic feedforward neural networks geometric interpretation parameter estimation simple example statistical language model constructed using methodology
one characteristics design designers rely extensively past experience order create new designs because memorybased techniques artificial intelligence help store organise retrieve reuse experiential knowledge held memory good candidates aiding designers another characteristic design phenomenon exploration early stages design configuration a designer begins illstructured partially defined problem specification process exploration gradually refines modifies hisher understanding problem improves in paper describe demex interactive computeraided design system employs memorybased techniques help users explore design problems pose system order acquire better understanding requirements problems demex applied domain structural design buildings
uppropagation algorithm inverting learning neural network generative models sensory input processed inverting model generates patterns hidden variables using topdown connections the inversion process iterative utilizing negative feedback loop depends error signal propagated bottomup connections the error signal also used learn generative model examples the algorithm benchmarked principal component analysis in doctrine unconscious inference helmholtz argued perceptions formed interaction bottomup sensory data topdown expectations according one interpretation doctrine perception procedure sequential hypothesis testing we propose new algorithm called uppropagation realizes interpretation layered neural networks it uses topdown connections generate hypotheses bottomup connections revise it important understand difference uppropagation ancestor backpropagation algorithm backpropagation learning algorithm recognition models as shown figure bottomup connections recognize patterns topdown connections propagate error signal used learn recognition model in contrast uppropagation algorithm inverting learning generative models shown figure b topdown connections generate patterns set hidden variables sensory input processed inverting generative model recovering hidden variables could generated sensory data this operation called either pattern recognition pattern analysis depending meaning hidden variables inversion generative model done iteratively negative feedback loop driven error signal bottomup connections the error signal also used learning connections experiments images handwritten digits
this paper demonstrates exploitation certain vision processing techniques index case base surfaces the surfaces result reinforcement learning represent optimum choice actions achieve goal anywhere state space this paper shows strong features occur interaction system environment detected early learning process such features allow system identify identical similar task solved previously retrieve relevant surface this results orders magnitude increase learning rate
combining different machine learning algorithms system produce benefits beyond either method could achieve alone this paper demonstrates genetic algorithms used conjunction lazy learning solve examples difficult class delayed reinforcement learning problems better either method alone this class class differential games includes numerous important control problems arise robotics planning game playing areas solutions differential games suggest solution strategies general class planning control problems we conducted series experiments applying three learning approacheslazy qlearning knearest neighbor knn genetic algorithmto particular differential game called pursuit game our experiments demonstrate knn great difficulty solving problem lazy version qlearning performed moderately well genetic algorithm performed even better these results motivated next step experiments hypothesized knn difficulty good examplesa common source difficulty lazy learning therefore used genetic algorithm bootstrapping method knn create system provide examples our experiments demonstrate resulting joint system learned solve pursuit games high degree accuracyoutperforming either method aloneand relatively small memory requirements
we describe hierarchical generative model viewed nonlinear generalization factor analysis implemented neural network the model uses bottomup topdown lateral connections perform bayesian perceptual inference correctly once perceptual inference performed connection strengths updated using simple learning rule requires locally available information we demon strate network learns extract sparse distributed hierarchical representations
in applications neuroevolution individual population represents complete neural network recent work sane system however demonstrated evolving individual neurons often produces efficient genetic search this paper demonstrates sane solve easy tasks quickly often stalls larger problems a hierarchical approach neuroevolution presented overcomes sanes difficulties integrating neuronlevel exploratory search networklevel exploitive search in robot arm manipulation task hierarchical approach outperforms neuronbased search networkbased search
reinforcement learning addresses problem learning select actions order maximize ones performance unknown environments to scale reinforcement learning complex realworld tasks typically studied ai one must ultimately able discover structure world order abstract away myriad details operate tractable problem spaces this paper presents skills algorithm skills discovers skills partially defined action policies arise context multiple related tasks skills collapse whole action sequences single operators they learned minimizing compactness action policies using description length argument representation empirical results simple grid navigation tasks illustrate successful discovery structure reinforcement learning
we consider generalization mistakebound model learning f gvalued functions learner must satisfy general constraint number m incorrect predictions number m incorrect predictions we describe generalpurpose optimal algorithm formulation problem we describe several applications general results involving situations learner wishes satisfy linear inequalities m m
a critical issue users markov chain monte carlo mcmc methods applications determine safe stop sampling use samples estimate characteristics distribution interest research methods computing theoretical convergence bounds holds promise future currently yielded relatively little practical use applied work consequently mcmc users address convergence problem applying diagnostic tools output produced running samplers after giving brief overview area provide expository review thirteen convergence diagnostics describing theoretical basis practical implementation we compare performance two simple models conclude methods fail detect sorts convergence failure designed identify we thus recommend combination strategies aimed evaluating accelerating mcmc sampler convergence including applying diagnostic procedures small number parallel chains monitoring autocorrelations crosscorrelations modifying parameterizations sampling algorithms appropriately we emphasize however possible say certainty finite sample mcmc algorithm representative underlying stationary distribution mary kathryn cowles assistant professor biostatistics harvard school public health boston ma bradley p carlin associate professor division biostatistics school public health university minnesota minneapolis mn much work done first author graduate student divison biostatistics university minnesota assistant professor biostatistics section department preventive societal medicine university nebraska medical center omaha ne the work authors supported part national institute allergy infectious diseases first award rai the authors thank developers diagnostics studied sharing insights experiences software drs thomas louis luke tierney helpful discussions suggestions greatly improved manuscript
although detection invariant structure given set input patterns vital many recognition tasks connectionist learning rules tend focus directions high variance principal components the prediction paradigm often used reconcile dichotomy suggest direct approach invariant learning based antihebbian learning rule an unsupervised twolayer network implementing method competitive setting learns extract coherent depth information randomdot stereograms
instancebased learning methods explicitly remember data receive they usually training phase prediction time perform computation then take query search database similar datapoints build online local model local average local regression predict output value in paper review advantages instance based methods autonomous systems also note ensuing cost hopelessly slow computation database grows large we present evaluate new way structuring database new algorithm accessing maintains advantages instancebased learning earlier attempts combat cost instancebased learning sacrificed explicit retention data applicable instancebased predictions based small number near neighbors reintroduce explicit training phase form interpolative data structure our approach builds multiresolution data structure summarize database experiences resolutions interest simultaneously this permits us query database exibility conventional linear search greatly reduced computational cost
discrete bayesian models used model uncertainty mobilerobot navigation question actions chosen remains largely unexplored this paper presents optimal solution problem formulated partially observable markov decision process since solving optimal control policy intractable general goes explore variety heuristic control strategies the control strategies compared experimentally simulation runs robot
this nrl ncarai technical note aic describes work salzbergs nge i recently implemented algorithm run case studies the purpose note publicize implementation note curious result using this implementation nge available www address
technical report no department statistics university toronto abstract i present new markov chain sampling method appropriate distributions isolated modes like recentlydeveloped method simulated tempering tempered transition method uses series distributions interpolate distribution interest distribution sampling easier the new method advantage require approximate values normalizing constants distributions needed simulated tempering tedious estimate simulated tempering performs random walk along series distributions used in contrast tempered transitions new method move systematically desired distribution easilysampled distribution back desired distribution this systematic movement avoids inefficiency random walk advantage unfortunately cancelled increase number interpolating distributions required because sampling efficiency tempered transition method simple problems similar simulated tempering on complex distributions however simulated tempering tempered transitions may perform differently which better depends ways interpolating distributions deceptive
we describe ongoing project develop adaptive training system ats dynamically models students learning processes provide specialized tutoring adapted students knowledge state learning style the student modeling component ats mlmodeler uses machine learning ml techniques emulate students novicetoexpert transition mlmodeler infers learning methods student used reach current knowledge state comparing students solution trace expert solution generating plausible hypotheses misconceptions errors student made a casebased approach used generate hypotheses incorrectly applying analogy overgeneralization overspecialization the student expert models use networkbased representation includes abstract concepts relationships well strategies problem solving fuzzy methods used represent uncertainty student model this paper describes design ats mlmodeler gives detailed example system would model tutor student typical session the domain use example highschool level chemistry
metacognition addresses issues knowledge cognition regulating cognition we argue regulation process improved growing experience therefore mental models needed facilitate reuse previous regulation processes we satisfy requirement describing casebased approach introspection planning utilises previous experience obtained reasoning metalevel object level the introspection plans used approach support various metacognitive tasks identified generation selfquestions as example introspection planning metacognitive behaviour system iulian described
graphical markov models use graphs either undirected directed mixed represent possible dependences among statistical variables applications undirected graphs udgs include models spatial dependence image analysis acyclic directed graphs adgs especially convenient statistical analysis arise fields genetics psychometrics models expert systems bayesian belief networks lauritzen wermuth frydenberg lwf introduced markov property chain graphs mixed graphs used represent simultaneously causal associative dependencies include udgs adgs special cases in paper alternative markov property amp chain graphs introduced ways direct extension adg markov property lwf property chain graph
the fault hierarchy representation widely used expert systems diagnosis complex mechanical devices this paper describes theory revision algorithm revises fault hierarchies this task presents several challenges typical training instances missing feature values pattern missing features significant rather merely effect noise quality candidate theory depends correctness diagnoses returns set tests uses reach diagnoses this paper describes addresses challenges reports experiments use improve performance two fielded diagnostic systems fl this extended version paper appeared proceedings fifth international workshop principles diagnosis dx new york october we gratefully acknowledge receiving helpful comments cheoungnam lee glenn meredith chandra mouleeswaran z current address robotics laboratory computer science department stanford university stanford ca email langleyflamingostanfordedu phone fax
machine learning game strategies often depended competitive methods continually develop new strategies capable defeating previous ones we use inclusive definition game consider framework within competitive algorithm makes repeated use strategy learning component learn strategies defeat given set opponents we describe game learning terms sets h x first second player strategies connect model familiar models concept learning we show importance ideas teaching set specification number k new context the performance several competitive algorithms investigated using worstcase randomized strategy learning algorithms our central result theorem competitive algorithm solves games total number strategies polynomial lgjhj lgjx j k its use demonstrated including application concept learning new kind counterexample oracle we conclude complexity analysis game learning list number new questions arising work
most work attempts give bounds generalization error hypothesis generated learning algorithm based methods theory uniform convergence these bounds apriori bounds hold distribution examples calculated data observed in paper propose different approach bounding generalization error data observed a selfbounding learning algorithm algorithm addition hypothesis outputs outputs reliable upper bound generalization error hypothesis we first explore idea statistical query learning framework kearns after give explicit self bounding algorithm learning algorithms based local search
in paper propose new framework studying markov decision processes mdps based ideas statistical mechanics the goal learning mdps find policy yields maximum expected return time in choosing policies agents must therefore weigh prospects shortterm versus longterm gains we study simple mdp agent must constantly decide exploratory jumps local reward mining state space the number policies choose grows exponentially size state space n we view expected returns defining energy landscape policy space methods statistical mechanics used analyze landscape thermodynamic limit n we calculate overall distribution expected returns well distribution returns policies fixed hamming distance optimal one we briefly discuss problem learning optimal policies empirical estimates expected return as first step relate findings entropy limit hightemperature learning numerical simulations support theoretical results
this paper shows neural networks use continuous activation functions vc dimension least large square number weights w this result settles longstanding open question namely whether wellknown ow log w bound known hardthreshold nets also held general sigmoidal nets implications number samples needed valid generalization discussed
novel online learning algorithms self adaptive learning rates parameters blind separation signals proposed the main motivation development new learning rules improve convergence speed reduce crosstalking especially nonstationary signals furthermore discovered conditions proposed neural network models associated learning algorithms exhibit random switch attention ie ability chaotic random switching crossover output signals way specified separated signal may appear various outputs different time windows validity performance dynamic properties proposed learning algorithms investigated computer simulation experiments
i present modular network architecture learning algorithm based incremental dynamic programming allows single learning agent learn solve multiple markovian decision tasks mdts significant transfer learning across tasks i consider class mdts called composite tasks formed temporally concatenating number simpler elemental mdts the architecture trained set composite elemental mdts the temporal structure composite task assumed unknown architecture learns produce temporal decomposition it shown certain conditions solution composite mdt constructed computationally inexpensive modifications solutions constituent elemental mdts
scientists engineers face recurring problems constructing testing modifying numerical simulation programs the process coding revising simulators extremely timeconsuming almost always written conventional programming languages scientists engineers therefore benefit software facilitates construction programs simulating physical systems our research adapts methodology deductive program synthesis problem constructing numerical simulation codes we focused simulators represented second order functional programs composed numerical integration root extraction routines we developed system uses first order horn logic synthesize numerical simulators built components our approach based two ideas first axiomatize relationship integration differentiation we neither attempt require complete axiomatization mathematical analysis second system uses representation functions reified objects function objects encoded lambda expressions our knowledge base includes axiomatization term equality lambda calculus it also includes axioms defining semantics numerical integration root extraction routines we use depth bounded sld resolution construct proofs synthesize programs our system successfully constructed numerical simulators computational design jet engine nozzles sailing yachts among others our results demonstrate deductive synthesis techniques used construct numerical simulation programs realistic applications ellman murata automatic design optimization highly sensitive problem formulation the choice objective function constraints design parameters dramatically impact computational cost optimization quality resulting design the best formulation varies one application another a design engineer usually know best formulation advance in order address problem developed system supports interactive formulation testing reformulation design optimization strategies our system includes executable dataflow language representing optimization strategies the language allows engineer define multiple stages optimization using different approximations objective constraints different abstractions design space we also developed set transformations reformulate strategies represented language the transformations approximate objective constraint functions abstract reparameterize search spaces divide optimization process multiple stages the system applicable principle design problem expressed terms constrained op
bayesian networks provide language qualitatively representing conditional independence properties distribution this allows natural compact representation distribution eases knowledge acquisition supports effective inference algorithms it wellknown however certain independencies capture qualitatively within bayesian network structure independencies hold certain contexts ie given specific assignment values certain variables in paper propose formal notion contextspecific independence csi based regularities conditional probability tables cpts node we present technique analogous based dseparation determining independence holds given network we focus particular qualitative representation schemetreestructured cpts capturing csi we suggest ways representation used support effective inference algorithms in particular present structural decomposition resulting network improve performance clustering algorithms alternative algorithm based cutset conditioning
the eligibility trace one basic mechanisms used reinforcement learning handle delayed reward in paper introduce new kind eligibility trace replacing trace analyze theoretically show results faster reliable learning conventional trace both kinds trace assign credit prior events according recently occurred conventional trace gives greater credit repeated events our analysis conventional replacetrace versions oine td algorithm applied undiscounted absorbing markov chains first show methods converge repeated presentations training set predictions two well known monte carlo methods we analyze relative efficiency two monte carlo methods we show method corresponding conventional td biased whereas method corresponding replacetrace td unbiased in addition show method corresponding replacing traces closely related maximum likelihood solution tasks mean squared error always lower long run computational results confirm analyses show applicable generally in particular show replacing traces significantly improve performance reduce parameter sensitivity mountaincar task full reinforcementlearning problem continuous state space using featurebased function approximator
reading studied decades variety cognitive disciplines yet theories exist sufficiently describe explain people accomplish complete task reading realworld texts in particular type knowledge intensive reading known creative reading largely ignored past research we argue creative reading aspect practically reading experiences result theory overlooks insufficient we built results psychology artificial intelligence education order produce functional theory complete reading process the overall framework describes set tasks necessary reading performed within framework developed theory creative reading the theory implemented isaac integrated story analysis and creativity system reading system reads science fiction stories
we study problem combining updates special instance theory change counterfactual conditionals propositional knowledgebases intuitively update means world described knowledgebase changed this opposed revisions another instance theory change knowledge static world changes a counterfactual implication statement form if a case b would also case negation a may derivable current knowledge we present decidable logic called vcu update counterfactual implication connectives object language our update operator generalization operators previously proposed studied literature we show operator satisfies certain postulates set forth reasonable update the logic vcu extension d k lewis logic vcu counterfactual conditionals the semantics vcu multimodal propositional calculus based possible worlds the infamous ramsey rule becomes derivation rule sound complete axiomatization we show gardenfors triviality theorem impossibility combine theory change counterfactual conditionals via ramsey rule hold logic it thus seen triviality theorem applies revision operators updates fl a preliminary version paper presented second international conference principles knowledge representation reasoning cambridge massachusetts april the work partially performed author visiting department computer science university toronto
many neural net learning algorithms aim finding simple nets explain training data the expectation simpler networks better generalization test data occams razor previous implementations however use measures simplicity lack power universality elegance based kolmogorov complexity solomonoffs algorithmic probability likewise previous approaches especially bayesian kind suffer problem choosing appropriate priors this paper addresses issues it first reviews basic concepts algorithmic complexity theory relevant machine learning solomonofflevin distribution universal prior deals prior problem the universal prior leads probabilistic method finding algorithmically simple problem solutions high generalization capability the method based levin complexity timebounded generalization kolmogorov complexity inspired levins optimal universal search algorithm for given problem solution candidates computed efficient selfsizing programs influence runtime storage size the probabilistic search algorithm finds good programs ones quickly computing algorithmically probable solutions fitting training data simulations focus task discovering algorithmically simple neural networks low kolmogorov complexity high generalization capability it demonstrated method least certain toy problems computationally feasible lead generalization results unmatchable previous neural net algorithms much remains done however make large scale applications incremental learning feasible
we studied problem generating expressive musical performances context tenor saxophone interpretations we done several recordings tenor sax playing different jazz ballads different degrees expressiveness including inexpressive interpretation ballad these recordings analyzed using sms spectral modeling techniques extract information related several expressive parameters this set parameters scores constitute set cases examples casebased system from set cases system infers set possible expressive transformations given new phrase applying similarity criteria based background musical knowledge new phrase set cases finally saxex applies inferred expressive transformations new phrase using synthesis capabilities sms
one surprising recurring phenomena observed experiments boosting test error generated classifier usually increase size becomes large often observed decrease even training error reaches zero in paper show phenomenon related distribution margins training examples respect generated voting classification rule margin example simply difference number correct votes maximum number votes received incorrect label we show techniques used analysis vapniks support vector classifiers neural networks small weights applied voting methods relate margin distribution test error we also show theoretically experimentally boosting especially effective increasing margins training examples finally compare explanation based biasvariance decomposition
realworld learning tasks may involve highdimensional data sets arbitrary patterns missing data in paper present framework based maximum likelihood density estimation learning data sets we use mixture models density estimates make two distinct appeals expectationmaximization em principle dempster et al deriving learning algorithmem used estimation mixture components coping missing data the resulting algorithm applicable wide range supervised well unsupervised learning problems results classification benchmarkthe iris data setare presented
the hierarchical feature map system recognizes input story instance particular script classifying three levels scripts tracks role bindings the recognition taxonomy ie breakdown script tracks roles extracted automatically independently script examples script instantiations unsupervised selforganizing process the process resembles human learning differentiation frequently encountered scripts become gradually detailed the resulting structure hierachical pyramid feature maps the hierarchy visualizes taxonomy maps lay topology level the number input lines selforganization time considerably reduced compared ordinary singlelevel feature mapping the system recognize incomplete stories recover missing events the taxonomy also serves memory organization scriptbased episodic memory the maps assign unique memory location script instantiation the salient parts input data separated resources concentrated representing accurately
it shown static neural approaches adaptive target detection replaced efficient sequential alternative the latter inspired observation biological systems employ sequential eyemovements pattern recognition a system described builds adaptive model timevarying inputs artificial fovea controlled adaptive neural controller the controller uses adaptive model learning sequential generation fovea trajectories causing fovea move target visual scene the system also learns track moving targets no teacher provides desired activations eyemuscles various times the goal information shape target since task rewardonlyatgoal task involves complex temporal credit assignment problem some implications adaptive attentive systems general discussed
we present treestructured architecture supervised learning the statistical model underlying architecture hierarchical mixture model mixture coefficients mixture components generalized linear models glims learning treated maximum likelihood problem particular present expectationmaximization em algorithm adjusting parameters architecture we also develop online learning algorithm parameters updated incrementally comparative simulation results presented robot dynamics domain we want thank geoffrey hinton tony robinson mitsuo kawato daniel wolpert helpful comments manuscript this project supported part grant mcdonnellpew foundation grant atr human information processing research laboratories grant siemens corporation grant iri national science foundation grant nj office naval research the project also supported nsf grant asc support center biological computational learning mit including funds provided darpa hpcc program nsf grant ecs support initiative intelligent control mit michael i jordan nsf presidential young investigator
the em algorithm performs maximum likelihood estimation data variables unobserved we present function resembles negative free energy show m step maximizes function respect model parameters e step maximizes respect distribution unobserved variables from perspective easy justify incremental variant em algorithm distribution one unobserved variables recalculated e step this variant shown empirically give faster convergence mixture estimation problem a variant algorithm exploits sparse conditional distributions also described wide range variant algorithms also seen possible
a network wilsoncowan oscillators constructed emergent properties synchronization desynchronization investigated computer simulation formal analysis the network twodimensional matrix oscillator coupled neighbors we show analytically chain locally coupled oscillators piecewise linear approximation wilsoncowan oscillator synchronizes present technique rapidly entrain finite numbers oscillators the coupling strengths change fast time scale based hebbian rule a global separator introduced receives input sends feedback oscillator matrix the global separator used desynchronize different oscillator groups unlike many models properties network emerge local connections preserve spatial relationships among components critical encoding gestalt principles feature grouping the ability synchronize desynchronize oscillator groups within network offers promising approach pattern segmentation figureground segregation based oscillatory correlation
in paper i describe implementation probabilistic regression model bugs bugs program carries bayesian inference statistical problems using simulation technique known gibbs sampling it possible implement surprisingly complex regression models environment i demonstrate simultaneous inference interpolant inputdependent noise level
classifying hand complex data coming psychology experiments long difficult task quantity data classify amount training may require one way alleviate problem use machine learning techniques we built classifier based decision trees reproduces classifying process used two humans sample data learns classify unseen data the automatic classifier proved accurate constant much faster classification hand
the estimation training methods neural network literature usually simple form gradient descent algorithm suitable implementation hardware using massively parallel computations for ordinary computers massively parallel optimization algorithms several sas procedures usually far efficient this talk shows fit neural networks using sasor r fl sasets r fl sasstat r fl software
selecting right reference class right interval faced conflicting candidates possibility establishing subset style dominance problem kyburgs evidential probability system various methods proposed loui kyburg solve problem way intuitively appealing justifiable within kyburgs framework the scheme proposed paper leads stronger statistical assertions without sacrificing much intuitive appeal kyburgs latest proposal
we apply reinforcement learning methods learn domainspecific heuristics job shop scheduling a repairbased scheduler starts criticalpath schedule incrementally repairs constraint violations goal finding short conflictfree schedule the temporal difference algorithm t d applied train neural network learn heuristic evaluation function states this evaluation function used onestep lookahead search procedure find good solutions new scheduling problems we evaluate approach synthetic problems problems nasa space shuttle payload processing task the evaluation function trained problems involving small number jobs tested larger problems the td scheduler performs better best known existing algorithm taskzwebens iterative repair method based simulated annealing the results suggest reinforcement learning provide new method constructing highperformance scheduling systems
a neural network approach classic inverted pendulum task presented this task task keeping rigid pole hinged cart free fall plane roughly vertical orientation moving cart horizontally plane keeping cart within maximum distance starting position this task constitutes difficult control problem parameters cartpole system known precisely variable it also forms basis even complex controllearning problem controller must learn proper actions successfully balancing pole given current state system failure signal pole angle vertical becomes great cart exceeds one boundaries placed position the approach presented demonstrated effective realtime control small selfcontained minirobot specially outfitted task origins details learning scheme specifics minirobot hardware results actual learning trials presented
platts resourceallocation network ran platt b modified reinforcementlearning paradigm restart existing hidden units rather adding new units after restarting units continue learn via backpropagation the resulting restart algorithm tested qlearning network learns solve inverted pendulum problem solutions found faster average restart algorithm without
we propose new processing paradigm called expandable split window esw paradigm exploiting finegrain parallelism this paradigm considers window instructions possibly dependencies single unit exploits finegrain parallelism overlapping execution multiple windows the basic idea connect multiple sequential processors decoupled decentralized manner achieve overall multiple issue this processing paradigm shares number properties restricted dataflow machines derived sequential von neumann architecture we also present implementation expandable split window execution model preliminary performance results
algorithms based nested generalized exemplar nge theory salzberg classify new data points computing distance nearest generalized exemplar ie either point axisparallel rectangle they combine distancebased character nearest neighbor nn classifiers axisparallel rectangle representation employed many rulelearning systems an implementation nge compared knearest neighbor knn algorithm domains found significantly inferior knn several modifications nge studied understand cause poor performance these show performance substantially improved preventing nge creating overlapping rectangles still allowing complete nesting rectangles performance improved modifying distance metric allow weights features salzberg best results obtained study weights computed using mutual information features output class the best version nge developed batch algorithm bnge fw mi usertunable parameters bnge fw mi performance comparable firstnearest neighbor algorithm also incorporating feature weights however knearest neighbor algorithm still significantly superior bnge fw mi domains inferior we conclude even improvements nge approach sensitive shape decision boundaries classification problems in domains decision boundaries axisparallel nge approach produce excellent generalization interpretable hypotheses in domains tested nge algorithms require much less memory store generalized exemplars required nn algorithms
selecting good model set input points cross validation computationally intensive process especially number possible models number training points high techniques gradient descent helpful searching space models problems local minima importantly lack distance metric various models reduce applicability search methods hoeffding races technique finding good model data quickly discarding bad models concentrating computational effort differentiating better ones this paper focuses special case leaveoneout cross validation applied memorybased learning algorithms also argue applicable class model selection problems
in many learning problems learning system presented values features actually irrelevant concept trying learn the focus algorithm due almuallim dietterich performs explicit search smallest possible input feature set s permits consistent mapping features s output feature the focus algorithm also seen algorithm learning determinations functional dependencies suggested another algorithm learning determinations appears the focus algorithm superpolynomial runtime almuallim dietterich leave open question tractability underlying problem in paper problem shown npcomplete we also describe briefly experiments demonstrate benefits determination learning show finding lowestcardinality determinations easier practice finding minimal determi define minfeatures problem follows given set x examples composed binary value specifying value target feature vector binary values specifying values features number n determine whether exists feature set s we show minfeatures npcomplete reducing vertexcover minfeatures the vertexcover problem may stated question given graph g vertices v edges e subset v v size edge e connected least one vertex v we may reduce instance vertexcover instance minfeatures mapping edge e example x one input feature every vertex v in proof reported result reduction set covering the proof therefore fails show npcompleteness nations
an unsupervised learning algorithm multilayer network stochastic neurons described bottomup recognition connections convert input representations successive hidden layers topdown generative connections reconstruct representation one layer representation layer in wake phase neurons driven recognition connections generative connections adapted increase probability would reconstruct correct activity vector layer in sleep phase neurons driven generative connections recognition connections adapted increase probability would produce supervised learning algorithms multilayer neural networks face two problems they require teacher specify desired output network require method communicating error information connections the wakesleep algorithm avoids problems when external teaching signal matched goal required force hidden units extract underlying structure in wakesleep algorithm goal learn representations economical describe allow input reconstructed accurately we quantify goal imagining communication game vector raw sensory inputs communicated receiver first sending hidden representation sending difference input vector topdown reconstruction hidden representation the aim learning minimize description length total number bits would required communicate input vectors way no communication actually takes place minimizing description length would required forces network learn economical representations capture underlying regularities data correct activity vector layer
properly structured software libraries crucial success software reuse specifically structure software library ought reect functional similarity stored software components order facilitate retrieval process we propose application artificial neural network technology achieve structured library in detail utilize artificial neural network adhering unsupervised learning paradigm the distinctive feature model make semantic relationship stored software components geographically explicit thus actual user software library gets notion semantic relationship components terms geographical closeness
learning fundamental component intelligence key consideration designing cognitive architectures soar laird et al this chapter considers question constitutes appropriate generalpurpose learning mechanism we interested mechanisms might explain reproduce rich variety learning capabilities humans ranging learning perceptualmotor skills ride bicycle learning highly cognitive tasks play chess research learning fields cognitive science artificial intelligence neurobiology statistics led identification two distinct classes learning methods inductive analytic inductive methods neural network backpropagation learn general laws finding statistical correlations regularities among large set training examples in contrast analytical methods explanationbased learning acquire general laws many fewer training examples they rely instead prior knowledge analyze individual training examples detail use analysis distinguish relevant example features irrelevant the question considered chapter best combine inductive analytical learning architecture seeks cover range learning exhibited intelligent systems humans we present specific learning mechanism explanation based neural network learning ebnn blends two types learning present experimental results demonstrating ability learn control strategies mobile robot using
simulation plays important role stochastic geometry related fields simplest random set models tend intractable analysis many simulation algorithms deliver approximate samples random set models example simulating equilibrium distribution markov chain spatial birthanddeath process the samples usually fail exact algorithm simulates markov chain long finite time thus convergence equilibrium approximate the seminal work propp wilson made important contribution simulation proposing coupling method coupling past cftp delivers perfect say exact simulations markov chains in paper introduce new idea perfect simulation illustrate using two common models stochastic geometry dead leaves model boolean model conditioned cover finite set points
in recent years casebased reasoning demonstrated highly useful problem solving complex domains also mixed paradigm approaches emerged combining cbr induction techniques aiming verifying knowledge andor building efficient case memory however complex domains induction whole problem space often possible time consuming in paper approach presented owing close interaction cbr part attempts induce rules particular context ie problem solved cbroriented system these rules may used indexing purposes similarity assessment order support cbr process future
this paper demonstrates tandem use finite automata learning algorithm utility planner adversarial robotic domain for many applications robot agents need predict movement objects environment plan avoid when robot reasoning model object machine learning techniques used generate one in project learn dfa model adversarial robot use automaton predict next move adversary the robot agent plans path avoid adversary predicted location fulfilling goal requirements
claudia cargnoni dipartimento statistico universita di firenze firenze italy peter muller assistant professor mike west professor institute statistics decision sciences duke university durham nc research cargnoni performed visiting isds muller west partially supported nsf grant dms
our theoretical understanding properties genetic algorithms gas used function optimization gafos strong would like traditional schema analysis provides first order insights doesnt capture nonlinear dynamics ga search process well markov chain theory used primarily steady state analysis gas in paper explore use transient markov chain analysis model understand behavior finite population gafos observed transition steady states this approach appears provide new insights circumstances gafos perform well some preliminary results presented initial evaluation merits approach provided
in paper consider application training noise multilayer perceptron input variables relevance determination noise injection modified order penalize irrelevant features the proposed algorithm attractive requires tuning single parameter this parameter controls penalization inputs together complexity model after presentation method experimental evidences given simulated data sets
coins technical report january abstract in paper present new multivariate decision tree algorithm lmdt combines linear machines decision trees lmdt constructs test decision tree training linear machine eliminating irrelevant noisy variables controlled manner to examine lmdts ability find good generalizations present results variety domains we compare lmdt empirically univariate decision tree algorithm observe multivariate tests appropriate bias given data set lmdt finds small accurate trees
reinforcement learning rl modelfree tuning adaptation method control dynamic systems contrary supervised learning based usually gradient descent techniques rl require model sensitivity function process hence rl applied systems poorly understood uncertain nonlinear reasons untractable conventional methods in reinforcement learning overall controller performance evaluated scalar measure called reinforcement depending type control task reinforcement may represent evaluation recent control action often entire sequence past control moves in latter case rl system learns predict outcome individual control action this prediction used adjust parameters controller the mathematical background rl closely related optimal control dynamic programming this paper gives comprehensive overview rl methods presents application attitude control satellite some well known applications literature reviewed well
a biologically motivated mechanism selforganizing neural network modifiable lateral connections presented the weight modification rules purely activitydependent unsupervised local the lateral interaction weights initially random develop mexican hat shape around neuron at time external input weights selforganize form topological map input space the algorithm demonstrates selforganization bootstrap using input information predictions algorithm agree well experimental observations development lateral connections cortical feature maps
some main users statistical methods economists social scientists epidemiologists discovering fields rest statistical causal foundations the blurring foundations years follows lack mathematical notation capable distinguishing causal equational relationships by providing formal natural explication relations graphical methods potential revolutionize statistics used knowledgerich applications statisticians response beginning realize causality metaphysical deadend meaningful concept clear mathematical underpinning the paper surveys developments outlines future challenges
this paper describes new method inducing logic programs examples attempts integrate best aspects existing ilp methods single coherent framework in particular combines bottomup method similar golem topdown method similar foil it also includes method predicate invention similar champ elegant solution noisy oracle problem allows system learn recursive programs without requiring complete set positive examples systematic experimental comparisons golem foil range problems used clearly demonstrate advantages approach
we present deterministic techniques computing upper lower bounds marginal probabilities sigmoid noisyor networks these techniques become useful size network clique size precludes exact computations we illustrate tightness bounds numerical experi ments
mit computational cognitive science technical report abstract we develop recursive nodeelimination formalism efficiently approximating large probabilistic networks no constraints set network topologies yet formalism straightforwardly integrated exact methods whenever arebecome applicable the approximations use controlled maintain consistently upper lower bounds desired quantities times we show boltzmann machines sigmoid belief networks combination ie chain graphs handled within framework the accuracy methods verified exper imentally
we prove lower bound ln ffi vcdimc number random examples required distributionfree learning concept class c vcdimc vapnikchervonenkis dimension ffi accuracy confidence parameters this improves previous best lower bound ln ffi vcdimc comes close known general upper bound o ffi vcdimc ln consistent algorithms we show many interesting concept classes including kcnf kdnf bound actually tight within constant factor
the ability handle temporal variation important dealing realworld dynamic signals in many applications inputs come fixedrate sequences rather signals time scales vary one instance next thus modeling dynamic signals requires ability recognize sequences also ability handle temporal changes signal this paper discusses tau net neural network modeling dynamic signals application speech in tau net sequence learning accomplished using combination prediction recurrence timedelay connections temporal variability modeled adaptable time constants network adjusted respect prediction error adapting time constants changes time scale network adapted value networks time constant provides measure temporal variation signal tau net applied several simple signals sets sine waves differing frequency phase multidimensional signal representing walking gait children energy contour simple speech utterance tau net also shown work voicing distinction task using synthetic speech data in paper tau net applied two speakerindependent tasks vowel recognition faeiyuxg consonant recognition fptkg using speech data taken timit database it shown tau nets trained mediumrate tokens achieved performance networks without time constants trained tokens rates performed better networks without time constants trained mediumrate tokens our results demonstrate tau nets ability identify vowels consonants variable speech rates extrapolating rates represented training set
backpropagation learning bp known serious limitations generalising knowledge certain types learning material bpsom extension bp overcomes limitations bpsom combination multilayered feedforward network mfn trained bp kohonens selforganising maps soms in earlier reports shown bpsom improved generalisation performance whereas decreased simultaneously number necessary hidden units without loss generalisation performance these two effects use som learning training mfns in paper focus two additional effects first show bpsom training activations hidden units mfns tend oscillate among limited number discrete values second identify som elements adequate organisers instances task hand we visualise effects argue lead intelligible neural networks employed basis automatic rule extraction
the discrimination powers multilayer perceptron mlp learning vector quantisation lvq networks compared overlapping gaussian distributions it shown analytically monte carlo studies mlp network handles high dimensional problems efficient way lvq this mainly due sigmoidal form mlp transfer function also fact mlp uses hyperplanes efficiently both algorithms equally robust limited training sets learning curves fall like m m training set size compared theoretical predictions statistical estimates vapnikchervonenkis bounds
in article approximate rate convergence gibbs sampler normal approximation target distribution based approximation consider many implementational issues gibbs sampler eg updating strategy parameterization blocking we give theoretical results justify approximation illustrate methods number realistic examples
instancebased learning methods explicitly remember data receive they usually training phase prediction time perform computation then take query search database similar datapoints build online local model local average local regression predict output value in paper review advantages instance based methods autonomous systems also note ensuing cost hopelessly slow computation database grows large we present evaluate new way structuring database new algorithm accessing maintains advantages instancebased learning earlier attempts combat cost instancebased learning sacrificed explicit retention data applicable instancebased predictions based small number near neighbors reintroduce explicit training phase form interpolative data structure our approach builds multiresolution data structure summarize database experiences resolutions interest simultaneously this permits us query database exibility conventional linear search greatly reduced computational cost
we implemented reinforcement learning architecture reactive component two layer control system simulated race car we found separating layers expedited gradually improving competition multagent interaction we ran experiments test tuning decomposition coordination low level behaviors we extended control system allow passing cars tested ability avoid collisions the best design used reinforcement learning separate networks behavior coarse coded input simple rule based coordination mechanism
this study concerned whether possible detect information contained training data background knowledge relevant solving learning problem whether irrelevant information eliminated preprocessing starting learning process a case study data preprocessing hybrid genetic algorithm shows elimination irrelevant features substantially improve efficiency learning in addition costsensitive feature elimination effective reducing costs induced hypotheses
hierarchical genetic programming hgp approaches rely discovery modification use new functions accelerate evolution this paper provides qualitative explanation improved behavior hgp based analysis evolution process dual perspective diversity causality from static point view use hgp approach enables manipulation population higher diversity programs higher diversity increases exploratory ability genetic search process demonstrated theoretical experimental fitness distributions expanded structural complexity individuals from dynamic point view analysis causality crossover operator suggests hgp discovers exploits useful structures bottomup hierarchical manner diversity causality complementary affecting exploration exploitation genetic search unlike machine learning techniques need extra machinery control tradeoff hgp automatically trades exploration exploitation
previous neural network learning algorithms sequence processing computationally expensive perform poorly comes long time lags this paper first introduces simple principle reducing descriptions event sequences without loss information a consequence principle unexpected inputs relevant this insight leads construction neural architectures learn divide conquer recursively decomposing sequences i describe two architectures the first functions selforganizing multilevel hierarchy recurrent networks the second involving two recurrent networks tries collapse multilevel predictor hierarchy single recurrent net experiments show system require less computation per time step many fewer training sequences conventional training algorithms recurrent nets
a neural network model selforganization ocular dominance lateral connections binocular input presented the selforganizing process results network afferent weights neuron organize smooth hillshaped receptive fields primarily one retinas neurons common eye preference form connected intertwined patches lateral connections primarily link regions eye preference similar selforganization cortical structures observed experimentally strabismic kittens the model shows patterned lateral connections cortex may develop based correlated activity explains lateral connection patterns follow receptive field properties ocular dominance
relaxation oscillations exhibiting one time scale arise naturally many physical systems this paper proposes method numerically integrate large systems relaxation oscillators the numerical technique called singular limit method derived analysis relaxation oscillations singular limit in limit system evolution gives rise time instants fast dynamics takes place intervals slow dynamics takes place a full description method given legion locally excitatory globally inhibitory oscillator networks fast dynamics characterized jumping leads dramatic phase shifts captured method iterative operation slow dynamics entirely solved the singular limit method evaluated computer experiments produces remarkable speedup compared methods integrating systems the speedup makes possible simulate largescale oscillator networks
a selforganizing model spiking neurons dynamic thresholds lateral excitatory inhibitory connections presented tested image segmentation task the model integrates two previously separate lines research modeling visual cortex laterally connected selforganizing maps used model afferent structures lateral connections could selforganize inputdriven hebbian adaptation spiking neurons leaky integrator synapses used model image segmentation binding synchronization desynchronization neuronal activity although approaches differ model neuron overall layout laterally connected twodimensional network this paper shows selforganization segmentation achieved network thus presenting unified model development func tional dynamics primary visual cortex
the full bayesian method applying neural networks prediction problem set priorhyperprior structure net perform necessary integrals however integrals tractable analytically markov chain monte carlo mcmc methods slow especially parameter space highdimensional using gaussian processes approximate weight space integral analytically small number hyperparameters need integrated mcmc methods we applied idea classification problems obtaining ex cellent results realworld problems investigated far
recently propp wilson proposed algorithm called coupling past cftp allows approximate perfect ie exact simulation stationary distribution certain finite state space markov chains perfect sampling using cftp successfully extended context point processes amongst authors haggstrom et al in gibbs sampling applied bivariate point process penetrable spheres mixture model however general running time cftp terms number transitions independent state sampled thus impatient user aborts long runs may introduce subtle bias user impatience bias fill introduced exact sampling algorithm finite state space markov chains contrast cftp unbiased user impatience fills algorithm form rejection sampling similar cftp requires sufficient monotonicity properties transition kernel used we show fills version rejection sampling extended infinite state space context produce exact sample penetrable spheres mixture process related models following use gibbs sampling make use partial order mixture model state space thus
cells visual cortex selective ocular dominance orientation input also size spatial frequency the simulations reported paper show size selectivity could develop hebbian selforganization receptive fields different sizes could organize columns like orientation ocular dominance the lateral connections network selforganize cooperatively simultaneously receptive field sizes produce patterns lateral connectivity closely follow receptive field organization together previous work ocular dominance orientation selectivity results suggest single hebbian selforganizing process give rise major receptive field properties visual cortex also structured patterns lateral interactions verified experimentally others predicted model the model also suggests functional role selforganized structures the afferent receptive fields develop sparse coding visual input recurrent lateral interactions eliminate redundancies cortical activity patterns allowing cortex efficiently process massive amounts visual information
a pilot study described practical application artificial neural networks the limit cycle attitude control satellite selected test case one sources limit cycle position dependent error observed attitude a reinforcement learning method selected able adapt controller cost function optimised an estimate cost function learned neural critic in approach estimated cost function directly represented function parameters linear controller the critic implemented cmac network results simulations show method able find optimal parameters without unstable behaviour in particular case large discontinuities attitude measurements method shows clear improvement compared conventional approach rms attitude error decreases approximately
in nutshell describe generic ilp problem following given set e positive negative examples target predicate background knowledge b world usually logic program including facts auxiliary predicates task find logic program h hypothesis positive examples deduced b h negative example in paper review results achieved area discuss techniques used moreover prove following new results predicates described nonrecursive local clauses k literals paclearnable distribution this generalizes previous result valid constrained clauses predicates described k nonrecursive local clauses paclearnable distribution this generalizes previous result non construc tive valid class distributions finally introduce believe first theoretical framework learning prolog clauses presence errors to purpose introduce new noise model call fixed attribute noise model learning propositional concepts boolean domain this new noise model interest
the expectationmaximization algorithm given dempster et al enjoyed considerable popularity solving map estimation problems this note gives simple derivation algorithm due luttrell better illustrates convergence properties algorithm variants the algorithm illustrated two examples pooling data multiple noisy sources fitting mixture density
we consider problem incorporate prior knowledge supervised learning techniques we set problem framework regularization theory consider case know approximated function radial symmetry the problem solved two alternative ways use invariance constraint regularization theory framework derive rotation invariant version radial basis functions use radial symmetry create new virtual examples given data set we show two apparently different methods learning
i present computational results suggesting gainadaptation algorithms based part connectionist learning methods may improve least squares classical parameterestimation methods stochastic timevarying linear systems the new algorithms evaluated respect classical methods along three dimensions asymptotic error computational complexity required prior knowledge system the new algorithms order complexity lms methods on n dimensionality system whereas leastsquares methods kalman filter on the new methods also improve kalman filter require complete statistical model system varies time in simple computational experiment new methods shown produce asymptotic error levels near optimal kalman filter significantly leastsquares lms methods the new methods may perform better even kalman filter error filters model system varies time
windowing proposed procedure efficient memory use id decision tree learning algorithm however previous work shown windowing may often lead decrease performance in work try argue separateandconquer rule learning algorithms appropriate windowing divideandconquer algorithms learn rules independently less susceptible changes class distributions in particular present new windowing algorithm achieves additional gains efficiency exploiting property separateandconquer algorithms while presented algorithm suitable redundant noisefree data sets also briefly discuss problem noisy data windowing present preliminary ideas might solved extension algorithm introduced paper
this article describes comprehensive approach automatic theory revision given imperfect theory approach combines explanation attempts incorrectly classified examples order identify failing portions theory for theory fault correlated subsets examples used inductively generate correction because corrections focused tend preserve structure original theory because system starts approximate domain theory general fewer training examples required attain given level performance classification accuracy compared purely empirical system the approach applies classification systems employing propositional hornclause theory the system tested variety application domains results presented problems domains molecular biology plant disease diagnosis
suppose one wishes sample density x using markov chain monte carlo mcmc an auxiliary variable u conditional distribution ujx defined giving joint distribution x u xujx a mcmc scheme samples joint distribution lead substantial gains efficiency compared standard approaches the revolutionary algorithm swendsen wang one example in addition reviewing swendsenwang algorithm generalizations paper introduces new auxiliary variable method called partial decoupling two applications bayesian image analysis considered the first binary classification problem partial decoupling performs sw single site metropolis the second pet reconstruction uses gray level prior geman mcclure a generalized swendsenwang algorithm developed problem reduces computing time point mcmc viable method posterior exploration
in paper analyse theoretical properties slice sampler we find algorithm extremely robust geometric ergodicity properties for case one auxiliary variable demonstrate algorithm stochastically monotone deduce analytic bounds total variation distance stationarity method using fosterlyapunov drift condition methodology
this paper studies well combination simulated annealing adfs solves genetic programming gp style program discovery problems on suite composed evenkparity problems k analyses performance simulated annealing adfs compared using adfs in contrast gp results suite simulated annealing run adfs problem size increases advantage using standard gp program representation marginal when performance simulated annealing compared gp algorithm using adfs evenparity problem gp advantageous evenparity problem sa gp equal evenparity problem sa advantageous
most learning algorithms work effectively training data contain completely specified labeled samples in many diagnostic tasks however data include values attributes model blocking process hides values attributes learner while blockers remove values critical attributes handicap learner paper instead focuses blockers remove irrelevant attribute values ie values needed classify instance given values unblocked attributes we first motivate formalize model superfluousvalue blocking demonstrate omissions useful proving certain classes seem hard learn general pac model viz decision trees dnf formulae trivial learn setting we also show model extended deal theory revision ie modifying existing formula blockers occasionally include superfluous values exclude required values cor ruptions training data
this paper presents approach automatic discovery functions genetic programming the approach based discovery useful building blocks analyzing evolution trace generalizing blocks define new functions finally adapting problem representation onthefly adaptating representation determines hierarchical organization extended function set enables restructuring search space solutions found easily measures complexity solution trees defined adaptive representation framework the minimum description length principle applied justify feasibility approaches based hierarchy discovered functions suggest alternative ways defining problems fitness function preliminary empirical results presented
a decision problem associated fundamental nonconvex model linearly inseparable pattern sets shown npcomplete another nonconvex model employs norm instead norm solved polynomial time solving n linear programs n usually small dimensionality pattern space an effective lpbased finite algorithm proposed solving latter model the algorithm employed obtain nonconvex piecewiselinear function separating points representing measurements made fine needle aspirates taken benign malignant human breasts a computer program trained samples correctly diagnosed new samples encountered currently use university wisconsin hospitals introduction the fundamental problem wish address
many connectionist approaches musical expectancy music composition let question what next overshadow equally important question when next one escape latter question one temporal structure considering perception musical meter we view perception metrical structure dynamic process temporal organization external musical events synchronizes entrains listeners internal processing mechanisms this article introduces novel connectionist unit based upon mathematical model entrainment capable phase frequencylocking periodic components incoming rhythmic patterns networks units selforganize temporally structured responses rhythmic patterns the resulting network behavior embodies perception metrical structure the article concludes discussion implications approach theories metrical structure musical expectancy
over years several packages developed provide workbench genetic algorithm ga research most packages use generational model inspired genesis a adopted steadystate model used genitor unfortunately deficiencies working orderbased problems packing routing scheduling this paper describes libga developed specifically orderbased problems also works easily kinds problems it offers easy use userfriendly interface allows comparisons made generational steadystate genetic algorithms particular problem it includes variety genetic operators reproduction crossover mutation libga makes easy use operators new ways particular applications develop include new operators finally offers unique new feature dynamic generation gap
human episodic memory provides seemingly unlimited storage everyday experiences retrieval system allows us access experiences partial activation components the system believed consist fast temporary storage hippocampus slow longterm storage within neocortex this paper presents neural network model hippocampal episodic memory inspired damasios idea convergence zones the model consists layer perceptual feature maps binding layer a perceptual feature pattern coarse coded binding layer stored weights layers a partial activation stored features activates binding pattern turn reactivates entire stored pattern for many configurations model theoretical lower bound memory capacity derived order magnitude higher number units model several orders magnitude higher number bindinglayer units computational simulations indicate average capacity order magnitude larger theoretical lower bound making connectivity layers sparser causes even increase capacity simulations also show descriptive binding patterns used errors tend plausible patterns confused similar patterns slight cost capacity the convergencezone episodic memory therefore accounts immediate storage associative retrieval capability large capacity hippocampal memory shows memory encoding areas much smaller perceptual maps consist rather coarse computational units sparsely connected perceptual maps
empirical learning results pollyanna the value empirical learning demonstrated results testing theory space search tss component pollyanna empirical data shows approximations generated generic simplifying assumptions widely varying levels accuracy efficiency the candidate theory space includes theories pareto optimal combinations accuracy efficiency well others nonoptimal empirical learning thus needed separate optimal theories nonoptimal ones it works filter process generating approximations generic simplifying assumptions empirical tests serve additional purpose well theory space search collects data precisely characterizes tradeoff accuracy efficiency among candidate approximate theories the tradeoff data used select theory best balances competing objectives accuracy efficiency manner appropriate intended performance context the feasibility empirical learning also addressed results testing theory space search component pollyanna in order empirical testing feasible candidate approximate theories must operationally usable candidate hearts theories generated pollyanna shown operationally usable experimental results theory space search tss phase learning they run real machine producing results compared training examples feasibility also depends information computation costs empirical testing information costs result need supply system training examples computation costs result need execute candidate theories both types costs grow numbers candidate theories tested experimental results show empirical testing pollyanna limited computation costs executing candidate theories information costs obtaining many training examples pollyanna contrasts respect traditional inductive learning systems the feasibility empirical learning depends also intended performance context resources available context learning measurements theory space search phase indicate tss algorithms performing exhaustive search would feasible hearts domain although may feasible applications tss algorithms avoid exhaustive search hold considerably promise
in paper adopt generalsum stochastic games framework multiagent reinforcement learning our work extends previous work littman zerosum stochastic games broader framework we design multiagent qlearning method framework prove converges nash equilibrium specified conditions this algorithm useful finding optimal strategy exists unique nash equilibrium game when exist multiple nash equilibria game algorithm combined learning techniques find optimal strategies
rising operating costs structural transformations resizing globalization companies world brought focus emerging discipline knowledge management concerned making knowledge pay corporate memories form important part knowledge management initiatives company in paper discuss viewing corporate memories distributed case libraries benefit existing techniques distributed casebased reasoning resource discovery exploitation previous expertise we present two techniques developed context multiagent casebased reasoning accessing exploiting past experience corporate memory resources the first approach called negotiated retrieval deals retrieving assembling case pieces different resources corporate memory form good overall case the second approach based federated peer learning deals two modes cooperation called distcbr colcbr let agent exploit experience expertise peer agents achieve local task fl the first author would like acknowledge support national science foundation grant nos iri eec the second authors research reported paper developed iiia inside analog project funded spanish cicyt grant the content paper necessarily reflect position policy us government kingdom spain government catalonia government official endorsement inferred
when learning reasoning failures knowledge system behaves powerful lever deciding went wrong system deciding system needs learn a number benefits arise systems possess knowledge operation knowledge abstract knowledge cognition used select diagnosis repair strategies among alternatives specific kinds selfknowledge used distinguish failure hypothesis candidates making selfknowledge explicit also facilitate use knowledge across domains provide principled way incorporate new learning strategies to illustrate advantages selfknowledge learning provide implemented examples two different systems a plan execution system called rapter story understanding system called metaaqua
theory revision integrates inductive learning background knowledge combining training examples coarse domain theory produce accurate theory there two challenges theory revision theoryguided systems face first representation language appropriate initial theory may inappropriate improved theory while original representation may concisely express initial theory accurate theory forced use representation may bulky cumbersome difficult reach second theory structure suitable coarse domain theory may insufficient finetuned theory systems produce small local changes theory limited value accomplishing complex structural alterations may required consequently advanced theoryguided learning systems require flexible representation flexible structure an analysis various theory revision systems theoryguided learning systems reveals specific strengths weaknesses terms two desired properties designed capture underlying qualities system new system uses theoryguided constructive induction experiments three domains show improvement previous theoryguided systems this leads study behavior limitations potential theoryguided constructive induction
if experiment requires statistical analysis establish result one better experiment ernest rutherford most proponents cold fusion reporting excess heat electrolysis experiments claiming one main characteristics cold fusion irreproducibility jr huizenga cold fusion p abstract amid ever increasing research various aspects neural computing much progress evident theoretical advances empirical studies on empirical side wealth data experimental studies reported it however clear best report neural computing experiments may replicated interested researchers in particular nature iterative learning randomised initial architecture backpropagation training multilayer perceptron precise replication reported result virtually impossible the outcome experimental replication reported results touchstone scientific method option researchers popular subfield neural computing in paper address issue replicability experiments based backpropagation training multilayer perceptrons although many results applicable subfield plagued characteristics first attempt produce complete abstract specification neural computing experiment from specification identify full range parameters needed support maximum replicability use show absolute replicability option practice we propose statistical framework support replicability we demonstrate framework empirical studies replicability respect experimental controls validity implementations backpropagation algorithm finally suggest degree replicability neural computing experiment estimated reflected claimed precision empirical results reported
in paper propose unsupervised neural network allowing robot learn sensorimotor associations delayed reward the robot task learn meaning pictograms order survive maze first introduce new neural conditioning rule pcr probabilistic conditioning rule allowing test hypotheses associations visual categories movements given time span second describe real maze experiment mobile robot we propose neural architecture solve problem discuss difficulty build visual categories dynamically associating movements third propose use algorithm simulation order test exhaustively we give results different kind mazes compare system adapted version qlearning algorithm finally conclude showing limitations approaches take account intrinsic complexity reasonning based image recognition
we present framework analysis synthesis acoustical instruments based datadriven probabilistic inference modeling audio time series boundary conditions played instrument recorded nonlinear mapping control data audio space inferred using general inference framework clusterweighted modeling the resulting model used realtime synthesis audio sequences new input data
in many realworld domains task machine learning algorithms learn theory predicting numerical values in particular several standard test domains used inductive logic programming ilp concerned predicting numerical values examples relational mostly nondeterminate background knowledge however far ilp algorithm except one predict numbers cope nondeterminate background knowledge the exception covering algorithm called fors in paper present structural regression trees srt new algorithm applied class problems integrating statistical method regression trees ilp srt constructs tree containing literal atomic formula negation conjunction literals node assigns numerical value leaf srt provides comprehensible results purely statistical methods applied class problems ilp systems handle experiments several realworld domains demonstrate approach competitive existing methods indicating advantages expense predictive accuracy
a quantitative practical bayesian framework described learning mappings feedforward networks the framework makes possible objective comparisons solutions using alternative network architectures objective stopping rules network pruning growing procedures objective choice magnitude type weight decay terms additive regularisers penalising large weights etc measure effective number welldetermined parameters model quantified estimates error bars network parameters network output objective comparisons alternative learning interpolation models splines radial basis functions the bayesian evidence automatically embodies occams razor penalising overflexible overcomplex models the bayesian approach helps detect poor underlying assumptions learning models for learning models well matched problem good correlation generalisation ability this paper makes use bayesian framework regularisation model comparison described companion paper bayesian interpolation mackay this framework due gull skilling gull bayesian evidence obtained
simultaneous multithreading technique permits multiple independent threads issue multiple instructions cycle in previous work demonstrated performance potential simultaneous multithreading based somewhat idealized model in paper show throughput gains simultaneous multithreading achieved without extensive changes conventional wideissue superscalar either hardware structures sizes we present architecture simultaneous multithreading achieves three goals minimizes architectural impact conventional superscalar design minimal performance impact single thread executing alone achieves significant throughput gains running multiple threads our simultaneous multithreading architecture achieves throughput instructions per cycle fold improvement unmodified superscalar similar hardware resources this speedup enhanced advantage multithreading previously unexploited architectures ability favor fetch issue threads efficiently using processor cycle thereby providing best instructions processor
the theory revision problem problem best go revising deficient domain theory using information contained examples expose inaccuracies in paper present approach theory revision problem propositional domain theories the approach described called ptr uses probabilities associated domain theory elements numerically track ow proof theory this allows us measure precise role clause literal allowing preventing desired undesired derivation given example this information used efficiently locate repair awed elements theory ptr proved converge theory correctly classifies examples shown experimentally fast accurate even deep theories
new methodology fully bayesian mixture analysis developed making use reversible jump markov chain monte carlo methods capable jumping parameter subspaces corresponding different numbers components mixture a sample full joint distribution unknown variables thereby generated used basis thorough presentation many aspects posterior distribution the methodology applied analysis univariate normal mixtures using hierarchical prior model offers approach dealing weak prior information avoiding mathematical pitfalls using improper priors mixture context
northeastern university college computer science technical report nuccs fl we gratefully acknowledge substantial contributions effort provided andy barto sparked original interest questions whose continued encouragement insightful comments criticisms helped us greatly recent discussions satinder singh vijay gullapalli also helpful impact work special thanks also rich sutton influenced thinking subject numerous ways this work supported grant iri national science foundation u s air force
active learning differs passive learning examples learning algorithm assumes least control part input domain receives information in situations active learning provably powerful learning examples alone giving better generalization fixed number training examples in paper consider problem learning binary concept absence noise valiant we describe formalism active concept learning called selective sampling show may approximately implemented neural network in selective sampling learner receives distribution information environment queries oracle parts domain considers useful we test implementation called sgnetwork three domains observe significant improvement generalization
high performance architectures always deal performancelimiting impact branch operations microprocessor designs going deal problem well move towards deeper pipelines support multiple instruction issue branch prediction schemes often used alleviate negative impact branch operations allowing speculative execution instructions unresolved branch another technique eliminate branch instructions altogether predication remove forward branch instructions translating instructions following branch predicate form this paper analyzes variety existing predication models eliminating branch operations effect elimination branch prediction schemes existing processors including single issue architectures simple prediction mechanisms newer multiissue designs correspondingly sophisticated branch predictors the effect branch prediction accuracy branch penalty basic block size studied hhhhhhhhhhhhhhhhhhhhhhhh
this paper describes model complementarity rules precedents classification task under model precedents assist rulebased reasoning operationalizing abstract rule antecedents conversely rules assist casebased reasoning case elaboration process inferring case facts order increase similarity cases term reformulation process replacing term whose precedents weakly match case terms whose precedents strongly match case fully exploiting complementarity requires control strategy characterized impartiality absence arbitrary ordering restrictions use rules precedents an impartial control strategy implemented grebe domain texas workers compensation law in preliminary evaluation grebes performance found good slightly better performance law students task a case classified belonging particular category relating description criteria category membership the justifications warrants toulmin relate case category vary widely generality antecedents for example consider warrants classifying case legal category negligence a rule an action negligent actor fails use reasonable care failure proximate cause injury general antecedent terms eg breach reasonable care conversely precedent dr jones negligent failed count sponges surgery result left sponge smith specific antecedent terms eg failure count sponges both types warrants used classification systems relate cases categories classification systems used precedents help match antecedents rules cases completing match difficult terms antecedent opentextured ie significant uncertainty whether match specific facts gardner mccarty sridharan this problem results generality gap separating abstract terms specific facts porter et al precedents opentextured term ie past cases term applied used bridge gap unlike rule antecedents antecedents precedents level generality cases generality gap exists precedents new cases precedents therefore reduce problem matching specific case facts opentextured terms problem matching two sets specific facts for example injured employees entitlement workers compensation depends whether injured activity furtherance employment determining whether particular case classified compensable injury therefore requires matching specific facts case eg john injured automobile accident driving office opentextured term activity furtherance employment the gap generality case description abstract term makes match problematical however completing match may much easier precedents term activity furtherance employment eg marys injury compensable occurred driving work activity furtherance employment bills injury compensable occurred driving house deliver pizza activity furtherance employment in case johns driving office closely matches marys driving work
we introduce modelbased average reward reinforcement learning method called hlearning compare discounted counterpart adaptive realtime dynamic programming simulated robot scheduling task we also introduce extension hlearning automatically explores unexplored parts state space always choosing greedy actions respect current value function we show autoexploratory hlearning performs better original hlearning previously studied exploration methods random recencybased counterbased exploration
this paper proposes using fuzzy logic techniques dynamically control parameter settings genetic algorithms gas we describe dynamic parametric ga ga uses fuzzy knowledgebased system control ga parameters we introduce technique automatically designing tuning fuzzy knowledgebase system using gas results initial experiments show performance improvement simple static ga one dynamic parametric ga system designed automatic method demonstrated improvement application included design phase may indicate general applicability dynamic parametric ga wide range ap plications
in previous work olshausen field algorithm described learning linear sparse codes trained natural images produces set basis functions spatially localized oriented bandpass ie waveletlike this note shows algorithm may interpreted within maximumlikelihood framework several useful insights emerge connection makes explicit relation statistical independence ie factorial coding shows formal relationship algorithm bell sejnowski suggests adapt parameters previously fixed this report describes research done within center biological computational learning department brain cognitive sciences massachusetts institute technology this research sponsored individual national research service award bao nimh fmh grant national science foundation contract asc award includes funds arpa provided hpcc program cbcl
we study layered belief networks binary random variables conditional probabilities prchildjparents depend monotonically weighted sums parents for networks give efficient algorithms computing rigorous bounds marginal probabilities evidence output layer our methods apply generally computation upper lower bounds well generic transfer function parameterizations conditional probability tables sigmoid noisyor we also prove rates convergence accuracy bounds function network size our results derived applying theory large deviations weighted sums parents node network bounds marginal probabilities computed two contributions one assuming weighted sums fall near mean values assuming this gives rise interesting tradeoff probable explanations evidence improbable deviations mean in networks child n parents gap upper lower bounds behaves sum two terms one order p in addition providing rates convergence large networks methods also yield efficient algorithms approximate inference fixed networks
feature selection proven valuable technique supervised learning improving predictive accuracy reducing number attributes considered task we investigate potential similar benefits unsupervised learning task conceptual clustering the issues raised feature selection absence class labels discussed implementation sequential feature selection algorithm based existing conceptual clustering system described additionally present second implementation employs technique improving efficiency search optimal description compare performance algorithms
robust flexible sufficiently general vision systems recognition description complex dimensional objects require adequate armamentarium representations learning mechanisms this paper briefly analyzes strengths weaknesses different learning paradigms symbol processing systems connectionist networks statistical syntactic pattern recognition systems possible candidates providing capabilities points several promising directions integrating multiple paradigms synergistic fashion towards goal
a selforganizing neural network sequence classification called sardnet described analyzed experimentally sardnet extends kohonen feature map architecture activation retention decay order create unique distributed response patterns different sequences sardnet yields extremely dense yet descriptive representations sequential input training iterations the network proven successful mapping arbitrary sequences binary real numbers well phonemic representations english words potential applications include isolated spoken word recognition cognitive science models sequence processing
liacc technical report abstract in paper address problem acquiring knowledge integration our aim construct integrated knowledge base several separate sources the objective integration construct one system exploits knowledge available good performance the aim paper discuss methodology knowledge integration present concrete results in experiments performance integrated theory exceeded performance individual theories quite significant amount also performance fluctuate much experiments repeated these results indicate knowledge integration complement existing ml methods
in introduction define term bias used machine learning systems we motivate importance automated methods evaluating selecting biases using framework bias selection search bias metabias spaces recent research field machine learning bias summarized
a method initial results comparative study abstract a standard approach determining decision trees learn examples a disadvantage approach decision tree learned difficult modify suit different decision making situations such problems arise example attribute assigned node measured significant change costs measuring attributes frequency distribution events different decision classes an attractive approach resolving problem learn store knowledge form decision rules generate whenever needed decision tree suitable given situation an additional advantage approach facilitates building compact decision trees much simpler logically equivalent conventional decision trees compact trees meant decision trees may contain branches assigned set values nodes assigned derived attributes ie attributes logical mathematical functions original ones the paper describes efficient method aqdt takes decision rules generated aqtype learning system aq aq builds decision tree optimizing given optimality criterion the method work two modes standard mode produces conventional decision trees compact mode produces compact decision trees the preliminary experiments aqdt shown decision trees generated decision rules conventional compact outperformed generated examples wellknown c program terms simplicity predictive accuracy
ogi cse technical report abstract smoothing regularizers radial basis functions studied extensively general smoothing regularizers projective basis functions pbfs widelyused sigmoidal pbfs heretofore proposed we derive new classes algebraicallysimple th order smoothing regularizers networks projective basis functions f w x p n fi fl u general transfer functions g these simple algebraic forms rw enable direct enforcement smoothness without need costly monte carlo integrations sw the regularizers tested illustrative sample problems compared quadratic weight decay the new regularizers shown yield better generalization errors
we address problem musical variation identification different musical sequences variations implications mental representations music according reductionist theories listeners judge structural importance musical events forming mental representations these judgments may result production reduced memory representations retain musical gist in study improvised music performance pianists produced variations melodies analyses musical events retained across variations provided support reductionist account structural importance a neural network trained produce reduced memory representations melodies represented structurally important events efficiently others agreement among musicians improvisations network model musictheoretic predictions suggest perceived constancy across musical variation natural result reductionist mechanism producing memory representations
ensemble learning variational free energy minimization tool introduced neural networks hinton van camp learning described terms optimization ensemble parameter vectors the optimized ensemble approximation posterior probability distribution parameters this tool applied variety statistical inference problems in paper i study linear regression model parameters hyperparameters i demonstrate evidence approximation optimization regularization constants derived detail free energy minimization view point
the self regenerative mcmc tool constructing markov chain given stationary distribution constructing auxiliary chain stationary distribution elements auxiliary chain picked suitable random number times resulting chain stationary distribution sahu zhigljavsky in article provide generic adaptation scheme algorithm the adaptive scheme use knowledge stationary distribution gathered far update course simulation this method easy implement often leads considerable improvement we obtain theoretical results adaptive scheme our proposed methodology illustrated number realistic examples bayesian computation performance compared available mcmc techniques in one applications develop nonlinear dynamics model modeling predatorprey relationships wild
conceptual analogy ca approach integrates conceptualization ie memory organization based prior experiences analogical reasoning borner it implemented prototypically tested support design process building engineering borner janetzko borner there number features distinguish ca standard approaches cbr ar first ca automatically extracts knowledge needed support design tasks ie complex case representations relevance object features relations proper adaptations attributevalue representations prior layouts secondly effectively determines similarity complex case representations terms adaptability thirdly implemented integrated highly interactive adaptive system architecture allows incremental knowledge acquisition user support this paper surveys basic assumptions psychological results influenced development ca it sketches knowledge representation formalisms employed characterizes subprocesses needed integrate memory organization analogical reasoning
even sophisticated branchprediction techniques necessarily suffer mispredictions even relatively small mispredict rates hurt performance substantially currentgeneration processors in paper investigate schemes improving performance face imperfect branch predictors processor simultaneously execute code taken nottaken outcomes branch this paper presents data regarding limits multipath execution considers fetchbandwidth needs multipath execution discusses various dynamic confidenceprediction schemes gauge likelihood branch mispredictions our evaluations consider executing along several paths using paths relatively simple confidence predictor multipath execution garners speedups compared singlepath case average speedup specint suite while associated increases instructionfetchbandwidth requirements surprising less expected result significance separate returnaddress stack forked path overall results indicate multipath execution offers significant improvements singlepath performance could especially useful combined multithreading hardware costs amortized approaches
this paper presents algorithms robustness analysis bayesian networks global neighborhoods robust bayesian inference calculation bounds posterior values given perturbations probabilistic model we present algorithms robust inference including expected utility expected value variance bounds global perturbations modeled contaminated constant density ratio constant density bounded total variation classes distributions c fl carnegie mellon university
this paper describes selflearning control system mobile robot based local sensor data robot taught avoid collisions obstacles the feedback control system binaryvalued external reinforcement signal indicates whether collision occured a reinforcement learning scheme used find correct mapping input sensor space output steering signal space an adaptive quantisation scheme introduced discrete division input space built scratch system
rules extracted trained feedforward networks used explanation validation crossreferencing network output decisions this paper introduces rule evaluation ordering mechanism orders rules extracted feedforward networks based three performance measures detailed experiments using three rule extraction techniques applied wisconsin breast cancer database illustrate power proposed methods moreover method integrating output decisions extracted rulebased system corresponding trained network proposed the integrated system provides improvements
standard methods inducing structure weight values recurrent neural networks fit assumed class architectures every task this simplification necessary interactions network structure function well understood evolutionary computation includes genetic algorithms evolutionary programming populationbased search method shown promise complex tasks this paper argues genetic algorithms inappropriate network acquisition describes evolutionary program called gnarl simultaneously acquires structure weights recurrent networks this algorithms empirical acquisition method allows emergence complex behaviors topologies potentially excluded artificial architectural constraints imposed standard network induction methods
a new mechanism genetic encoding neural networks proposed loosely based marker structure biological dna the mechanism allows aspects network structure including number nodes connectivity evolved genetic algorithms the effectiveness encoding scheme demonstrated object recognition task requires artificial creatures whose behaviour driven neural network develop highlevel finitestate exploration discrimination strategies the task requires solving sensorymotor grounding problem ie developing functional understanding effects creatures movement sensory input
we study multivariate smoothing spline estimate function several variables based anova decomposition sums main effect functions one variable twofactor interaction functions two variables etc we derive bayesian confidence intervals components decomposition demonstrate even multiple smoothing parameters efficiently computed using publicly available code rkpack originally designed compute estimates we carry small monte carlo study see closely actual properties componentwise confidence intervals match nominal confidence levels lastly analyze lake acidity data function calcium concentration latitude longitude using polynomial thin plate spline main effects model
appear cowan j tesauro g alspector j eds advances neural information processing systems san francisco ca morgan kaufmann publishers this paper written tersely accomodate page limitation presented refereed poster session conference neural information processing systemsnatural synthetic november denver co supported nsf grants dms dms national eye institutenih grants ey ey
virtually largescale sequencing projects use automatic sequenceassembly programs aid determination dna sequences the computergenerated assemblies require substantial handediting transform submissions genbank as size sequencing projects increases becomes essential improve quality automated assemblies timeconsuming handediting may reduced current abi sequencing technology uses base calls made fluorescentlylabeled dna fragments run gels we present new representation fluorescent trace data associated individual base calls this representation used fragment assembly improve quality assemblies we demonstrate one use endtrimming suboptimal data results significant improvement quality subsequent assemblies
extensive research done extracting parallelism single instruction stream processors this paper presents results investigation ways modify mimd architectures allow extract instruction level parallelism achieved current superscalar vliw machines a new architecture proposed utilizes advantages multiple instruction stream design addressing limitations prevented mimd architectures performing ilp operation a new code scheduling mechanism described support new architecture partitioning instructions across multiple processing elements order exploit level parallelism
this paper describes single chip multiple instruction stream computer misc capable extracting instruction level parallelism broad spectrum programs the misc architecture uses multiple asynchronous processing elements separate program streams executed parallel integrates conflictfree message passing system lowest level processor design facilitate low latency intramisc communication this approach allows increased machine parallelism minimal code expansion provides alternative approach single instruction stream multiissue machines superscalar vliw
in paper define examine two versions bridge problem the first variant bridge problem determistic model agent knows superset transitions priori probabilities transitions intact in second variant transitions break fixed probability time step these problems applicable planning uncertain domains well packet routing computer network we show agent act optimally models reduction markov decision processes we describe methods solving note methods intractable reasonably sized problems finally suggest neurodynamic programming method value function approximation types models
if several mental states reliably distinguished recognizing patterns eeg paralyzed person could communicate device like wheelchair composing sequencesof mental states in article report study comparing four representations eeg signals classification twolayer neural network sigmoid activation functions the neural network implemented cnaps server processor simd architecture adaptive solutions inc gaining fold decrease training time sun
recurrent perceptron classifiers generalize classical perceptron model they take account correlations dependences among input coordinates arise linear digital filtering this paper provides tight bounds sample complexity associated fitting models experimental data
i present general taxonomy neural net architectures processing timevarying patterns this taxonomy subsumes many existing architectures literature points several promising architectures yet examined any architecture processes timevarying patterns requires two conceptually distinct components shortterm memory holds relevant past events associator uses shortterm memory classify predict my taxonomy based characterization shortterm memory models along dimensions form content adaptability experiments predicting future values financial time series us dollarswiss franc exchange rates presented using several alternative memory models the results experiments serve baseline sophisticated architectures compared neural networks proven promising alternative traditional techniques nonlinear temporal prediction tasks eg curtiss brandemuehl kreider lapedes farber weigend huberman rumelhart however temporal prediction particularly challenging problem conventional neural net architectures algorithms well suited patterns vary time the prototypical use neural nets structural pattern recognition in task collection featuresvisual semantic otherwiseis presented network network must categorize input feature pattern belonging one classes for example network might trained classify animal species based set attributes describing living creatures tail lives water carnivorous network could trained recognize visual patterns twodimensional pixel array letter fa b zg in tasks network presented relevant information simultaneously in contrast temporal pattern recognition involves processing patterns evolve time the appropriate response particular point time depends current input potentially previous inputs this illustrated figure shows basic framework temporal prediction problem i assume time quantized discrete steps sensible assumption many time series interest intrinsically discrete continuous series sampled fixed interval the input time denoted xt for univariate series input
dislex artificial neural network model mental lexicon it built test computationally whether lexicon could consist separate feature maps different lexical modalities lexical semantics connected ordered pathways in model orthographic phonological semantic feature maps associations formed unsupervised process based cooccurrence lexical symbol meaning after model organized various damage lexical system simulated resulting dyslexic categoryspecific aphasic impairments similar observed human patients
in paper describe new selforganizing decomposition technique learning highdimensional mappings problem decomposition performed errordriven manner resulting subtasks patches equally well approximated our method combines unsupervised learning scheme feature maps koh nonlinear approximator backpropagation rhw the resulting learning system stable effective changing environments plain backpropagation much powerful extended feature maps proposed rs rms extensions method give rise active exploration strategies autonomous agents facing unknown environments the appropriateness general purpose method demonstrated ex ample mathematical function approximation
irrelevant features weakly relevant features may reduce comprehensibility accuracy concepts induced supervised learning algorithms we formulate search feature subset abstract search problem probabilistic estimates searching space using evaluation function random variable requires trading accuracy estimates increased state exploration we show recent feature subset selection algorithms machine learning literature fit search problem simple hill climbing approaches conduct small experiment using bestfirst search technique
as field genetic programming gp matures breadth application increases need parallel implementations becomes absolutely necessary the transputerbased system recently presented koza one rare parallel implementations until today implementation proposed parallel gp using simd architecture except dataparallel approach although others exploited workstation farms pipelined supercomputers one reason certainly apparent difficulty dealing parallel evaluation different sexpressions single instruction executed time every processor the aim paper present implementation parallel gp simd system processor efficiently evaluate different sexpression we implemented approach maspar mp computer present timing results to extent simd machines like maspar available offer costeffective cycles scien tific experimentation useful approach
reinforcement learning problem generating optimal behavior sequential decisionmaking environment given opportunity interacting many algorithms solving reinforcementlearning problems work computing improved estimates optimal value function we extend prior analyses reinforcementlearning algorithms present powerful new theorem provide unified analysis valuefunctionbased reinforcementlearning algorithms the usefulness theorem lies allows asynchronous convergence complex reinforcementlearning algorithm proven verifying simpler synchronous algorithm converges we illustrate application theorem analyzing convergence qlearning modelbased reinforcement learning qlearning multistate updates qlearning markov games risksensitive reinforcement learning
hyperspectral image sensors provide images large number contiguous spectral channels per pixel enable information different materials within pixel obtained the problem spectrally unmixing materials may viewed specific case blind source separation problem data consists mixed signals case minerals goal determine contribution mineral mix without prior knowledge minerals mix the technique independent component analysis ica assumes spectral components close statistically independent provides unsupervised method blind source separation we introduce contextual ica context hyperspectral data analysis apply method mineral data synthetically mixed minerals real image signatures
partially observable markov decision processes pomdps allow one model complex dynamic decision control problems include action outcome uncertainty imperfect observability the control problem formulated dynamic optimization problem value function combining costs rewards multiple steps in paper propose analyse test various incremental methods computing bounds value function control problems infinite discounted horizon criteria the methods described tested include novel incremental versions gridbased linear interpolation method simple lower bound method sondiks updates both work arbitrary points belief space enhanced various heuristic point selection strategies also introduced new method computing initial upper bound fast informed bound method this method able improve significantly standard commonly used upper bound computed mdpbased method the quality resulting bounds tested maze navigation problem states actions observations
the energy prediction competition involved prediction series building energy loads series environmental input variables nonlinear regression using neural networks popular technique modeling tasks since obvious large timewindow inputs appropriate preprocessing inputs best viewed regression problem many possible input variables may actually irrelevant prediction output variable because finite data set show random correlations irrelevant inputs output conventional neural network even regularisation weight decay set coefficients junk inputs zero thus irrelevant variables hurt models performance the automatic relevance determination ard model puts prior regression parameters embodies concept relevance this done simple soft way introducing multiple regularisation constants one associated input using bayesian methods regularisation constants junk inputs automatically inferred large preventing inputs causing significant overfitting
several different approaches used describe concepts supervised learning tasks in paper describe two approaches prototypebased incremental neural networks casebased reasoning approaches we show improve prototypebased neural network model storing specific instances cbr memory system this leads us propose coprocessing hybrid model classification
extensive research done extracting parallelism single instruction stream processors this paper presents investigation ways modify mimd architectures allow extract instruction level parallelism achieved current superscalar vliw machines a new architecture proposed utilizes advantages multiple instruction stream design addressing limitations prevented mimd architectures performing ilp operation a new code scheduling mechanism described support new architecture partitioning instructions across multiple processing elements order exploit level parallelism
a performance prediction method presented indicating performance range mimd parallel processor systems neural network simulations the total execution time parallel application modeled sum calculation communication times the method scalable based times measured one processor one communication link performance speedup efficiency predicted larger processor system it validated quantitatively applying two popular neural networks backpropagation kohonen selforganizing feature map decomposed gcel transputer system agreement model measurements within
algorithms learning classification trees successes artificial intelligence statistics many years this paper outlines tree learning algorithm derived using bayesian statistics this introduces bayesian techniques splitting smoothing tree averaging the splitting rule similar quinlans information gain smoothing averaging replace pruning comparative experiments reimplementations minimum encoding approach quinlans c quinlan et al breiman et als cart breiman et al show full bayesian algorithm produce publication this paper final draft submitted publication statistics computing journal version minor changes appeared volume pages accurate predictions versions approaches though pay computational price
pomdps general models sequential decisions actions observations probabilistic many problems interest formulated pomdps yet use pomdps limited lack effective algorithms recently started change number problems robot navigation planning beginning formulated solved pomdps the advantage pomdp approach clean semantics ability produce principled solutions integrate physical information gathering actions in paper pursue approach context two learning tasks learning sort vector numbers learning decision trees data both problems formulated pomdps solved general pomdp algorithm the main lessons results use suitable heuristics representations allows solution sorting classification pomdps nontrivial sizes quality resulting solutions competitive best algorithms problematic aspects decision tree learning test misclassification costs noisy tests missing values naturally accommodated
given arbitrary learning situation difficult determine appropriate learning strategy the goal research provide general representation processing framework introspective reasoning strategy selection the learning framework introspective system perform reasoning task as system also records trace reasoning along results reasoning if reasoning failure occurs system retrieves applies introspective explanation failure order understand error repair knowledge base a knowledge structure called metaexplanation pattern used explain conclusions derived conclusions fail if reasoning represented explicit declarative manner system examine reasoning analyze reasoning failures identify needs learn select appropriate learning strategies order learn required knowledge without overreli ance programmer
we describe ongoing project develop adaptive training system ats dynamically models students learning processes provide specialized tutoring adapted students knowledge state learning style the student modeling component ats mlmodeler uses machine learning ml techniques emulate students novicetoexpert transition mlmodeler infers learning methods student used reach current knowledge state comparing students solution trace expert solution generating plausible hypotheses misconceptions errors student made a casebased approach used generate hypotheses incorrectly applying analogy overgeneralization overspecialization the student expert models use networkbased representation includes abstract concepts relationships well strategies problem solving fuzzy methods used represent uncertainty student model this paper describes design ats mlmodeler gives detailed example system would model tutor student typical session the domain use example highschool level chemistry
many algorithms parameters set user for machine learning algorithms parameter setting nontrivial task influence knowledge model returned algorithm parameter values usually set approximately according characteristics target problem obtained different ways the usual way use background knowledge target problem perform testing experiments the paper presents approach automated model selection based local optimization uses empirical evaluation constructed concept description guide search the approach tested using inductive concept learning system magnus
this paper presents method learning logic programs without explicit negative examples exploiting assumption output completeness a mode declaration supplied target predicate training input assumed accompanied legal outputs any outputs generated incomplete program implicitly represent negative examples however large numbers ground negative examples never need generated this method incorporated two ilp systems chillin ifoil use intensional background knowledge tests two natural language acquisition tasks caserole mapping pasttense learning illustrate advantages approach
instancebased learning methods explicitly remember data receive they usually training phase prediction time perform computation then take query search database similar datapoints build online local model local average local regression predict output value in paper review advantages instance based methods autonomous systems also note ensuing cost hopelessly slow computation database grows large we present evaluate new way structuring database new algorithm accessing maintains advantages instancebased learning earlier attempts combat cost instancebased learning sacrificed explicit retention data applicable instancebased predictions based small number near neighbors reintroduce explicit training phase form interpolative data structure our approach builds multiresolution data structure summarize database experiences resolutions interest simultaneously this permits us query database exibility conventional linear search greatly reduced computational cost
in paper introduce new agglomerative clustering algorithm pattern cluster represented collection fuzzy hyperboxes initially number hyperboxes calculated represent pattern samples then algorithm applies multiresolution techniques progressively combine hyperboxes hierarchial manner such agglomerative scheme found yield encouraging results realworld clustering problems
this paper introduces randomized technique partitioning examples using oblique hyperplanes standard decision tree techniques id descendants partition set points axisparallel hyperplanes our method contrast attempts find hyperplanes orientation the purpose general technique find smaller equally accurate decision trees created methods we tested algorithm real simulated data found cases produces surprisingly small trees without losing predictive accuracy small trees allow us turn obtain simple qualitative descriptions problem domain
this paper introduces icet new algorithm costsensitive classification icet uses genetic algorithm evolve population biases decision tree induction algorithm the fitness function genetic algorithm average cost classification using decision tree including costs tests features measurements costs classification errors icet compared three algorithms costsensitive classification eg csid idx also c classifies without regard cost the five algorithms evaluated empirically five realworld medical datasets three sets experiments performed the first set examines baseline performance five algorithms five datasets establishes icet performs significantly better competitors the second set tests robustness icet variety conditions shows icet maintains advantage the third set looks icets search bias space discovers way improve search
this paper highlights role mathematical programming particularly linear programming training neural networks a neural network description given terms separating planes input space suggests use linear programming determining planes a standard description terms mean square error output space also given leads use unconstrained minimization techniques training neural network the linear programming approach demonstrated brief description system breast cancer diagnosis use last four years major medical facility
dissatisfaction existing standard casebased reasoning cbr systems prompted us investigate make systems creative broadly would mean creative this paper discusses three research goals understanding creative processes better investigating role cases cbr creative problem solving understanding framework supports interesting kind casebased reasoning in addition discusses methodological issues study creativity particular use cbr research paradigm exploring creativity
this work presents application machine learning characterizing important property natural dna sequences compositional inhomogeneity compositional segments often correspond meaningful biological units taking account inhomogeneity prerequisite successful recognition functional features dna sequences especially proteincoding genes here present technique dna segmentation using hidden markov models a dna sequence represented chain homogeneous segments described one statistically discriminated hidden states whose contents form firstorder markov chain the technique used describe compare chromosomes i iv completely sequenced saccharomyces cerevisiae yeast genome our results indicate existence well separated states gives support isochore theory we also explore models likelihood landscape analyze dynamics optimization process thus addressing problem reliability obtained optima efficiency algorithms
weight modifications traditional neural nets computed hardwired algorithms without exception previous weight change algorithms many specific limitations is principle possible overcome limitations hardwired algorithms allowing neural nets run improve weight change algorithms this paper constructively demonstrates answer principle yes i derive initial gradientbased sequence learning algorithm selfreferential recurrent network speak weight matrix terms activations it uses input output units observing errors explicitly analyzing modifying weight matrix including parts weight matrix responsible analyzing modifying weight matrix the result first introspective neural net explicit potential control adaptive parameters a disadvantage algorithm high computational complexity per time step independent sequence length equals on conn logn conn n conn number connections another disadvantage high number local minima unusually complex error surface the purpose paper however come efficient introspective selfreferential weight change algorithm show algorithms possible
this paper discusses problem implement manytomany multiassociative mapping within connectionist models traditional symbolic approaches work explicit representation alternatives via stored links implicitly enumerative algorithms classical pattern association models ignore issue generating multiple outputs single input pattern recent research recurrent networks promising field clearly focused upon multiassociativity goal in paper define multiassociative memory mm several possible variants discuss utility general cognitive modeling we extend sequential cascaded networks pollack fit task perform several initial experiments demonstrate feasibility concept
human visual systems maintain stable internal representation scene even though image retina constantly changing eye movements such stabilization theoretically effected dynamic shifts receptive field rf neurons visual system this paper examines neural circuit learn generate shifts the shifts controlled eye position signals compensate movement retinal image caused eye movements the development neural shifter circuit olshausen anderson van essen modeled using triadic connections these connections gated signals indicate direction gaze eye position signals in simulations neural model exposed sequences stimuli paired appropriate eye position signals the initially
this paper tries identify rules factors predictive outcome international conflict management attempts we use c advanced machine learning algorithm generating decision trees prediction rules cases confman database the results show simple patterns rules often understandable also reliable complex rules simple decision trees able improve chances correctly predicting outcome conflict management attempt this suggests mediation repetitive conflicts per se results achieved far
c fl uwcc comma technical report no february x no part article may reproduced commercial purposes abstract a technique described allows unimodal function optimization methods extended efficiently locate optima multimodal problems we describe algorithm based traditional genetic algorithm ga this involves iterating ga uses knowledge gained one iteration avoid researching subsequent iterations regions problem space solutions already found this achieved applying fitness derating function raw fitness function fitness values depressed regions problem space solutions already found consequently likelihood discovering new solution iteration dramatically increased the technique may used various styles ga optimization methods simulated annealing the effectiveness algorithm demonstrated number multimodal test functions the technique least fast fitness sharing methods it provides speedup p problem p optima depending value p convergence time complexity
in paper examine intuition td meant operate approximating asynchronous value iteration we note important class discrete acyclic stochastic tasks value iteration inefficient compared dagsp algorithm essentially performs one sweep instead many working backwards goal the question address paper whether analogous algorithm used large stochastic state spaces requiring function approximation we present algorithm analyze give comparative results td several domains state using vi solve mdps belonging either special classes quite inefficient since vi performs backups entire space whereas backups useful improving v fl frontier alreadycorrect notyetcorrect v fl values in fact classical algorithms problem classes compute v fl efficiently explicitly working backwards deterministic class dijkstras shortestpath algorithm acyclic class directedacyclicgraphshortestpaths dagsp dagsp first topologically sorts mdp producing linear ordering states every state x precedes states reachable x then runs list reverse performing one backup per state worstcase bounds vi dijkstra dagsp deterministic domains x states a actionsstate although presents dagsp deterministic acyclic problems applies straightforwardly
automatic design optimization highly sensitive problem formulation the choice objective function constraints design parameters dramatically impact computational cost optimization quality resulting design the best formulation varies one application another a design engineer usually know best formulation advance in order address problem developed system supports interactive formulation testing reformulation design optimization strategies our system includes executable dataflow language representing optimization strategies the language allows engineer define multiple stages optimization using different approximations objective constraints different abstractions design space we also developed set transformations reformulate strategies represented language the transformations approximate objective constraint functions abstractor reparameterize search spaces divide optimization process multiple stages the system applicable principle design problem expressed terms constrained optimization however expect system useful design artifact governed algebraic ordinary differential equations we tested system problems racing yacht design jet engine nozzle design we report experimental results demonstrating reformulation techniques significantly improve performance automatic design optimization our research demonstrates viability reformulation methodology combines symbolic program transformation numerical experimentation it important first step research program aimed automating entire strategy formulation process fl fully accepted research engineering design
the classification performance neural network combined sixband landsattm oneband erssar pri imagery scene carried different combinations data either raw segmented filtered using available ground truth polygons training test sets created the training sets used learning test sets used verification neural network the different combinations evaluated
the problem modeling complicated data sequences dna speech often arises practice most algorithms select hypothesis within model class assuming observed sequence direct output underlying generation process in paper consider case output passes memoryless noisy channel observation in particular show class markov chains variable memory length learning affected factors despite superpolynomial still small practical cases markov models variable memory length probabilistic finite suffix automata introduced learning theory ron singer tishby also described polynomial time learning algorithm we present modification algorithm uses noisecorrupted sample knowledge noise structure the algorithm still viable noise known exactly good estimation available finally experimental results presented removing noise corrupted english text measure performance learning algorithm affected size noisy sample noise rate
we present evaluate implemented system rapidly easily build intelligent software agents webbased tasks our design centered around two basic functions scorethislink scorethispage if given highly accurate functions standard heuristic search would lead efficient retrieval useful information our approach allows users tailor systems behavior providing approximate advice functions this advice mapped neural network implementations two functions subsequent reinforcements web eg dead links ratings retrieved pages user wishes provide respectively used refine link pagescoring functions hence architecture provides appealing middle ground nonadaptive agent programming languages systems solely learn user preferences users ratings pages we describe internal representation web pages major predicates advice language advice mapped neural networks mechanisms refining advice based subsequent feedback we also present case study provide simple advice specialize generalpurpose system homepage finder an empirical study demonstrates approach leads effective homepage finder leading commercial web search site
in concept learning objects domain grouped together based similarity determined attributes used describe existing concept learners require set attributes known advance presented entirety learning begins additionally systems possess mechanisms altering attribute set concepts learned consequently veridical attribute set relevant task concepts used must supplied onset learning turn usefulness concepts limited task attributes originally selected in order efficiently accommodate changing contexts concept learner must able alter set descriptors without discarding prior knowledge domain we introduce notion attributeincrementation dynamic modification attribute set used describe instances problem domain we implemented capability concept learning system evaluated along several dimensions using existing concept formation system com parison
it shown bayesian inference data modeled mixture distribution feasibly performed via monte carlo simulation this method exhibits true bayesian predictive distribution implicitly integrating entire underlying parameter space an infinite number mixture components accommodated without difficulty using prior distribution mixing proportions selects reasonable subset components explain finite training set the need decide correct number components thereby avoided the feasibility method shown empirically simple classification task
this article presents new reinforcement learning method called sane symbiotic adaptive neuroevolution evolves population neurons genetic algorithms form neural network capable performing task symbiotic evolution promotes cooperation specialization results fast efficient genetic search discourages convergence suboptimal solutions in inverted pendulum problem sane formed effective networks times faster adaptive heuristic critic times faster qlearning genitor neuroevolution approach without loss generalization such efficient learning combined domain assumptions make sane promising approach broad range reinforcement learning problems including many realworld applications
the paper concerns probabilistic evaluation plans presence unmeasured variables plan consisting several concurrent sequential actions we establish graphical criterion recognizing effects given plan predicted passive observations measured variables when criterion satisfied closedform expression provided probability plan achieve specified goal
we introduce technique enhance ability dynamic ilp processors exploit speculatively executed parallelism existing branch prediction mechanisms used establish dynamic window ilp extracted limited abilities create large accurate dynamic window ii initiate large number instructions window every cycle iii traverse multiple branches control flow graph per prediction we introduce control flow prediction uses information control flow graph program overcome limitations we discuss information present control flow graph represented using multiblocks conveyed hardware using control flow tables control flow prediction buffers we evaluate potential control flow prediction abstract machine dynamic ilp processing model our results indicate control flow prediction powerful effective assist hardware making informed run time decisions program control flow
we develop mean field theory sigmoid belief networks based ideas statistical mechanics our mean field theory provides tractable approximation true probability distribution networks also yields lower bound likelihood evidence we demonstrate utility framework benchmark problem statistical pattern recognitionthe classification handwritten digits
many learning experience systems use information extracted problem solving experiences modify performance element pe forming new element pe solve similar problems efficiently however transformations improve performance one set problems degrade performance sets new pe always better original pe depends distribution problems we therefore seek performance element whose expected performance distribution optimal unfortunately actual distribution needed determine element optimal usually known moreover task finding optimal element even knowing distribution intractable interesting spaces elements this paper presents method palo sidesteps problems using set samples estimate unknown distribution using set transformations hillclimb local optimum this process based mathematically rigorous form utility analysis particular uses statistical techniques determine whether result proposed transformation better original system we also present efficient way implementing learning system context general class performance elements include empirical evidence approach work effectively fl much work performed university toronto supported institute robotics intelligent systems operating grant national science engineering research council canada we also gratefully acknowledge receiving many helpful comments william cohen dave mitchell dale schuurmans anonymous referees
compositional qlearning cql singh modular approach learning perform composite tasks made several elemental tasks reinforcement learning skills acquired performing elemental tasks also applied solve composite tasks individual skills compete right act winning skills included decomposition composite task we extend original cql concept two ways general reward function agent one actuator we use cql architecture acquire skills performing composite tasks simulated twolinked manipulator large state action spaces the manipulator nonlinear dynamical system require endeffector specific positions workspace fast function approximation qmodules achieved use array cerebellar model articulation controller cmac albus our research interests involve scaling machine learning methods especially reinforcement learning autonomous robot control we interested function approximators suitable reinforcement learning problems large state spaces cerebellar model articulation controller cmac albus permit fast online learning good local generalization in addition interested task decomposition reinforcement learning use hierarchical modular function approximator architectures we examining effectiveness modified hierarchical mixtures experts hme jordan jacobs approach reinforcement learning since original hme developed mainly supervised learning batch learning tasks the incorporation domain knowledge reinforcement learning agents important way extending capabilities default policies specified domain knowledge also used restrict size stateaction space leading faster learning we investigating use qlearning watkins planning tasks using classifier system holland encode necessary conditionaction rules jordan m jacobs r hierarchical mixtures experts em algorithm technical report mit computational cognitive science
the processing performed feedforward neural network often interpreted use decision hyperplanes layer the adaptation process however normally explained using picture gradient descent error landscape in paper dynamics decision hyperplanes used model adaptation process a electromechanical analogy drawn dynamics hyperplanes determined interaction forces hyperplanes particles represent patterns relaxation system determined increasing hyperplane inertia mass this picture used clarify dynamics learning go way explaining learning deadlocks escaping certain local minima furthermore network plasticity introduced dynamic property system reduction necessary consequence information storage hyperplane inertia used explain avoid destructive relearning trained networks
modifications recursive autoassociative memory presented allow store deeper complex data structures previously reported these modifications include adding extra layers compressor reconstructor networks employing integer rather realvalued representations preconditioning weights presetting representations compatible the resulting system tested data set syntactic trees extracted penn treebank
the problem combining preferences arises several applications combining results different search engines this work describes efficient algorithm combining multiple preferences we first give formal framework problem we describe analyze new boosting algorithm combining preferences called rankboost we also describe efficient implementation algorithm restricted case we discuss two experiments carried assess performance rankboost in first experiment used algorithm combine different search strategies query expension given domain for task compare performance rankboost individual search strategies the second experiment collaborativefiltering task specifically making movie recommendations here present results comparing rankboost nearest neighbor regression algorithms
this paper shows decision trees used improve performance casebased learning cbl systems we introduce performance task machine learning systems called semiflexible prediction lies classification task performed decision tree algorithms flexible prediction task performed conceptual clustering systems in semiflexible prediction learning improve prediction specific set features known priori rather single known feature classification arbitrary set features conceptual clustering we describe one task natural language processing present experiments compare solutions problem using decision trees cbl hybrid approach combines two in hybrid approach decision trees used specify features included knearest neighbor case retrieval results experiments show hybrid approach outperforms decision tree casebased approaches well two casebased systems incorporate expert knowledge case retrieval algorithms results clearly indicate decision trees used improve performance cbl systems without reliance potentially expensive expert knowledge
technical report no department statistics university toronto we describe linear network models correlations realvalued visible variables using one realvalued hidden variables factor analysis model this model seen linear version helmholtz machine parameters learned using wakesleep method learning primary generative model assisted recognition model whose role fill values hidden variables based values visible variables the generative recognition models jointly learned wake sleep phases using delta rule this learning procedure comparable simplicity ojas version hebbian learning produces somewhat different representation correlations terms principal components we argue simplicity wakesleep learning makes factor analysis plau sible alternative hebbian learning model activitydependent cortical plasticity
a bayesian method estimating amino acid distributions states hidden markov model hmm protein family columns multiple alignment family introduced this method uses dirichlet mixture densities priors amino acid distributions these mixture densities determined examination previously constructed hmms multiple alignments it shown bayesian method improve quality hmms produced small training sets specific experiments efhand motif reported priors shown produce hmms higher likelihood unseen data fewer false positives false negatives database search task
this paper proposes simple cost model machine learning applications based notion net present value the model extends unifies models used pazzani et al masand piatetskyshapiro it attempts answer question should given machine learning system prototype stage fielded the models inputs systems confusion matrix cash flow matrix application cost per decision onetime cost deploying system rate return investment like provost fawcetts roc convex hull method present model used decisionmaking even input variables known exactly despite simplicity number nontrivial consequences for example free lunch theorems learning theory longer apply
this paper demonstrates use graphs mathematical tool expressing independenices formal language communicating processing causal information statistical analysis we show complex information external interventions organized represented graphically conversely graphical representation used facilitate quantitative predictions effects interventions we first review markovian account causation show directed acyclic graphs dags offer economical scheme representing conditional independence assumptions deducing displaying logical consequences assumptions we introduce manipulative account causation show dag defines simple transformation tells us probability distribution change result external interventions system using transformation possible quantify nonexperimental data effects external interventions specify conditions randomized experiments necessary finally paper offers graphical interpretation rubins model causal effects demonstrates equivalence manipulative account causation we exemplify tradeoffs two approaches deriving nonparametric bounds treatment effects conditions imperfect compliance
in casebased design adaptation design case new design requirements plays important role if sufficient adapt predefined set design parameters task easily automated if however farreaching creative changes required current systems provide limited success this paper describes approach creative design adaptation based notion creativity goal oriented shift focus search process an evolving representation used restructure search space designs similar example case lie focus search this focus used starting point create new designs
we consider novel nonlinear model time series analysis the study model emphasizes theoretical aspects well practical applicability the architecture model demonstrated sufficiently rich sense approximating unknown functional forms yet retains simple intuitive characteristics linear models a comparison established nonlinear models emphasized theoretical issues backed prediction results benchmark time series well computer generated data sets efficient estimation algorithms seen applicable made possible mixture based structure model large sample properties estimators discussed well well specified well misspecified settings we also demonstrate inference pertaining data structure may made parameterization model resulting better intuitive understanding structure performance model
the coverage learning algorithm number concepts learned algorithm samples given size this paper asks whether good learning algorithms designed maximizing coverage the paper extends previous upper bound coverage boolean concept learning algorithm describes two algorithmsmultiballs largeballwhose coverage approaches upper bound experimental measurement coverage id fringe algorithms shows coverage far bound further analysis largeball shows although learns many concepts seem interesting concepts hence coverage maximization alone appear yield practicallyuseful learning algorithms the paper concludes definition coverage within bias suggests way coverage maximization could applied strengthen weak preference biases
at previous foga workshop presented initial results using markov models analyze transient behavior genetic algorithms gas used function optimizers gafos in paper states markov model ordered via simple mathematically convenient lexicographic ordering used initially nix vose in paper explore alternative orderings states based interesting semantic properties average fitness degree homogeneity average attractive force etc we also explore lumping techniques reducing size state space analysis reordered lumped markov models provides new insights transient behavior gas general gafos particular
an important aspect creative design concept emergence though emergence important mechanism either well understood limited domain shapes this deficiency compensated considering definitions emergent behaviour artificial life alife research community with new insights proposed computational technique called evolving representations design genes extended emergent behaviour we demonstrate emergent behaviour coevolutionary model design this coevolutionary approach design allows solution space structure space evolve response problem space behaviour space since behaviour space active participant behaviour may emerge new structures end design process this paper hypothesizes emergent behaviour identified using technique the floor plan example gero schnier extended demonstrate behaviour emerge coevolutionary design process
we investigate learnability pac model data used learning attributes labels either corrupted incomplete in order prove main results define new complexity measure statistical query sq learning algorithms the view sq algorithm maximum queries algorithm number input bits query depends we show restricted view sq algorithm class general sufficient condition learnability models attribute noise covered missing attributes we show since algorithms question statistical also simultaneously tolerate classification noise classes results hold therefore learned simultaneous attribute noise classification noise include kdnf ktermdnf dnf representations conjunctions relevant variables uniform distribution decision lists these noise models first pac models training data attributes labels may corrupted random process previous researchers shown class kdnf learnable attribute noise attribute noise rate known exactly we show attribute noise learnability results either without classification noise also hold exact noise rate known provided learner instead polynomially good approximation noise rate in addition show results also hold one noise rate distinct noise rate attribute our results learning random covering require learner told even approximation covering rate addition hold setting distinct covering rates attribute finally give lower bounds number examples required learning presence attribute noise covering
this study describes new hidden markov model hmm system segmenting uncharacterized genomic dna sequences exons introns intergenic regions separate hmm modules designed trained specific regions dna exons introns intergenic regions splice sites the models tied together form biologically feasible topology the integrated hmm trained set eukaryotic dna sequences tested using segment separate set sequences the resulting hmm system called veil viterbi exonintron locator obtains overall accuracy test data total bases correctly labelled correlation coefficient using stringent test exact exon prediction veil correctly located ends coding exons exons predicts exactly correct these results compare favorably best previous results gene structure prediction demonstrate benefits using hmms problem
recently increased interest lifelong machine learning methods transfer knowledge across multiple learning tasks such methods repeatedly found outperform conventional singletask learning algorithms learning tasks appropriately related to increase robustness approaches methods desirable reason relatedness individual learning tasks order avoid danger arising tasks unrelated thus potentially misleading this paper describes taskclustering tc algorithm tc clusters learning tasks classes mutually related tasks when facing new learning task tc first determines related task cluster exploits information selectively task cluster an empirical study carried mobile robot domain shows tc outperforms nonselective counterpart situations small number tasks relevant
belief revision belief update proposed two types belief change serving different purposes belief revision intended capture changes agents belief state reflecting new information static world belief update intended capture changes belief response changing world we argue belief revision belief update restrictive routine belief change involves elements we present model generalized update allows updates response external changes inform agent prior beliefs this model update combines aspects revision update providing realistic characterization belief change we show certain assumptions original update postulates satisfied we also demonstrate plain revision plain update special cases model way formally verifies intuition revision suitable static belief change
we propose bayesian framework regression problems covers areas usually dealt function approximation an online learning algorithm derived solves regression problems kalman filter its solution always improves increasing model complexity without risk overfitting in infinite dimension limit approaches true bayesian posterior the issues prior selection overfitting also discussed showing commonly held beliefs misleading the practical implementation summarised simulations using popular publicly available data sets used demonstrate method highlight important issues concerning choice priors
this report deals efficient mapping sparse neural networks cns we develop parallel vector code idealized sparse network determine performance three memory systems we use code evaluate memory systems one implemented prototype pinpoint bottlenecks current cns design
in nature genotype many organisms exhibits diploidy ie includes two copies every gene in paper describe results simulations comparing behavior haploid diploid populations ecological neural networks living fixed changing environments we show diploid genotypes create variability fitness population haploid genotypes buffer better environmental change consequence one wants obtain good results average peak fitness single population one choose diploid population appropriate mutation rate some results simulations parallel biological findings
the problem belief changehow agent revise beliefs upon learning new informationhas active area research philosophy artificial intelligence many approaches belief change proposed literature our goal introduce yet another approach examine carefully rationale underlying approaches already taken literature highlight view methodological problems literature the main message study belief change carefully must quite explicit ontology scenario underlying belief change process this something missing previous work focus postulates our analysis shows must pay particular attention two issues often taken granted the first model agents epistemic state do use set beliefs richer structure ordering worlds and use set beliefs language beliefs expressed the second status observations are observations known true believed in latter case firm belief for example argue even postulates called beyond controversy unreasonable agents beliefs include beliefs epistemic state well external world issues status observations arise particularly consider iterated belief revision must confront possibility revising
the study belief change active area philosophy ai in recent years two special cases belief change belief revision belief update studied detail roughly speaking revision treats surprising observation sign previous beliefs wrong update treats surprising observation indication world changed in general would expect agent making observation may want revise earlier beliefs assume change occurred world we define novel approach belief change allows us applying ideas probability theory qualitative settings the key idea use qualitative markov assumption says state transitions independent we show recent approach modeling qualitative uncertainty using plausibility measures allows us make qualitative markov assumption relatively straightforward way show markov assumption used provide attractive beliefchange model
in reinforcement learning frequently necessary resort approximation true optimal value function here investigate benefits online search cases we examine local searches agent performs finitedepth lookahead search global searches agent performs search trajectory way current state goal state the key success methods lies taking value function gives rough solution hard problem finding good trajectories every single state combining online search gives accurate solution easier problem finding good trajectory specifically current state
probabilistic contextfree grammars pcfgs provide simple way represent particular class distributions sentences contextfree language efficient parsing algorithms answering particular queries pcfg ie calculating probability given sentence finding likely parse applied variety patternrecognition problems we extend class queries answered several ways allowing missing tokens sentence sentence fragment supporting queries intermediate structure presence particular nonterminals flexible conditioning variety types evidence our method works constructing bayesian network represent distribution parse trees induced given pcfg the network structure mirrors chart standard parser generated using similar dynamicprogramming approach we present algorithm constructing bayesian networks pcfgs show queries patterns queries network correspond interesting queries pcfgs
this paper presents recent developments toward formalism combines useful properties logic probabilities like logic formalism admits qualitative sentences provides symbolic machinery deriving deductively closed beliefs like probability permits us express ifthen rules different levels firmness retract beliefs response changing observations rules interpreted orderofmagnitude approximations conditional probabilities impose constraints rankings worlds inferences supported unique priority ordering rules syntactically derived knowledge base this ordering accounts rule interactions respects specificity considerations facilitates construction coherent states beliefs practical algorithms developed analyzed testing consistency computing rule ordering answering queries imprecise observations incorporated using qualitative versions jeffreys rule bayesian updating result coherent belief revision embodied naturally tractably finally causal rules interpreted imposing markovian conditions constrain world rankings reflect modularity causal organizations these constraints shown facilitate reasoning causal projections explanations actions change
clay evolutionary architecture autonomous robots integrates motor schemabased control reinforcement learning robots utilizing clay benefit realtime performance motor schemas continuous dynamic environments taking advantage adaptive reinforcement learning clay coordinates assemblages groups motor schemas using embedded reinforcement learning modules the coordination modules activate specific assemblages based presently perceived situation learning occurs robot selects assemblages samples reinforcement signal time experiments robot soccer simulation illustrate performance utility system
most known learning algorithms dynamic neural networks nonstationary environments need global computations perform credit assignment these algorithms either local time local space those algorithms local time space usually deal sensibly hidden units in contrast far judge learning rules biological systems many hidden units local space time in paper propose parallel online learning algorithm performs local computations yet still designed deal hidden units units whose past activations hidden time the approach inspired hollands idea bucket brigade classifier systems transformed run neural network fixed topology the result feedforward recurrent neural dissipative system consuming weightsubstance permanently trying distribute substance onto connections appropriate way simple experiments demonstrating feasability algorithm reported
although creativity largely studied problem solving contexts creativity consists generative component comprehension component in particular creativity essential part reading understanding natural language stories we formalized understanding process developed algorithm capable producing creative understanding behavior we also created novel knowledge organization scheme assist process our model creativity implemented portion isaac integrated story analysis and creativity reading system system models creative reading science fiction stories
creative designers often see solutions pending design problems everyday objects surrounding this often lead innovation insight sometimes revealing new functions purposes common design pieces process we interested modeling serendipitous recognition solutions pending problems context creative mechanical design this paper characterizes ability analyzing observations made placing context forms recognition we propose computational model capture explore serendipitous recognition based ideas reconstructive dynamic memory situation assessment casebased reasoning
in paper analyze two wellknown measures attribute selection decision tree induction informativity gini index in particular interested influence different methods estimating probabilities two measures the results experiments show different measures obtained different probability estimation methods determine preferential order attributes given node therefore determine structure constructed decision tree this feature beneficial especially realworld applications several different trees often required
we consider learning situations function used classify examples may switch back forth small number different concepts course learning we examine several models situations oblivious models switches made independent selection examples adversarial models single adversary controls concept switches example selection we show relationships benign models pconcepts kearns schapire present polynomialtime algorithms learning switches two kdnf formulas for adversarial model present model success patterned popular competitive analysis used studying online algorithms we describe randomized query algorithm adversarial switches two monotone disjunctions competitive total number mistakes plus queries high probability bounded number switches plus fixed polynomial n number variables we also use notions described provide sufficient conditions learning pconcept class decision rule implies able learn class model probability
case based systems typically retrieve cases case base applying similarity measures the measures usually constructed ad hoc manner this report presents toolbox systematic construction similarity measures in addition paving way design methodology similarity measures systematic approach facilitates identification opportunities parallelisation case base retrieval
in real world applications software engineers recognise use memory must organised via data structures software using data must independant data structures implementation details they achieve using abstract data structures records files buffers we demonstrate genetic programming automatically implement simple abstract data structures considering detail task evolving list we show general reasonably efficient implementations automatically generated simple primitives a model maintaining evolved code demonstrated using list problem much published work genetic programming gp evolves functions without sideeffects learn patterns test data in contrast human written programs often make extensive explicit use memory indeed memory form required programming system turing complete ie possible write computable program system however inclusion memory make interactions parts programs much complex make harder produce programs despite shown gp automatically create programs explicitly use memory teller in normal genetic programming considerable benefits found adopting structured approach for example koza shows introduction evolvable code modules automatically defined functions adfs greatly help gp reach solution we suggest corresponding structured approach use data similarly significant advantage gp earlier work demonstrated genetic programming automatically generate simple abstract data structures namely stacks queues langdon that gp evolve programs organise memory accessed via simple read write primitives data structures used external software without needing know implemented this chapter shows possible evolve list data structure basic primitives aho hopcroft ullman suggest three different ways implement list experiments show gp evolve implementation this requires list components agree one implementation coevolve together section describes gp architecture including use pareto multiple component fitness scoring measures aimed speeding gp search the evolved solutions described section section presents candidate model maintaining evolved software this followed discussion learned conclusions drawn
report sycon abstract we pursue particular approach analog computation based dynamical systems type used neural networks research our systems fixed structure invariant time corresponding unchanging number neurons if allowed exponential time computation turn unbounded power however polynomialtime constraints limits capabilities though powerful turing machines a similar restricted model shown polynomialtime equivalent classical digital computation previous work moreover precise correspondence nets standard nonuniform circuits equivalent resources consequence one lower bound constraints compute this relationship perhaps surprising since analog devices change manner input size we note networks likely solve polynomially nphard problems equality p np model implies almost complete collapse standard polynomial hierarchy
we introduce convergence diagnostic procedure mcmc operates estimating total variation distances distribution algorithm certain numbers iterations the method advantages many existing methods terms applicability utility computational expense interpretability it used assess convergence marginal joint posterior densities show applied two commonly used mcmc samplers gibbs sampler metropolis hastings algorithm illustrative examples highlight utility interpretability proposed diagnostic also highlight limitations
because distance skull brain different resistivities electroencephalographic eeg data collected point human scalp includes activity generated within large brain area this spatial smearing eeg data volume conduction involve significant time delays however suggesting independent component analysis ica algorithm bell sejnowski suitable performing blind source separation eeg data the ica algorithm separates problem source identification source localization first results applying ica algorithm eeg eventrelated potential erp data collected sustained auditory detection task show ica training insensitive different random seeds ica may used segregate obvious artifactual eeg components line muscle noise eye movements sources ica capable isolating overlapping eeg phenomena including alpha theta bursts spatiallyseparable erp components separate ica channels nonstationarities eeg behavioral state tracked using ica via changes amount residual correlation icafiltered output channels
miller g the magical number seven plus minus two some limits capacity processing information the psychological review schmidhuber j b towards compositional learning dynamic neural networks technical report fki technische universitat munchen institut fu informatik servanschreiber d cleermans a mcclelland j encoding sequential structure simple recurrent networks technical report cmucs carnegie mellon university computer science department
the standard approach decision tree induction topdown greedy algorithm makes locally optimal irrevocable decisions node tree in paper study alternative approach algorithms use limited lookahead decide test use node we systematically compare using large number decision trees quality decision trees induced greedy approach trees induced using lookahead the main results experiments greedy approach produces trees accurate trees produced much expensive lookahead step ii decision tree induction exhibits pathology sense lookahead produce trees larger less accurate trees produced without
this thesis presents machine learning model capable extracting discrete classes continuous valued input features this done using neurally inspired novel competitive classifier cc feeds discrete classifications forward supervised machine learning model the supervised learning model uses discrete classifications perhaps information available solve problem the supervised learner generates feedback guide cc potentially useful classifications continuous valued input features two supervised learning models combined cc creating asocsafe idafe both models simulated results analyzed based results several areas future research proposed
we frequently called upon perform multiple tasks compete attention resource often know optimal solution task isolation paper describe knowledge exploited efficiently find good solutions tasks parallel we formulate problem dynamically merging multiple markov decision processes mdps composite mdp present new theoreticallysound dynamic programming algorithm finding optimal policy composite mdp we analyze various aspects algorithm every day faced problem multiple tasks parallel competes attention resource if running job shop must decide machines allocate jobs order jobs miss deadlines if mail delivery robot must find intended recipients mail simultaneously avoiding fixed obstacles walls mobile obstacles people still manage keep sufficiently charged frequently know perform task isolation paper considers take information individual tasks combine efficiently find optimal solution entire set tasks parallel more importantly describe theoreticallysound algorithm merging dynamically new tasks new job arrival job shop assimilated online solution found ongoing set simultaneous tasks illustrate use simple merging problem
we consider problem fitting n fi n distance matrix d tree metric t let distance closest tree metric min t fk t d k g first present on algorithm finding additive tree t k t d k giving first algorithm problem performance guarantee second show n phard find tree t k t d k lt
casebased planning cbp provides way scaling domainindependent planning solve large problems complex domains it replaces detailed lengthy search solution retrieval adaptation previous planning experiences in general cbp demonstrated improve performance generative fromscratch planning however performance improvements provides dependent adequate judgements problem similarity in particular although cbp may substantially reduce planning effort overall subject misretrieval problem the success cbp depends retrieval errors relatively rare this paper describes design implementation replay framework casebased planner dersnlpebl dersnlpebl extends current cbp methodology incorporating explanationbased learning techniques allow explain learn retrieval failures encounters these techniques used refine judgements case similarity response feedback wrong decision made the failure analysis used building case library addition repairing cases large problems split stored single goal subproblems multigoal problems stored smaller cases fail merged full solution an empirical evaluation approach demonstrates advantage learning experienced retrieval failure
modern processors improve instruction level parallelism speculation the outcome data control decisions predicted operations speculatively executed committed original predictions correct there number ways processor resources could used threading eager execution as use speculation increases believe processors need form speculation control balance benefits speculation possible activities confidence estimation one technique exploited architects speculation control in paper introduce performance metrics compare confidence estimation mechanisms argue metrics appropriate speculation control we compare number confidence estimation mechanisms focusing mechanisms small implementation cost gain benefit exploiting characteristics branch predictors clustering mispredicted branches we compare performance different confidence estimation methods using detailed pipeline simulations using simulations show improve confidence estimators providing better insight future investigations comparing applying confidence estimators
relational learning algorithms special interest members machine learning community offer practical methods extending representations used algorithms solve supervised learning tasks five approaches currently explored address issues involved using relational representations this paper surveys algorithms embodying approaches summarizes empirical evaluations highlights commonalities suggests potential directions future research
the learning process boltzmann machines computationally expensive the computational complexity exact algorithm exponential number neurons we present new approximate learning algorithm boltzmann machines based mean field theory linear response theorem the computational complexity algorithm cubic number neurons in absence hidden units show weights directly computed fixed point equation learning rules thus case need use gradient descent procedure learning process we show solutions method close optimal solutions give significant improvement correlations play significant role finally apply method pattern completion task show good performance networks neurons
this paper introduces methodology solving combinatorial optimization problems application reinforcement learning methods the approach applied cases several similar instances combinatorial optimization problem must solved the key idea analyze set training problem instances learn search control policy solving new problem instances the search control policy twin goals finding highquality solutions finding quickly results applying methodology nasa scheduling problem show learned search control policy much effective best known nonlearning search procedurea method based simulated annealing
markov decision processes mdps undiscounted rewards represent important class problems decision control the goal learning mdps find policy yields maximum expected return per unit time in large state spaces computing averages directly feasible instead agent must estimate stochastic exploration state space in case longer exploration times enable accurate estimates informed decisionmaking the learning curve mdp measures agents performance depends allowed exploration time t in paper analyze learning curves simple control problem undiscounted rewards in particular methods statistical mechanics used calculate lower bounds agents performance thermodynamic limit t n ff t n finite t number time steps allotted per policy evaluation n size state space in limit provide lower bound return policies appear optimal based imperfect statistics
one effectively utilize predicated execution improve branch handling instructionlevel parallel processors although potential benefits predicated execution high tradeoffs involved design instruction set support predicated execution difficult on one end design spectrum architectural support full predicated execution requires increasing number source operands instructions full predicate support provides flexibility largest potential performance improvements on end partial predicated execution support conditional moves requires little change existing architectures this paper presents preliminary study qualitatively quantitatively address benefit full partial predicated execution support with current compiler technology show compiler use partial full predication achieve speedup large controlintensive programs some details code generation techniques shown provide insight benefit going partial full predication preliminary experimental results encouraging partial predication provides average performance improvement issue processor predicate support full predication provides additional improvement
this paper studies selfdirected learning variant online learning model learner selects presentation order instances we give tight bounds complexity selfdirected learning concept classes monomials kterm dnf formulas orthogonal rectangles f ng these results demonstrate number mistakes selfdirected learning surprisingly small we prove model selfdirected learning powerful commonly used online query learning models next explore relationship complexity selfdirected learning vapnikchervonenkis dimension finally explore relationship mitchells version space algorithm existence selfdirected learning algorithms make mistakes fl supported part ge foundation junior faculty grant nsf grant ccr part research conducted author mit laboratory computer science supported nsf grant dcr grant siemens corporation net address sgcswustledu
a lowerbound result power abstract this paper presents lowerbound result computational power genetic algorithm context combinatorial optimization we describe new genetic algorithm merged genetic algorithm prove class monotonic functions algorithm finds optimal solution exponential convergence rate the analysis pertains ideal behavior algorithm main task reduces showing convergence probability distributions search space combinatorial structures optimal one we take exponential convergence indicative efficient solvability samplebounded algorithm although sampling theory needed better relate limit behavior actual behavior the paper concludes discussion immediate problems lie ahead genetic algorithm
in paper study forecasting model based mixture experts predicting french electric daily consumption energy we split task two parts using mixture experts first model predicts electricity demand exogenous variables temperature degree cloud cover viewed nonlinear regression model mixture gaussians using single neural network second model predicts evolution residual error first one viewed nonlinear autoregression model we analyze splitting input space generated mixture experts model compare performance models presently used
suttons td metho aims provide represen tation cost function absorbing mark ov chain transition costs a simple example given represen tation obtained dep ends for represen tation optimal resp ect least squares error criterion decreases towards represen tation becomes progressiv ely worse cases poor the example suggests need understand better circumstances td qlearning obtain satisfactory neural net workbased compact represen tations cost function a variation td also prop osed performs b etter example
casebased reasoning involves reasoning cases specific pieces experience reasoners anothers used solve problems we use term graphstructured representations capable expressing relations two objects case allow set relations used vary case case allow set possible relations expanded necessary describe new cases such representations implemented example semantic networks lists concrete propositions logic we believe graphstructured representations offer significant advantages thus investigating ways implement representations efficiently we make casebased argument using examples two systems chiron caper show graphstructured representation supports two different kinds casebased planning two different domains we discuss costs associated graphstructured representations describe approach reducing costs imple mented caper
the advantage using linear regression leaves regression tree analysed paper it carried modification affects construction pruning interpretation regression tree the modification tested artificial reallife domains impact classification error stability induced trees considered the results show modification beneficial leads smaller classification errors induced regression trees the bayesian approach estimation class distributions used experiments
general theory quantitative results abstract the human genotype represents ten billion binary informations whereas human brain contains million times billion synapses so differentiated brain structure essentially due selforganization such selforganization relevant areas ranging medicine design intelligent complex systems many brain structures emerge collective phenomenon microscopic neurosynaptic dynamics stochastic dynamics mimics neuronal action potentials synaptic dynamics modeled local coupling dynamics type hebbrule synaptic efficiency increases coincident spiking pre postsynaptic neuron the microscopic dynamics transformed collective dynamics reminiscent hydrodynamics the theory models empirical findings quantitatively topology preserving neuronal maps assumed descartes selforganization suggested weiss empirical observation reported marshall shown neurosynaptically stable due ubiquitous infinitesimal short range electrical chemical leakage in visual cortex neuronal stimulus orientation preference emerges empirically measured orientation patterns determined poisson equation electrostatics poisson equation orientation pattern emergence derived complex cognitive abilities emerge basic local synaptic changes regulated valuation emergent valuation attention attention focus combination subnetworks altogether general theory presented emergence functionality synaptic growth neurobiological systems the theory provides transformation collective dynamics used quantitative modeling empirical data
a methodology evaluating theory revision systems results abstract theory revision systems learning systems goal making small changes original theory account new data a measure distance two theories proposed this measure corresponds minimum number edit operations literal level required transform one theory another by computing distance original theory revised theory claim theory revision system makes revisions theory may quantitatively evaluated we present data using accuracy distance metric audrey ii audrey ii fl
we present novel data mining approach based decomposition in order analyze given dataset method decomposes hierarchy smaller less complex datasets analyzed independently the method experimentally evaluated realworld housing loans allocation dataset showing decomposition discover meaningful intermediate concepts decompose relatively complex dataset datasets easy analyze comprehend derive classifier high classification accuracy we also show human interaction positive effect comprehensibility classification accuracy
most empirical evaluations machine learning algorithms case studies evaluations multiple algorithms multiple databases authors case studies implicitly explicitly hypothesize pattern results often suggests one algorithm performs significantly better others limited small number databases investigated instead holds general class learning problems however hypotheses rarely supported additional evidence leaves suspect this paper describes empirical method generalizing results case studies example application this method yields rules describing algorithms significantly outperform others dependent measures advantages generalizing case studies limitations particular approach also described
learning multiple descriptions class data shown reduce generalization error amount error reduction varies greatly domain domain this paper presents novel empirical analysis helps understand variation our hypothesis amount error reduction linked degree descriptions class make errors correlated manner we present precise novel definition notion use twentynine data sets show amount observed error reduction negatively correlated degree descriptions make errors correlated manner we empirically show possible learn descriptions make less correlated errors domains many ties search evaluation measure eg information gain experienced learning the paper also presents results help understand multiple descriptions help irrelevant attributes much help large amounts class noise
this paper presents plannett system combines artificial neural networks achieve expert level accuracy difficult scientific task recognizing volcanos radar images surface planet venus plannett uses anns vary along two dimensions set input features used train number hidden units the anns combined simply averaging output activations when plannett used classification module threestage image analysis system called jar tool endtoend accuracy sensitivity specificity good human planetary geologist fourimage test suite jartoolplannett also achieves best algorithmic accuracy images date
planning learning multiple levels temporal abstraction key problem artificial intelligence in paper summarize approach problem based mathematical framework markov decision processes reinforcement learning conventional modelbased reinforcement learning uses primitive actions last one time step modeled independently learning agent these generalized macro actions multistep actions specified arbitrary policy way completing macro actions generalize classical notion macro operator closed loop uncertain variable duration macro actions needed represent commonsense higherlevel actions going lunch grasping object traveling distant city this paper generalizes prior work temporally abstract models sutton extends prediction setting include actions control planning we define semantics models macro actions guarantees validity planning using models this paper present new results theory planning macro actions illustrates potential advantages gridworld task
this paper reviews five approximate statistical tests determining whether one learning algorithm outperforms another particular learning task these tests compared experimentally determine probability incorrectly detecting difference difference exists type i error two widelyused statistical tests shown high probability type i error certain situations never used these tests test difference two proportions b paireddifferences test based taking several random traintest splits a third test paireddifferences test based fold crossvalidation exhibits somewhat elevated probability type i error a fourth test mcnemars test shown low type i error the fifth test new test xcv based iterations fold crossvalidation experiments show test also acceptable type i error the paper also measures power ability detect algorithm differences exist tests the crossvalidated test powerful the xcv test shown slightly powerful mcnemars test the choice best test determined computational cost running learning algorithm for algorithms executed mcnemars test test acceptable type i error for algorithms executed ten times xcv test recommended slightly powerful directly measures variation due choice training set
many classification algorithms passive assign classlabel instance based description given even description incomplete in contrast active classifier cost obtain values missing attributes deciding upon class label the expected utility using active classifier depends cost required obtain additional attribute values penalty incurred outputs wrong classification this paper considers problem learning nearoptimal active classifiers using variant probablyapproximatelycorrect pac model after defining framework perhaps main contribution paper describe situation task achieved efficiently show task often intractable
probabilistic inference algorithms finding probable explanation maximum aposteriori hypothesis maximum expected utility updating belief reformulated eliminationtype algorithm called bucket elimination this emphasizes principle common many algorithms appearing literature clarifies relationship nonserial dynamic programming algorithms we also present general way combining conditioning elimination within framework bounds complexity given algorithms function problems struc ture
this article published sociological methodology edited peter v marsden cambridge mass blackwells adrian e raftery professor statistics sociology department sociology dk university washington seattle wa this research supported nih grant rhd i would like thank robert hauser michael hout steven lewis scott long diane lye peter marsden bruce western yu xie two anonymous reviewers detailed comments earlier version i also grateful clem brooks sir david cox tom diprete john goldthorpe david grusky jennifer hoeting robert kass david madigan michael sobel chris volinsky helpful discussions correspondence
in paper propose family algorithms combining treeclustering conditioning trade space time such algorithms useful reasoning probabilistic deterministic networks well accomplishing optimization tasks by analyzing problem structure possible select spectrum algorithm best meets given timespace specifica tion
in paper propose new approach probabilistic inference belief networks global conditioning simple generalization pearls b method loopcutset conditioning we show global conditioning well loopcutset conditioning thought special case method lauritzen spiegelhalter refined jensen et al b nonetheless approach provides new opportunities parallel processing case sequential processing tradeoff time memory we also show hybrid method suermondt others combining loopcutset conditioning jensens method viewed within framework by exploring relationships methods develop unifying framework advantages approach combined successfully
the dynamics collective properties feedback networks spiking neurons investigated special emphasis given potential computational role subthreshold oscillations it shown model systems integrateandfire neurons function associative memories two distinct levels on first level binary patterns represented spike activity fire fire on second level analog patterns encoded relative firing times individual spikes spikes underlying subthreshold oscillation both coding schemes may coexist within network the results suggest cortical neurons may perform broad spectrum associative computations far beyond scope traditional firingrate picture
this paper considers new method maintaining diversity creating subpopulations standard generational evolutionary algorithm unlike methods replaces concept distance individuals tag bits identify subpopulation individual belongs two variations method presented illustrating feasibility approach
ideally pattern recognition machines provide constant output inputs transformed group g desired invariances these invariances achieved enhancing training data include examples inputs transformed elements g leaving corresponding targets unchanged alternatively cost function training include regularization term penalizes changes output input transformed group this paper relates two approaches showing precisely sense regularized cost function approximates result adding transformed distorted examples training data the cost function enhanced training set equivalent sum original cost function plus regularizer for unbiased models regularizer reduces intuitively obvious choice term penalizes changes output inputs transformed group for infinitesimal transformations coefficient regularization term reduces variance distortions introduced training data this correspondence provides simple bridge two approaches
a new method proposed exploiting causal independencies exact bayesian network inference a bayesian network viewed representing factorization joint probability multiplication set conditional probabilities we present notion causal independence enables one factorize conditional probabilities combination even smaller factors consequently obtain finergrain factorization joint probability the new formulation causal independence lets us specify conditional probability variable given parents terms associative commutative operator sum max contribution parent we start simple algorithm ve bayesian network inference given evidence query variable uses factorization find posterior distribution query we show algorithm extended exploit causal independence empirical studies based cpcs networks medical diagnosis show method efficient previous methods allows inference larger networks previous algorithms
our goal develop hybrid cognitive model humans acquire skills complex cognitive tasks we pursuing goal designing hybrid computational architectures nrl navigation task requires competent sensorimotor coordination in paper describe results directly fitting human execution data task we next present empirically compare two methods modeling control knowledge acquisition reinforcement learning novel variant action models human learning task the paper concludes experimental demonstration impact background knowledge system performance our results indicate performance action models approach closely approximates rate human learning task reinforcement learning
the statistical query learning model viewed tool creating demonstrating existence noisetolerant learning algorithms pac model the complexity statistical query algorithm conjunction complexity simulating sq algorithms pac model noise determine complexity noisetolerant pac algorithms produced although roughly optimal upper bounds shown complexity statistical query learning corresponding noisetolerant pac algorithms optimal due inefficient simulations in paper provide improved simulations new variant statistical query model order overcome inefficiencies we improve time complexity classification noise simulation statistical query algorithms our new simulation roughly optimal dependence noise rate we also derive simpler proof statistical queries simulated presence classification noise this proof makes fewer assumptions queries therefore allows one simulate general types queries we also define new variant statistical query model based relative error show variant natural strictly powerful standard additive error model we demonstrate efficient pac simulations algorithms new model give general upper bounds learning relative error statistical queries pac simulation we show statistical query algorithm simulated pac model malicious errors way resultant pac algorithm roughly optimal tolerable malicious error rate sample complexity finally generalize types queries allowed statistical query model we discuss advantages allowing generalized queries show results improved simulations also hold queries this paper available center research computing technology division applied sciences harvard university technical report tr
this paper outlines problems may occur reduced error pruning relational learning algorithms notably efficiency thereafter new method incremental reduced error pruning proposed attempts address problems experiments show many noisy domains method much efficient alternative algorithms along slight gain accuracy however experiments show well use algorithm recommended domains require specific concept description
one kind prosodic structure apparently underlies music language meter yet detailed measurements music speech show nested periodicities define metrical structure noisy sense what kind system could produce perceive variable metrical timing and would take store particular metrical patterns longterm memory system we developed network coupled oscillators produces perceives metrical patterns pulses in addition beginning initial state biases learns prefer beat patterns like waltzes beat patterns models general class could learn entrain musical patterns and given way process speech extract appropriate pulses model applicable metrical structure speech well is language metrical meter refers particular sorts patterns time abstract description patterns potentially cognitive representation in cases two hierarchical levels equally spaced events occur periods characterizing levels integral multiples usually the hierarchy implied standard western musical notation different levels indicated kinds notes quarter notes half notes etc bars separating measures for example basic waltztime meter individual beats spacing grouped sets three every third one receiving stronger accent in meter hierarchy consisting faster periodic cycle beat level slower one measure level fast onset zero phase angle coinciding zero phase angle every third beat metrical systems like seem underlie forms music around world often said underlie human speech well jones martin however awkward difficulty definition employs notion integer since data music speech show clearly perfect temporal ratios predicted definition observed performance in music performance various kinds systematic temporal deviations timing specified musical notation known
we propose model abduction based revision epistemic state agent explanations must sufficient induce belief sentence explained instance observation ensure consistency beliefs manner adequately accounts factual hypothetical sentences our model generate explanations nonmonotonically predict observation thus generalizing current accounts require deductive relationship explanation observation it also provides natural preference ordering explanations defined terms normality plausibility to illustrate generality approach reconstruct two key paradigms modelbased diagnosis abductive consistencybased diagnosis within framework this reconstruction provides alternative semantics extends systems accommodate predictive explanations semantic preferences explanations it also illustrates general information incorporated principled manner fl some parts paper appeared preliminary form abduction belief revision a model preferred explanations proc eleventh national conf artificial intelligence aaai washington dc pp
report r isrn sicsrse issn abstract the optimal probability activation corresponding performance studied three designs sparse distributed memory namely kanervas original design jaeckels selectedcoordinates design karlssons modifi cation jaeckels design we assume hard locations karlssons case masks storage addresses stored data randomly chosen consider different levels random noise reading address
report r isrn sicsrse issn abstract we consider sparse distributed memory randomly chosen hard locations unknown number t random data vectors stored a method given estimate t content memory high accuracy in fact estimate unbiased coefficient variation roughly inversely proportional p mu m number hard locations memory u length data accuracy made arbitrarily high making memory big enough a consequence good reading methods used without need special extra location introduced
we describe rankedmodel semantics ifthen rules admitting exceptions provides coherent framework many facets evidential causal reasoning rule priorities automatically extracted form knowledge base facilitate construction retraction plausible beliefs to represent causation formalism incorporates principle markov shielding imposes stratified set independence constraints rankings interpretations we show formalism resolves classical problems associated specificity prediction abduction offers natural way unifying belief revision belief update reasoning actions
we build mathematical connection expectationmaximization em algorithm gradientbased approaches maximum likelihood learning finite gaussian mixtures we show em step parameter space obtained gradient via projection matrix p provide explicit expression matrix we analyze convergence em terms special properties p provide new results analyzing effect p likelihood surface based mathematical results present comparative discussion advantages disadvantages em algorithms learning gaussian mixture models
a first order regression algorithm capable handling realvalued continuous variables introduced applications presented regressional learning assumes realvalued class discrete realvalued variables the algorithm combines regressional learning standard ilp concepts first order concept description background knowledge a clause generated successively refining initial clause adding literals form a v discrete attributes a v a v realvalued attributes background knowledge literals clause body the algorithm employs covering approach beam search heuristic impurity function stopping criteria based local improvement minimum number examples maximum clause length minimum local improvement minimum description length allowed error variable depth an outline algorithm results systems application artificial realworld domains presented the realworld domains comprise modelling water behavior surge tank modelling workpiece roughness steel grinding process modelling operators behavior process electrical discharge machining special emphasis given evaluation obtained models domain experts comments aspects practical use induced knowledge the results obtained knowledge acquisition process show several important guidelines knowledge acquisition concerning mainly process interaction domain experts exposing primarily importance comprehensibility induced knowledge
we address problem measuring degree hemispheric organization asymmetry organization computational model bihemispheric cerebral cortex a theoretical framework measures developed used produce algorithms measuring degree organization symmetry lateralization topographic map formation the performance resulting measures tested several topographic maps obtained selforganization initially random network results compared subjective assessments made humans it found closest agreement human assessments obtained using organization measures based sigmoidtype error averaging measures developed correct large constant displacements well curving hemispheric topographic maps
learning structure temporallyextended sequences difficult computational problem fraction relevant information available instant although variants back propagation principle used find structure sequences practice sufficiently powerful discover arbitrary contingencies especially spanning long temporal intervals involving high order statistics for example designing connectionist network music composition encountered problem net able learn musical structure occurs locally timeeg relations among notes within musical phrasebut structure occurs longer time periodseg relations among phrases to address problem require means constructing reduced description sequence makes global aspects explicit readily detectable i propose achieve using hidden units operate different time constants simulation experiments indicate slower timescale hidden units able pick global structure structure simply learned standard many patterns world intrinsically temporal eg speech music unfolding events recurrent neural net architectures devised accommodate timevarying sequences for example architecture shown figure map sequence inputs sequence outputs learning structure temporallyextended sequences difficult computational problem input pattern may contain taskrelevant information instant thus back propagation
an incremental higherorder nonrecurrent neuralnetwork combines two properties found useful sequence learning neuralnetworks higherorder connections incremental introduction new units the incremental higherorder neuralnetwork adds higher orders needed adding new units dynamically modify connection weights the new units modify weights next timestep information previous step since theoretically unlimited number units added network information arbitrarily distant past brought bear prediction temporal tasks thereby learned without use feedback contrast recurrent neuralnetworks because recurrent connections training simple fast experiments demonstrated speedups two orders magnitude recurrent networks
in complex models like hidden markov chains convergence mcmc algorithms used approximate posterior distribution bayes estimates parameters interest must controlled robust manner we propose paper series online controls rely classical nonparametric tests evaluate independence startup distribution stability markov chain asymptotic normality these tests lead graphical control spreadsheets presented setup normal mixture hidden markov chains compare full gibbs sampler aggregated gibbs sampler based forwardbackward formulae
three different methods investigated determine ability detect classify various categories diffuse liver disease a statistical method ie discriminant analysis supervised neural network called backpropagation nonsupervised selforganizing feature map examined the investigation performed basis previously selected set acoustic image texture parameters the limited number patients successfully extended generating additional independent data identical statistical properties the generated data used training test sets the final test made original patient data validation set it concluded neural networks attractive alternative traditional statistical techniques dealing medical detection classification tasks moreover use generated data training networks discriminant classifier shown justified profitable
nonlinear extensions oneunit multiunit principal component analysis pca neural networks introduced earlier authors reviewed the networks nonlinear hebbian learning rules related signal expansions like projection pursuit pp independent component analysis ica separation results mixtures real world signals im ages given
many hormones physiological processes vary circadian pattern although sinecosine function used model patterns functional form appropriate asymmetry peak nadir phases in paper describe semiparametric periodic spline function fit circadian rhythms the model includes phase amplitude time magnitude peak nadir estimated we also describe tests fit components model data experiment study immunological responses humans used demonstrate methods
we present highlevel decompositionbased algorithms largescale blockangular optimization problems containing integer variables demonstrate effectiveness solution largescale graph partitioning problems these algorithms combine subproblemcoordination paradigm lower bounds pricedirective decomposition methods knapsack genetic approaches utilization building blocks partial solutions even graph partitioning problems requiring billions variables standard formulation approach produces highquality solutions measured deviations easily computed lower bound substantially outperforms widelyused graph partitioning techniques based heuristics spectral methods
maps regional morbidity mortality rates useful tools determining spatial patterns disease combined sociodemographic census information also permit assessment environmental justice ie whether certain subgroups suffer disproportionately certain diseases adverse effects harmful environmental exposures bayes empirical bayes methods proven useful smoothing crude maps disease risk eliminating instability estimates lowpopulation areas maintaining geographic resolution in paper extend existing hierarchical spatial models account temporal effects spatiotemporal interactions fitting resulting highlyparametrized models requires careful implementation markov chain monte carlo mcmc methods well novel techniques model evaluation selection we illustrate approach using dataset countyspecific lung cancer rates state ohio period
a novel unsupervised neural network dimensionality reduction seeks directions emphasizing multimodality presented connection exploratory projection pursuit methods discussed this leads new statistical insight synaptic modification equations governing learning bienenstock cooper munro bcm neurons the importance dimensionality reduction principle based solely distinguishing features demonstrated using phoneme recognition experiment the extracted features compared features extracted using backpropagation network
this paper reprinted computational learning theory natural learning systems vol t petsche s hanson j shavlik eds copyrighted mit press abstract the ability inductive learning system find good solution given problem dependent upon representation used features problem a number factors including trainingset size ability learning algorithm perform constructive induction mediate effect input representation accuracy learned concept description we present experiments evaluate effect input representation generalization performance realworld problem finding genes dna our experiments demonstrate two different input representations task result significantly different generalization performance neural networks decision trees neural symbolic methods constructive induction fail bridge gap two representations we believe realworld domain provides interesting challenge problem machine learning subfield constructive induction relationship two representations well known conceptually representational shift involved constructing better representation imposing
in paper emphasize role selection evolutionary algorithms we briefly review common selection schemes fields genetic algorithms evolution strategies genetic programming however classify selection schemes according group evolutionary algorithm belong rather distinguish parent selection schemes global competition replacement schemes local competition replacement schemes this paper intend fully review analyse presented selection schemes tries short reference standard advanced selection schemes
selfsupervised backpropagation unsupervised learning procedure feedforward networks desired output vector identical input vector for backpropagation able use powerful simulators running parallel machines topologypreserving maps hand developed variant competitive learning procedure however degenerate case selfsupervised backpropagation version competitive learning a simple extension cost function backpropagation leads competitive version selfsupervised backpropagation used produce topographic maps we demonstrate approach applied traveling salesman problem tsp the algorithm implemented using backpropagation simulator clones parallel machine rap
this paper describes evolving computational model perception production simple rhythmic patterns the model consists network oscillators different resting frequencies couple input patterns oscillators whose frequencies match periodicities input tend become activated metrical structure represented explicitly network form clusters oscillators whose frequencies phase angles constrained maintain harmonic relationships characterize meter rests rhythmic patterns represented explicit rest oscillators network become activated expected beat pattern fails appear the model makes predictions relative difficulty the nested periodicity defines musical probably also linguistic meter appears fundamental way people perceive produce patterns time meter however sufficient describe patterns interesting memorable deviate metrical hierarchy the simplest deviations rests gaps one levels hierarchy would normally beat when beats removed regular intervals match period level metrical hierarchy call simple rhythmic pattern figure shows example simple rhythmic pattern below grid representation meter behind pattern patterns effect deviations periodicity input
in paper generalize several results uniform approximation orders radial basis functions buhmann dyn levin dyn ron l p approximation orders these results apply particular approximants spaces spanned translates radial basis functions scattered centres examples results apply include quasiinterpolation leastsquares approximation radial function spaces
the paper studies l ir norm approximations space spanned discrete set translates basis function attention restricted functions whose fourier transform smooth ir n singularity origin examples basis functions thinplate splines multiquadrics well types radial basis functions employed approximation theory the approximation problem wellunderstood case set points ffi used translating forms lattice ir many optimal quasioptimal approximation schemes already found literature in contrast mostly specific results known set ffi scattered points the main objective paper provide general tool extending approximation schemes use integer translates basis function nonuniform case we introduce single relatively simple conversion method preserves approximation orders provided large number schemes presently literature precisely almost stationary schemes in anticipation future introduction new schemes uniform grids effort made impose mild conditions function still allow unified error analysis hold in course discussion recent results budl scattered center approximation reproduced improved upon
an upper bound l p approximation power p provided principal shiftinvariant spaces derived mild assumptions generator it applies stationary nonstationary ladders shown apply spaces generated exponential box splines polyharmonic splines multiquadrics gauss kernel
in speeduplearning problems full descriptions operators always known explanationbased learning ebl reinforcement learning rl applied this paper shows methods involve fundamentally process propagating information backward goal toward starting state rl performs propagation statebystate basis ebl computes weakest preconditions operators hence performs propagation regionbyregion basis based observation rl form asynchronous dynamic programming paper shows develop dynamic programming version ebl call explanationbased reinforcement learning ebrl the paper compares batch online versions ebrl batch online versions rl standard ebl the results show ebrl combines strengths ebl fast learning ability scale large state spaces strengths rl learning optimal policies results shown chess endgames synthetic maze tasks
in paper present extensions kmeans algorithm vector quantization permit efficient use image segmentation pattern classification tasks it shown introducing state variables correspond certain statistics dynamic behavior algorithm possible find representative centers lower dimensional manifolds define boundaries classes clouds multidimensional multiclass data permits one example find class boundaries directly sparse data eg image segmentation tasks efficiently place centers pattern classification eg local gaussian classifiers the state variables used define algorithms determining adaptively optimal number centers clouds data spacevarying density some examples application extensions also given this report describes research done within cimat guanajuato mexico center biological computational learning department brain cognitive sciences artificial intelligence laboratory this research sponsored grants office naval research contracts nj nj grant national science foundation contract asc grant national institutes health contract nih srr additional support provided north atlantic treaty organization atr audio visual perception research laboratories mitsubishi electric corporation sumitomo metal industries siemens ag support ai laboratorys artificial intelligence research provided onr contract nj jl marroquin supported part grant consejo nacional de ciencia tecnologia mexico
the limitations using selforganizing maps som either clusteringvector quantization vq multidimensional scaling mds discussed reviewing recent empirical findings relevant theory soms remaining ability vq mds time challenged new combined technique online kmeans clustering plus sammon mapping cluster centroids som shown perform significantly worse terms quantization error recovering structure clusters preserving topology comprehensive empirical study using series multivariate normal clustering problems
while exploring find better solutions agent performing online reinforcement learning rl perform worse acceptable in cases exploration might unsafe even catastrophic results often modeled terms reaching failure states agents environment this paper presents method uses domain knowledge reduce number failures exploration this method formulates set actions rl agent composes control policy ensure exploration conducted policy space excludes unacceptable policies the resulting action set abstract relationship task solved common many applications rl although cost added safety learning may result suboptimal solution argue appropriate tradeoff many problems we illustrate method domain motion planning
in learning problems connectionist network trained finite sized training set better generalization performance often obtained unneeded weights network eliminated one source unneeded weights comes inclusion input variables provide little information output variables we propose method identifying eliminating input variables the method first determines relationship input output variables using nonparametric density estimation measures relevance input variables using information theoretic concept mutual information we present results method simple toy problem nonlinear time series
in work applying genetic algorithms populations neural networks real distinction genotype phenotype in nature information contained genotype mapping genetic information phenotype usually much complex the genotypes many organisms exhibit diploidy ie include two copies gene two copies identical sequences therefore functional difference products usually proteins expressed phenotypic feature termed dominant one one recessive expressed in paper review literature use diploidy dominance operators genetic algorithms present new results obtained simulations changing environments finally discuss results simulations parallel biological findings
the multiscalar architecture advocates distributed processor organization tasklevel speculation exploit high degrees instruction level parallelism ilp sequential programs without impeding improvements clock speeds the main goal paper understand key implications architectural features distributed processor organization tasklevel speculation compiler task selection point view performance we identify fundamental performance issues control ow speculation data communication data dependence speculation load imbalance task overhead we show issues intimately related key characteristics tasks task size intertask control ow intertask data dependence we describe compiler heuristics select tasks favorable characteristics we report experimental results show heuristics successful boosting overall performance establishing larger ilp windows
technical report january abstract this paper introduces introspection approach method learning agent employing reinforcement learning decide ask training agent instruction when using approach find number trainers responses produced significantly faster learners learner ask aid randomly guidance received via approach informative random guidance thus reduce interaction training agent learning agent without reducing speed learner develops policy in fact intelligent learner asks help even increase learning speed level trainer interaction
we present method feature construction selection finds minimal set conjunctive features appropriate perform classification task for problems bias appropriate method outperforms constructive induction algorithms able achieve higher classification accuracy the application method search minimal multilevel boolean expressions presented analyzed help examples
in paper discuss bayesian approach finding latent classes data in approach use finite mixture models describe underlying structure data demonstrate possibility use full joint probability models raises interesting new prospects exploratory data analysis the concepts methods discussed illustrated case study using data set recent educational study the bayesian classification approach described implemented presents appealing addition standard toolbox exploratory data analysis educational data
we present two additions hierarchical mixture experts hme architecture we view hme tree structured classifier firstly applying likelihood splitting criteria expert hme grow tree adaptively training secondly considering probable path tree may prune branches away either temporarily permanently become redundant we demonstrate results growing pruning algorithms show significant speed ups efficient use parameters conventional algorithms discriminating two interlocking spirals classifying bit parity patterns
systems interacting realworld data must address issues raised possible presence errors observations makes in paper first present framework discussing imperfect data resulting problems may cause we distinguish two categories errors data random errors noise systematic errors examine relationship task describing observations way also useful helping future problemsolving learning tasks secondly proceed examine techniques currently used ai research recognising errors
genetic programming applied task evolving general iterative sorting algorithms a connection size generality discovered adding inverse size fitness measure along correctness decreases size resulting evolved algorithms also dramatically increases generality thus effectiveness evolution process in addition variety differing problem formulations investigated relative probability success reported an example evolved sort problem formulation presented initial attempt made understand variations difficulty resulting differing problem formulations
irrelevant redundant features may reduce predictive accuracy comprehensibility induced concepts most common machine learning approaches selecting good subset relevant features rely crossvalidation as alternative present application particular minimum description length mdl measure task feature subset selection using mdl principle allows taking account available data the new measure informationtheoretically plausible yet still simple therefore efficiently computable we show empirically new method judging value feature subsets efficient performs least well methods based crossvalidation domains large number training examples large number possible features yield biggest gains efficiency thus new approach seems scale better large learning problems previous methods
rules rivest inductive algorithms aq cn learn decision lists incrementally one rule time such algorithms face rule overlap problem classification accuracy decision list depends overlap learned rules thus even though rules learned isolation evaluated concert existing algorithms solve problem adopting greedy iterative structure once rule learned training examples match rule removed training set we propose novel solution problem composing decision lists homogeneous rules rules whose classification accuracy change position decision list we prove problem finding maximally accurate decision list reduced problem finding maximally accurate homogeneous rules we report performance algorithm data sets uci repository monks problems
methods build function approximators example data gained considerable interest past especially methodologies build models allow interpretation attracted attention most existing algorithms however either complicated use infeasible highdimensional problems this article presents efficient easy use algorithm construct fuzzy graphs example data the resulting fuzzy graphs based locally independent fuzzy rules operate solely selected important attributes this enables application fuzzy graphs also problems high dimensional spaces using illustrative examples real world data set demonstrated resulting fuzzy graphs offer quick insights structure example data underlying model
we describe methodology enabling intelligent teaching system make high level strategy decisions basis low level student modeling information this framework less costly construct superior hand coding teaching strategies responsive learners needs in order accomplish reinforcement learning used learn associate superior teaching actions certain states students knowledge reinforcement learning rl shown flexible handling noisy data need expert domain knowledge a drawback rl often needs significant number trials learning we propose offline learning methodology using sample data simulated students small amounts expert knowledge bypass problem
any intelligent system whether human robotic must capable dealing patterns time temporal pattern processing achieved system shortterm memory capacity stm different representations maintained time in work propose neural model wherein stm realized leaky integrators selforganizing system the model exhibits compositionality ability extract construct progressively complex structured associations hierarchical manner starting basic primitive temporal elements an important feature proposed model use temporal correlations express dynamic bindings
in tasks requiring sustained attention human alertness varies minute time scale this serious consequences occupations ranging air traffic control monitoring nuclear power plants changes electroencephalographic eeg power spectrum accompany fluctuations level alertness assessed measuring simultaneous changes eeg performance auditory monitoring task by combining power spectrum estimation principal component analysis artificial neural networks show continuous accurate noninvasive near realtime estimation operators global level alertness feasible using eeg measures recorded two central scalp sites this demonstration could lead practical system noninvasive monitoring cognitive state human operators attentioncritical settings
this paper presents exact solutions convergent approximations inferences bayesian networks associated finitely generated convex sets distributions robust bayesian inference calculation bounds posterior values given perturbations probabilistic model the paper presents exact inference algorithms analyzes circumstances exact inference becomes intractable two classes algorithms numeric approximations developed transformations original model the first transformation reduces robust inference problem estimation probabilistic parameters bayesian network the second transformation uses lavines bracketing algorithm generate sequence maximization problems bayesian network the analysis extended contaminated lower density bounded belief function subsigma density bounded total variation density ratio classes distributions c fl carnegie mellon university
one fundamental problems learning identifying members two different classes for example diagnose cancer one must learn discriminate benign malignant tumors through examination tumors previously determined diagnosis one learns function distinguishing benign malignant tumors then acquired knowledge used diagnose new tumors the perceptron simple biologically inspired model twoclass learning problem the perceptron trained constructed using examples two classes then perceptron used classify new examples we describe geometrically perceptron capable learning using duality develop framework investigating different methods training perceptron depending define best perceptron different minimization problems developed training perceptron the effectiveness methods evaluated empirically four practical applications breast cancer diagnosis detection heart disease political voting habits sonar recognition this paper assume prior knowledge machine learning pattern recognition
i define latent variable model form neural network target outputs specified inputs unspecified although inputs missing still possible train model placing simple probability distribution unknown inputs maximizing probability data given parameters the model discover description data terms underlying latent variable space lower dimensionality i present preliminary results application models protein data
this paper studies aspects two categories usually differ like relevance generalization role loss function presents unifying formalism types information identified answers generalized questions shows kind generalized information necessary enable learning aims put usual training data prior information equal footing discussing possibilities variants measurement control generalized questions including examples smoothness symmetries reviews shortly measurement linguistic concepts based fuzzy priors principles combine preprocessors uses bayesian decision theoretic framework contrasting parallel inverse decision problems proposes problems nonapproximation aspects bayesian two step approximation consisting posterior maximization subsequent risk minimization analyses empirical risk minimization aspect nonlocal information compares bayesian two step approximation empirical risk minimization including interpretations occams razor formulates examples stationarity conditions maximum posterior approximation nonlocal nonconvex priors leading inhomogeneous nonlinear equations similar example equations scattering theory physics in summary paper focuses dependencies answers different questions because training examples alone dependencies enable generalization emphasizes need empirical measurement control explicit treatment theory this report describes research done within center biological computational learning department brain cognitive sciences massachusetts institute technology this research sponsored grant national science foundation contract asc grant onrarpa contract nj the author supported postdoctoral fellowship le deutsche forschungsgemeinschaft nsfcise postdoctoral fellowship
we present alternative cellular encoding technique gruau evolving graph network structures via genetic programming the new technique called edge encoding uses edge operators rather node operators cellular encoding while cellular encoding edge encoding produce possible graphs two encodings bias genetic search process different ways may therefore useful different set problems the problems techniques may used think edge encoding may particularly useful include evolution recurrent neural networks finite automata graphbased queries symbolic knowledge bases in preliminary report present technical description edge encoding initial comparison cellular encoding experimental investigation relative merits encoding schemes currently progress
we present technique evaluating classifications geometric comparison rule sets rules represented objects ndimensional hyperspace the similarity classes computed overlap geometric class descriptions the system produces correlation matrix indicates degree similarity pair classes the technique applied classifications generated different algorithms different numbers classes different attribute sets experimental results case study medical domain included
as natural resources become less abundant naturally become interested adept utilisation waste materials in bringing bear ploy key importance learning i argue paper in truth trash model learning viewed process uses environmental feedback assemble fortuitous sensory predispositions sensory trash useful information vehicles ie truthful indicators salient phenomena the main aim show computer implementation model used enhance learning strategic abilities simulated football playing mobot
this paper concerns probabilistic evaluation effects actions presence unmeasured variables we show identification causal effect singleton variable x set variables y accomplished systematically time polynomial number variables graph when causal effect identifiable closedform expression obtained probability action achieve specified goal set goals
visor large connectionist system shows visual schemas learned represented used mechanisms natural neural networks processing visor based cooperation competition parallel bottomup topdown activation schema representations simulations show visor robust noise variations inputs parameters it indicate confidence analysis pay attention important minor differences use context recognize ambiguous objects experiments also suggest representation learning stable behavior consistent human processes priming perceptual reversal circular reaction learning the schema mechanisms visor serve starting point building robust highlevel vision systems perhaps schemabased motor control natural language processing systems well
we present framework characterizing bayesian classification methods this framework thought spectrum allowable dependence given probabilistic model naive bayes algorithm restrictive end learning full bayesian networks general extreme while much work carried along two ends spectrum surprising little done along middle we analyze assumptions made one moves along spectrum show tradeoffs model accuracy learning speed become critical consider variety data mining domains we present general induction algorithm allows traversal spectrum depending available computational power carrying induction show application number domains different properties
traits acquired members evolving population lifetime adaptive processes learning become genetically specified later generations thus change level learning population evolutionary time this paper explores idea well benefits gained learning may also costs paid ability learn it costs supply selection pressure genetic assimilation acquired traits two models presented attempt illustrate assertion the first uses kauffmans nk fitness landscapes show effect explicit implicit costs assimilation learnt traits a characteristic hump observed graph level plasticity population showing learning first selected evolution progresses the second model practical example neural network controllers evolved small mobile robot results experiment also show hump
the evolution population guided phenotypic traits acquired members population lifetime this phenomenon known baldwin effect speed evolutionary process traits initially acquired become genetically specified later generations this paper presents conditions genetic assimilation take place as well benefits lifetime adaptation give population may cost paid adaptive ability it evolutionary tradeoff costs benefits provides selection pressure acquired traits become genetically specified it also noted genotypic space evolution operates phenotypic space adaptive processes learning operate general different nature to guarantee acquired characteristic become genetically specified spaces must property neighbourhood correlation means small distance two individuals phenotypic space implies small distance two individuals genotypic space
decision trees widely used classificationregression tasks they relatively much faster build compared neural networks understandable humans in normal decision trees based input vector one branch followed in probabilistic option trees based input vector follow subtrees probability these probabilities learned system probabilistic decisions likely useful boundary classes submerge noise input data in addition provide us confidence measure we allow option nodes trees again instead uniform voting learn weightage every subtree
the fundamental backpropagation bp algorithm training artificial neural networks cast deterministic nonmonotone perturbed gradient method under certain natural assumptions series learning rates diverging series squares converging established every accumulation point online bp iterates stationary point bp error function the results presented cover serial parallel online bp modified bp momentum term bp weight decay
recurrent neural networks trained behave like deterministic finitestate automata dfas show deteriorating performance tested long strings this deteriorating performance attributed instability internal representation learned dfa states the use sigmoidal discriminant function together recurrent structure contribute instability we prove simple algorithm construct secondorder recurrent neural networks sparse interconnection topology sigmoidal discriminant function internal dfa state representations stable ie constructed network correctly classifies strings arbitrary length the algorithm based encoding strengths weights directly neural network we derive relationship weight strength number dfa states robust string classification for dfa n states input alphabet symbols constructive algorithm generates programmed neural network on neurons omn weights we compare algorithm methods proposed literature
report sycon recent results lyapunovtheoretic techniques nonlinear stability abstract this paper presents converse lyapunov function theorem motivated robust control analysis design our result based upon generalizes various aspects wellknown classical theorems in unified natural manner includes arbitrary bounded disturbances acting system deals global asymptotic stability results smooth infinitely differentiable lyapunov functions applies stability respect necessarily compact invariant sets as corollary obtained converse theorem show wellknown lyapunov sufficient condition inputtostate stability also necessary settling positively open question raised several authors past years
technical report cstr umiacstr university maryland college park md abstract the extraction symbolic knowledge trained neural networks direct encoding partial knowledge networks prior training important issues they allow exchange information symbolic connectionist knowledge representations the focus paper quality rules extracted recurrent neural networks discretetime recurrent neural networks trained correctly classify strings regular language rules defining learned grammar extracted networks form deterministic finitestate automata dfas applying clustering algorithms output space recurrent state neurons our algorithm extract different finitestate automata consistent training set network we compare generalization performances different models trained network introduce heuristic permits us choose among consistent dfas model best approximates learned regular grammar
jobshop scheduling important task manufacturing industries we interested particular task scheduling payload processing nasas space shuttle program this paper summarizes previous work formulating task solution reinforcement learning algorithm t d a shortcoming previous work reliance handengineered input features this paper shows extend timedelay neural network tdnn architecture apply irregularlength schedules experimental tests show tdnnt d network match performance previous handengineered system the tests also show neural network approaches significantly outperform best previous nonlearning solution problem terms quality resulting schedules number search steps required construct
report sycon abstract this paper deals simulation turing machines neural networks such networks made interconnections synchronously evolving processors updates state according sigmoidal linear combination previous states units the main result states one may simulate turing machines nets linear time in particular possible give net made processors computes universal partialrecursive function this update report sycon new results include simulation linear time binarytape machines opposed unary alphabets used previous version
in paper develop new lrtabased algorithms variety tasks analyze complexity the lrta algorithm realtime search algorithm developed korf it used reach stationary moving goal state identify shortest paths given start state stationary goal state algorithm reset start state reaches goal state our algorithms search horizon one require internal memory must able store information states for example bidirectional lrts algorithm determines optimal universal plans ie finds optimal paths states set stationary goal states even reset actions available we show tasks studied paper solved lrtabased algorithms on action executions state spaces size n
in paper consider problem approximating function belonging function space linear combination n translates given function g using lemma jones barron show possible define function spaces functions g rate convergence zero error o p n number dimensions the apparent avoidance curse dimensionality due fact function spaces constrained dimension increases examples include spaces sobolev type number weak derivatives required larger number dimensions we give results approximation l norm l norm the interesting feature results thanks constructive nature jones barrons lemma iterative procedure defined achieve rate this paper describes research done within center biological information processing department brain cognitive sciences artificial intelligence laboratory department mathematics university trento italy gabriele anzellotti department mathematics university trento italy this research sponsored grant office naval research onr cognitive neural sciences division artificial intelligence center hughes aircraft corporation s support a i laboratorys artificial intelligence research provided advanced research projects agency department defense army contract dacac part onr contract nk c fl massachusetts institute technology
university wisconsin computer sciences technical report september abstract in explanationbased learning specific problems solution generalized form later used solve conceptually similar problems most research explanationbased learning involves relaxing constraints variables explanation specific example rather generalizing graphical structure explanation however precludes acquisition concepts iterative recursive process implicitly represented explanation fixed number applications this paper presents algorithm generalizes explanation structures reports empirical results demonstrate value acquiring recursive iterative concepts the bagger algorithm learns recursive iterative concepts integrates results multiple examples extracts useful subconcepts generalization on problems learning recursive rule appropriate system produces result standard explanationbased methods applying learned recursive rules requires minor extension prologlike problem solver namely ability explicitly call specific rule empirical studies demonstrate generalizing structure explanations helps avoid recently reported negative effects learning
we consider gibbs sampler applied uniform distribution bounded region r r we show convergence properties gibbs sampler depend greatly smoothness boundary r indeed sufficiently smooth boundaries sampler uniformly ergodic jagged boundaries sampler could fail even geometrically ergodic
in learning examples often useful expand attributevector representation intermediate concepts the usual advantage structuring learning problem makes learning easier improves comprehensibility induced descriptions in paper develop technique discovering useful intermediate concepts class attributes realvalued the technique based decomposition method originally developed design switching circuits recently extended handle incompletely specified multivalued functions it also applied machine learning tasks in paper introduce modifications needed decompose real functions present symbolic form the method evaluated number test functions the results show method correctly decomposes fairly complex functions the decomposition hierarchy depend given repertoir basic functions background knowledge
uncertainty sampling methods iteratively request class labels training instances whose classes uncertain despite previous labeled instances these methods greatly reduce number instances expert need label one problem approach classifier best suited application may expensive train use selection instances we test use one classifier highly efficient probabilistic one select examples training another c rule induction program despite chosen heterogeneous approach uncertainty samples yielded classifiers lower error rates random samples ten times larger
certain causal models involving unmeasured variables induce independence constraints among observed variables imply nevertheless inequality constraints observed distribution this paper derives general formula inequality constraints induced instrumental variables exogenous variables directly affect variables with help formula possible test whether model involving instrumental variables may account data conversely whether given vari able deemed instrumental
bayesian confidence intervals smoothing spline often used distinguish two curves in paper provide asymptotic formula sample size calculations based bayesian confidence intervals approximations simulations special functions indicate asymptotic formula reasonably accurate key words bayesian confidence intervals sample size smoothing spline fl address department statistics applied probability university california santa barbara ca tel fax email yuedongpstatucsbedu supported national institute health grants r ey p dk p hd
we describe several improvements freund schapires adaboost boosting algorithm particularly setting hypotheses may assign confidences predictions we give simplified analysis adaboost setting show analysis used find improved parameter settings well refined criterion training weak hypotheses we give specific method assigning confidences predictions decision trees method closely related one used quinlan this method also suggests technique growing decision trees turns identical one proposed kearns mansour we focus next apply new boosting algorithms multiclass classification problems particularly multilabel case example may belong one class we give two boosting methods problem one leads new method handling singlelabel case simpler effective techniques suggested freund schapire finally give experimental results comparing algorithms discussed paper
evolutionary algorithms direct random search algorithms imitate principles natural evolution method solve adaptation learning tasks general as several features common observed genetic phenotypic level living species in paper algorithms capability adaptation learning wider sense demonstrated focused genetic algorithms illustrate learning process population level first level learning evolution strategies demonstrate learning process metalevel strategy parameters second level learning
we examine novel addition known methods learning bayesian networks data improves quality learned networks our approach explicitly represents learns local structure conditional probability distributions cpds quantify networks this increases space possible models enabling representation cpds variable number parameters the resulting learning procedure induces models better emulate interactions present data we describe theoretical foundations practical aspects learning local structures provide empirical evaluation proposed learning procedure this evaluation indicates learning curves characterizing procedure converge faster number training instances standard procedure ignores local structure cpds our results also show networks learned local structures tend complex terms arcs yet require fewer parameters
this paper contains method bound test errors voting committees members chosen pool trained classifiers there many prospective committees validating directly achieve useful error bounds because fewer classifiers prospective committees better validate classifiers individually use linear programming infer committee error bounds we test method using credit card data also extend method infer bounds classifiers general
an accurate simulation heating coil used compare performance pi controller neural network trained predict steadystate output pi controller neural network trained minimize nstep ahead error coil output set point reinforcement learning agent trained minimize sum squared error time although pi controller works well task neural networks result improved performance
the cn algorithm induces ordered list classification rules examples using entropy search heuristic in short paper describe two improvements algorithm firstly present use laplacian error estimate alternative evaluation function secondly show unordered well ordered rules generated we experimentally demonstrate significantly improved performances resulting changes thus enhancing usefulness cn inductive tool comparisons quinlans c also made
neural computation also called connectionism parallel distributed processing neural network modeling brainstyle computation grown rapidly last decade despite explosion ultimately impressive applications dire need concise introduction theoretical perspective analyzing strengths weaknesses connectionist approaches establishing links disciplines statistics control theory the introduction theory neural computation hertz krogh palmer subsequently referred hkp written perspective physics home discipline authors the book fulfills mission introduction neural network novices provided background calculus linear algebra statistics it covers number models often viewed disjoint critical analyses fruitful comparisons models
controlflow misprediction penalties major impediment high performance wideissue superscalar processors in paper present selective eager execution see execution model overcome misspeculation penalties executing paths diffident branches we present microarchitecture polypath processor extension aggressive superscalar outoforder architecture the polypath architecture uses novel instruction tagging register renaming mechanism execute instructions multiple paths simultaneously processor pipeline retaining maximum resource availability singlepath code sequences results executiondriven pipelinelevel simulations show see improve performance much go benchmark average specint compared normal superscalar outoforder speculative execution monopath processor moreover architectural model elegant practical implement using small amount additional state control logic
this paper describes competitive tree learning algorithm derived first principles the algorithm approximates bayesian decision theoretic solution learning task comparative experiments algorithm several mature ai statistical families tree learning algorithms currently use show derived bayesian algorithm consistently good better although sometimes computational cost using strategy design algorithms many supervised model learning tasks given probabilistic representation kind knowledge learned as illustration second learning algorithm derived learning bayesian networks data implications incremental learning use multiple models also discussed
we address problem finding subset features allows supervised induction algorithm induce small highaccuracy concepts we examine notions relevance irrelevance show definitions used machine learning literature adequately partition features useful categories relevance we present definitions irrelevance two degrees relevance these definitions improve understanding behavior previous subset selection algorithms help define subset features sought the features selected depend features target concept also induction algorithm we describe method feature subset selection using crossvalidation applicable induction algorithm discuss experiments conducted id c artificial real datasets
we describe machine learning method predicting value realvalued function given values multiple input variables the method induces solutions samples form ordered disjunctive normal form dnf decision rules a central objective method representation induction compact easily interpretable solutions this rulebased decision model extended search efficiently similar cases prior approximating function values experimental results realworld data demonstrate new techniques competitive existing machine learning statistical methods sometimes yield superior regression performance
this work presents hybrid branch predictor scheme uses limited form dual path execution along dynamic branch prediction improve execution times the ability execute paths conditional branch enables branch penalty minimized however relying exclusively dual path execution infeasible due instruction fetch rates far exceed capability pipeline retire single branch others must processed by using confidence information available dynamic branch prediction state tables limited form dual path execution becomes feasible this reduces burden branch predictor allowing predictions low confidence avoided in study present new approach gather branch prediction confidence little overhead use confidence mechanism determine whether dual path execution branch prediction used comparing hybrid predictor model dynamic branch predictor shows dramatic decrease misprediction rate translates reduction runtime these results imply dual path execution often thought excessively resource consuming method may worthy approach restricted appropriate predicting set
this paper presents threaded multipath execution tme exploits existing hardware simultaneous multithreading smt processor speculatively execute multiple paths execution when fewer threads smt processor hardware contexts threaded multipath execution uses spare contexts fetch execute code along less likely path hardtopredict branches this paper describes hardware mechanisms needed enable smt processor efficiently spawn speculative threads threaded multipath execution the mapping synchronization bus described enables spawning multiple paths policies examined deciding branches fork managing competition primary alternate path threads critical resources our results show tme increases single program performance smt eight thread contexts average depending misprediction penalty programs high misprediction rate
in paper review research machine learning relation computational models human learning we focus initially concept induction examining five main approaches problem consider complex issue learning sequential behaviors after compare rhetoric sometimes appears machine learning psychological literature growing evidence different theoretical paradigms typically produce similar results in response suggest concrete computational models currently dominate field may less useful simulations operate abstract level we illustrate point abstract simulation explains challenging phenomenon area category learning conclude general observations abstract models
the function unknown biological sequence often accurately inferred identifying sequences homologous original sequence given query set known homologs exist least three general classes techniques finding additional homologs pairwise sequence comparisons motif analysis hidden markov modeling pair wise sequence comparisons typically employed single query sequence known hidden markov models hmms hand usually trained sets sequences motif based methods fall two extremes
future research directions knowledge discovery databases kdd include ability extract overlying concept relating useful data current limitations involve search complexity find concept means useful the pattern theory research crosses natural way aforementioned domain the goal paper threefold first present new approach problem learning discovery robust pattern finding second explore current limitations pattern theoretic approach applied general kdd problem third exhibit performance experimental results binary functions compare results c this new approach learning demonstrates powerful method finding patterns robust manner
prerequisites an understanding dynamic programming edit distance approach pairwise sequence alignment useful parts also familiarity use internet resources would helpful part for former see chapters latter see chapter hypertext book gnavsns biocomputing course httpwwwtechfakunibielefelddebcdcurricwelcomehtml general rationale you understand multiple alignment considered challenging problem study approaches try reduce number steps needed calculate optimal solution study fast heuristics in case study involving immunoglobulin sequences study multiple alignments obtained www servers recapitulating results original paper revision history version sep expanded ex updated ex revised solution sheet ex marked exercises a submitted instructor various minor clarifications content
this article describes new system induction oblique decision trees this system oc combines deterministic hillclimbing two forms randomization find good oblique split form hyperplane node decision tree oblique decision tree methods tuned especially domains attributes numeric although adapted symbolic mixed symbolicnumeric attributes we present extensive empirical studies using real artificial data analyze ocs ability construct oblique trees smaller accurate axisparallel counterparts we also examine benefits randomization construction oblique decision trees
explanationbased reinforcement learning ebrl introduced dietterich flann way combining ability reinforcement learning rl learn optimal plans generalization ability explanationbased learning ebl dietterich flann we extend work domains agent must order achieve sequence subgoals optimal fashion hierarchical ebrl effectively learn optimal policies sequential task domains even subgoals weakly interact we also show planner achieve individual subgoals available method converges even faster
discretization continuously valued data useful necessary tool many learning paradigms assume nominal data a list objectives efficient effective discretization presented a paradigm called brace boundary ranking and classification evaluation attempts meet objectives presented along algorithm follows paradigm the paradigm meets many objectives potential extension meet remainder empirical results promising for reasons brace potential effective efficient method discretization continuously valued data a advantage brace general enough extended types clusteringunsupervised learning
naive bayesian classifiers make independence assumptions perform remarkably well data sets poorly others we explore ways improve bayesian classifier searching dependencies among attributes we propose evaluate two algorithms detecting dependencies among attributes show backward sequential elimination joining algorithm provides improvement naive bayesian classifier the domains improvement occurs domains naive bayesian classifier significantly less accurate decision tree learner this suggests attributes used common databases independent conditioned class violations independence assumption affect accuracy classifier the bayesian classifier duda hart probabilistic method classification it used determine probability example j belongs class c given values attributes example represented set n nominallyvalued attributevalue pairs form a v j p a k v k j jc may estimated training data to determine likely class test example probability class computed equation a classifier created manner sometimes called simple langley naive kononenko bayesian classifier one important evaluation metric machine learning methods predictive accuracy unseen examples this measured randomly selecting subset examples database use training examples reserving remainder used test examples in case simple bayesian classifier training examples used estimate probabilities equation used detected training data
multiple sequence alignment distantly related viral proteins remains challenge currently available alignment methods the hidden markov model approach offers new flexible method generation multiple sequence alignments the results studies attempting infer appropriate parameter constraints generation de novo hmms globin kinase aspartic acid protease ribonuclease h sequences sam hmmer methods described
several recurrent networks proposed representations task formal language learning after training recurrent network next step understand information processing carried network some researchers giles et al watrous kuhn cleeremans et al resorted extracting finite state machines internal state trajectories recurrent networks this paper describes two conditions sensitivity initial conditions frivolous computational explanations due discrete measurements kolen pollack allow extraction methods return illusionary finite state descriptions
in order useful learning algorithm must able generalize well faced inputs previously presented system a bias necessary generalization shown several researchers recent years bias lead strictly better generalization summed possible functions applications this paper provides examples illustrate fact also explains bias learning algorithm better another practice probability occurrence functions taken account it shows domain knowledge understanding conditions learning algorithm performs well used increase probability accurate generalization identifies several conditions considered attempting select appropriate bias particular problem
in paper introduce modelbased reinforcement learning method called hlearning optimizes undiscounted average reward we compare three reinforcement learning methods domain scheduling automatic guided vehicles transportation robots used modern manufacturing plants facilities the four methods differ along two dimensions they either modelbased modelfree optimize discounted total reward undiscounted average reward our experimental results indicate hlearning robust respect changes domain parameters many cases converges fewer steps better average reward per time step methods an added advantage unlike methods parameters tune
this paper presents converse lyapunov function theorem motivated robust control analysis design our result based upon generalizes various aspects wellknown classical theorems in unified natural manner allows arbitrary bounded timevarying parameters system description deals global asymptotic stability results smooth infinitely differentiable lyapunov functions applies stability respect necessarily compact invariant sets introduction this work motivated problems robust nonlinear stabilization one main
technical report ai may abstract an important often neglected problem field artificial intelligence grounding systems environment representations manipulate inherent meaning system since humans rely heavily semantics seems likely grounding crucial development truly intelligent behavior this study investigates use simulated robotic agents neural network processors part method ensure grounding both topology weights neural networks optimized genetic algorithms although comprehensive optimization difficult empirical evidence gathered shows method tractable quite fruitful in experiments agents evolved wallfollowing control strategy able transfer novel environments their behavior suggests also learning build cognitive maps
explanationbased learning mitchell et al dejong mooney shown promise powerful analytical learning technique however ebl severely hampered requirement complete correct domain theory successful learning occur clearly nontrivial domains developing domain theory nearly impossible task therefore much research devoted understanding imperfect domain theory corrected extended system performance in paper present characterization problem use analyze past research area past characterizations problem eg mitchell et al rajamoney dejong viewed types performance errors caused faulty domain theory primary in contrast focus primarily types knowledge deficiencies present theory derive types performance errors result correcting theory viewed search space possible domain theories variety knowledge sources used guide search we examine types knowledge used variety past systems purpose the hope analysis indicate need universal weak method domain theory correction different sources knowledge theory correction freely flexibly combined
we study task tnding maximal posteriori map instantiation bayesian network variables given partial value assignment initial constraint this problem known nphard concentrate stochastic approximation algorithm simulated annealing this stochastic algorithm realized sequential process set bayesian network variables one variable allowed change time consequently method become impractically slow number variables increases we present method mapping given bayesian network massively parallel bolztmann machine neural network architecture sense instead using normal sequential simulated annealing algorithm use massively parallel stochastic process boltzmann machine architecture the neural network updating process provably converges state solves given map task
parameterized heuristics offers elegant powerful theoretical framework design analysis autonomous adaptive communication networks routing messages networks presents realtime instance multicriterion optimization problem dynamic uncertain environment this paper describes framework heuristic routing large networks the effectiveness heuristic routing mechanism upon quo vadis based described part simulation study within network grid topology a formal analysis underlying principles presented incremental design set heuristic decision functions used guide messages along nearoptimal eg minimum delay path large network this paper carefully derives properties heuristics set simplifying assumptions network topology load dynamics identify conditions guaranteed route messages along optimal path the paper concludes discussion relevance theoretical results presented paper design intelligent autonomous adaptive communication networks outline directions future research
technical report department statistics university washington derek stanford graduate research assistant adrian e raftery professor statistics sociology department statistics university washington box seattle wa usa email stanfordstatwashingtonedu rafterystatwashingtonedu web httpwwwstatwashingtoneduraftery this research supported onr grants n n the authors grateful simon byers gilles celeux christian posse helpful discussions
we analyze algorithms predict binary value combining predictions several prediction strategies called experts our analysis worstcase situations ie make assumptions way sequence bits predicted generated we measure performance algorithm difference expected number mistakes makes bit sequence expected number mistakes made best expert sequence expectation taken respect randomization predictions we show minimum achievable difference order square root number mistakes best expert give efficient algorithms achieve our upper lower bounds matching leading constants cases we show leads certain kinds pattern recognitionlearning algorithms performance bounds improve best results currently known context we also compare analysis case log loss used instead expected number mistakes
this paper presents formalization novel approach structural similarity assessment adaptation casebased reasoning cbr synthesis the approach informally presented exemplified implemented domain industrial building design borner by relating approach existing theories provide foundation systematic evaluation appropriate usage cases primary repository knowledge represented structurally using algebraic approach similarity relations provide structure preserving case modifications modulo underlying algebra equational theory algebra available this representation modeled universe discourse enables theorybased inference adapted solutions the approach enables us incorporate formally generalization abstraction geometrical transformation combinations cbr
a learning agent employing reinforcement learning hindered receives critics sparse weakly informative training information we present approach automated training agent may also provide occasional instruction learner form actions learner perform the learner access critics feedback trainers instruction in experiments vary level trainers interaction learner allowing trainer instruct learner almost every time step allowing trainer respond we also vary parameter controls learner incorporates trainers actions the results show significant reductions average number training trials necessary learn perform task
we present algorithm improving accuracy algorithms learning binary concepts the improvement achieved combining large number hypotheses generated training given learning algorithm different set examples our algorithm based ideas presented schapire paper the strength weak learnability represents improvement results the analysis algorithm provides general upper bounds resources required learning valiants polynomial pac learning framework best general upper bounds known today we show number hypotheses combined algorithm smallest number possible other outcomes analysis results regarding representational power threshold circuits relation learnability compression method parallelizing pac learning algorithms we provide extensions algorithms cases concepts binary case accuracy learning algorithm depends distribution instances
this paper proposes model ratio decidendi justification structure consisting series reasoning steps relate abstract predicates abstract predicates relate abstract predicates specific facts this model satisfies important set characteristics ratio decidendi identified jurisprudential literature in particular model shows theory case decided controls precedential effect by contrast purely exemplarbased model ratio decidendi fails account dependency precedential effect theory decision
in paper abstract computational principles underlying topographic maps discussed we give definition perfectly neighbourhood preserving map call topographic homeomorphism prove certain desirable properties it argued topographic homeomorphism exist usual case many equally valid choices available quantifying quality map we introduce particular measure encompasses several previous proposals discuss relation work this formulation problem sets within wellknown class quadratic assignment problems
this paper studies robustness pac learning algorithms instance space f g n examples corrupted purely random noise affecting instances labels in past conflicting results subject obtainedthe best agreement rule tolerate small amounts noise yet cases large amounts noise tolerated we show truth lies somewhere two alternatives for uniform attribute noise attribute flipped independently random probability present algorithm pac learns monomials unknown noise rate less contrasting positive result show product random attribute noise attribute flipped randomly independently probability p nearly harmful malicious noiseno algorithm tolerate small amount noise fl supported part ge foundation junior faculty grant nsf grant ccr part research conducted author mit laboratory computer science supported nsf grant dcr grant siemens corporation net address sgcswustledu
this paper describes research investigating behavioral specialization learning robot teams each agent provided common set skills motor schemabased behavioral assemblages builds taskachieving strategy using reinforcement learning the agents learn individually activate particular behavioral assemblages given current situation reward signal the experiments conducted robot soccer simulations evaluate agents terms performance policy convergence behavioral diversity the results show many cases robots automatically diversify choosing heterogeneous behaviors the degree diversification performance team depend reward structure when entire team jointly rewarded penalized global reinforcement teams tend towards heterogeneous behavior when agents provided feedback individually local reinforcement converge identical policies
product units provide method automatically learning higherorder input combinations required efficient synthesis boolean logic functions neural networks product units also higher information capacity sigmoidal networks however activation function received much attention literature a possible reason one encounters problems using standard backpropagation train networks containing units this report examines problems evaluates performance three training algorithms networks type empirical results indicate error surface networks containing product units local minima corresponding networks summation units for reason combination local global training algorithms found provide reliable convergence we investigate hints added training algorithm by extracting common frequency input weights training frequency separately show convergence accelerated in order compare performance transfer functions product units implemented candidate units cascade correlation cc system using candidate units resulted smaller networks trained faster standard three sigmoidal types one gaussian transfer functions used this superiority confirmed pool candidate units four different nonlinear activation functions used compete addition network extensive simulations showed problem implementing random boolean logic functions product units always chosen transfer functions
our goal develop cognitive model humans acquire skills complex cognitive tasks we pursuing goal designing computational architectures nrl navigation task requires competent sensorimotor coordination in paper analyze nrl navigation task depth we use data experiments human subjects learning task guide us constructing cognitive model skill acquisition task verbal protocol data augments black box view provided execution traces inputs outputs computational experiments allow us explore space alternative architectures task guided quality fit human performance data
we show paper agm postulates week ensure rational preservation conditional beliefs belief revision thus permitting improper responses sequences observations we remedy weakness proposing four additional postulates sound relative qualitative version probabilistic conditioning contrary agm framework proposed postulates characterize belief revision process may depend elements epistemic state necessarily captured belief set we also show simple modification agm framework allow belief revision function epistemic states we establish modelbased representation theorem characterizes proposed postulates constrains turn way entrenchment orderings may transformed iterated belief revision
results presented demonstrate learning finetuning search strategies using connectionist mechanisms previous studies strategy learning within symbolic productionrule formalism addressed finetuning behavior here twolayer connectionist system presented develops search weak taskspecific strategy finetunes performance the system applied simulated realtime balancecontrol task we compare performance onelayer twolayer networks showing ability twolayer network discover new features thus enhance original representation critical solving balancing task
following terminology used adaptive control distinguish indirect learning methods learn explicit models dynamic structure system controlled direct learning methods we compare existing indirect method uses conventional dynamic programming algorithm closely related direct reinforcement learning method applying methods infinite horizon markov decision problem unknown statetransition probabilities the simulations show although direct method requires much less space dramatically less computation per control action learning ability task superior compares favorably complex indirect method although results address methods performances compare problems become difficult suggest given fixed amount computational power available per control action may better use direct reinforcement learning method augmented indirect techniques devote available resources computationally costly indirect method comprehensive answers questions raised study depend many factors making eco nomic context computation
we propose general framework study belief change we begin defining belief terms knowledge plausibility agent believes knows true worlds considers plausible we consider properties defining interaction knowledge plausibility show properties affect properties belief in particular show assuming two natural properties belief becomes kd operator finally add time picture this gives us framework talk knowledge plausibility hence belief time extends framework halpern fagin hf modeling knowledge multiagent systems we show framework quite expressive lets us model natural way number different scenarios belief change for example show capture analogue prior probabilities updated conditioning in related paper show two best studied scenarios belief revision belief update fit framework
markov chain monte carlo mcmc used evaluating expectations functions interest target distribution this done calculating averages sample path markov chain stationary distribution for computational efficiency markov chain rapidly mixing this sometimes achieved careful design transition kernel chain basis detailed preliminary exploratory analysis an alternative approach might allow transition kernel adapt whenever new features encountered mcmc run however adaptation occurs infinitely often stationary distribution chain may disturbed we describe framework based concept markov chain regeneration allows adaptation occur infinitely often disturb stationary distribution chain consistency samplepath averages key words adaptive method bayesian inference gibbs sampling markov chain monte carlo
a traditional interpolation model characterized choice regularizer applied interpolant choice noise model typically regularizer single regularization constant ff noise model single parameter fi the ratio fffi alone responsible determining globally attributes interpolant complexity flexibility smoothness characteristic scale length characteristic amplitude we suggest interpolation models able capture one flavour simplicity complexity we describe bayesian models interpolant smoothness varies spatially we emphasize importance practical implementation concept conditional convexity designing models many hyperparameters we apply new models interpolation neuronal spike data demonstrate substantial improvement generalization error
author paper coordinator machine learning project statlog this project supported financially european community the main aim statlog evaluate different learning algorithms using real industrial commercial applications as industrial partner contributor daimlerbenz introduced different applications statlog among fault diagnosis letter digit recognition creditscoring prediction number registered trucks we learned lot lessons project effected application oriented research field machine learning ml daimlerbenz we distinguished especially research necessary prepare mlalgorithms handle real industrial commercial applications in paper describe shortly daimlerbenz applications statlog discuss shortcomings applied mlalgorithms finally outline fields think research necessary
this paper describes application reinforcement learning rl difficult real world problem elevator dispatching the elevator domain poses combination challenges seen rl research date elevator systems operate continuous state spaces continuous time discrete event dynamic systems their states fully observable nonstationary due changing passenger arrival rates in addition use team rl agents responsible controlling one elevator car the team receives global reinforcement signal appears noisy agent due effects actions agents random nature arrivals incomplete observation state in spite complications show results simulation surpass best heuristic elevator control algorithms aware these results demonstrate power rl large scale stochastic dynamic optimization problem practical utility
this paper presents fringe exploration technique efficient exploration partially observable domains the key idea applicable many exploration techniques keep statistics space possible shortterm memories instead agents current state space experimental results partially observable maze difficult driving task visual routines show dramatic performance improvements
performing policy iteration dynamic programming require knowledge relative rather absolute measures utility actions baird calls advantages actions states nevertheless existing methods dynamic programming including bairds compute form absolute utility function for smooth problems advantages satisfy two differential consistency conditions including requirement free curl show enforcing lead appropriate policy improvement solely terms advantages
we introduce parallel approach dtselect selecting features used inductive learning algorithms predict protein secondary structure dtselect able rapidly choose small nonredundant feature sets pools containing hundreds thousands potentially useful features it building decision tree using features pool classifies set training examples the features included tree provide compact description training data thus suitable use inputs inductive learning algorithms empirical experiments protein secondarystructure task sets complex features chosen dtselect used augment standard artificial neural network representation yield surprisingly little performance gain even though features selected large feature pools we discuss possible reasons result
this paper presents extension package developed author faculty sciences technology new university lisbon designed experimentation coarsegrained distributed genetic algorithms dga the package implemented extension basic sugal system developed andrew hunter university sunderland uk primarily intended used research sequential serial genetic algorithms sga
we explore representation d objects several distinct d views stored object we demonstrate ability twolayer network thresholded summation units support representations using unsupervised hebbian relaxation network learned recognize ten objects different viewpoints the training process led emergence compact representations specific input views when tested novel views objects network exhibited substantial generalization capability in simulated psychophysical experiments networks behavior qualitatively similar human subjects
internal models environment important role play adaptive systems general particular importance supervised learning paradigm in paper demonstrate certain classical problems associated notion teacher supervised learning solved judicious use learned internal models components adaptive system in particular show supervised learning algorithms utilized cases unknown dynamical system intervenes actions desired outcomes our approach applies supervised learning algorithm capable learning multilayer networks this paper revised version mit center cognitive science occasional paper we wish thank michael mozer andrew barto robert jacobs eric loeb james mcclelland helpful comments manuscript this project supported part brsg s rr awarded biomedical research support grant program division research resources national institutes health grant atr auditory visual perception research laboratories grant siemens corporation grant human frontier science program grant nj awarded office naval research
technical report february updated april this paper appear proceedings eleventh international conference machine learning abstract this paper presents algorithm incremental induction decision trees able handle numeric symbolic variables in order handle numeric variables new tree revision operator called slewing introduced finally nonincremental method given finding decision tree based direct metric candidate tree
fl this research primarily conducted author university calif santa cruz support onr grant nk harvard university supported onr grant nk darpa grant afosr current address nec research institute independence way princeton nj email address nicklresearchnjneccom supported onr grants nk nj part research done author sabbatical aiken computation laboratory harvard partial support onr grants nk nk address department computer science university california santa cruz email address manfredcsucscedu
many recent approaches avoiding utility problem speedup learning rely sophisticated utility measures significant numbers training data accurately estimate utility control knowledge empirical results presented elsewhere indicate simple selection strategy retaining control rules derived training problem explanation quickly defines efficient set control knowledge training problems this simple selection strategy provides lowcost alternative exampleintensive approaches improving speed problem solver
partigame new algorithm learning feasible trajectories goal regions high dimensional continuous statespaces in high dimensions essential learning plan uniformly statespace partigame maintains decisiontree partitioning statespace applies techniques gametheory computational geometry efficiently adaptively concentrate high resolution critical areas the current version algorithm designed find feasible paths trajectories goal regions high dimensional spaces future versions designed find solution optimizes realvalued criterion many simulated problems tested ranging twodimensional ninedimensional statespaces including mazes path planning nonlinear dynamics planar snake robots restricted spaces in cases good solution found less ten trials minutes
predictive inference seen process determining predictive distribution discrete variable given data set training examples values problem domain variables we consider three approaches computing predictive distribution assume joint probability distribution variables belongs set distributions determined set parametric models in simplest case predictive distribution computed using model maximum posteriori map posterior probability in evidence approach predictive distribution obtained averaging individual models model family in third case define predictive distribution using rissanens new definition stochastic complexity our experiments performed family naive bayes models suggest using data available stochastic complexity approach produces accurate predictions logscore sense however amount available training data decreased evidence approach clearly outperforms two approaches the map predictive distribution clearly inferior logscore sense two sophisticated approaches score map approach may still cases produce best results
given problem casebased reasoning cbr system search case memory use stored cases find solution possibly modifying retrieved cases adapt required input specifications in paper introduce neural network architecture efficient casebased reasoning we show rigorous bayesian probability propagation algorithm implemented feedforward neural network adapted cbr in approach efficient indexing problem cbr naturally implemented parallel architecture heuristic matching replaced probability metric this allows cbr perform theoretically sound bayesian reasoning we also show probability propagation actually offers solution adaptation problem natural way
we present new generalpurpose algorithm learning classes valued functions generalization prediction model prove general upper bound expected absolute error algorithm terms scalesensitive generalization vapnik dimension proposed alon bendavid cesabianchi haussler we give lower bounds implying upper bounds improved constant factor general we apply result together techniques due haussler benedek itai obtain new upper bounds packing numbers terms scalesensitive notion dimension using different technique obtain new bounds packing numbers terms kearns schapires fatshattering function we show apply packing bounds obtain improved general bounds sample complexity agnostic learning for gt establish weaker sufficient stronger necessary conditions class valued functions agnostically learnable within uniform glivenkocantelli class
it widely considered ultimate connectionist objective incorporate neural networks intelligent systems these systems intended possess varied repertoire functions enabling adaptable interaction nonstatic environment the first step direction develop various neural network algorithms models second step combine networks modular structure might incorporated workable system in paper consider one aspect second point namely processing reliability hiding wetware details pre sented architecture type neural expert module named authority an authority consists number minos modules each minos modules authority processing capabilities varies respect particular specialization aspects problem domain the authority employs collection minoses like panel experts the expert highest confidence believed answer confidence quotient transmitted levels system hierarchy
partially observable markov decision processes pomdps model decision problems agent tries maximize reward face limited andor noisy sensor feedback while study pomdps motivated need address realistic problems existing techniques finding optimal behavior appear scale well unable find satisfactory policies problems dozen states after brief review pomdps paper discusses several simple solution methods shows capable finding nearoptimal policies selection extremely small pomdps taken learning literature in contrast show none able solve slightly larger noisier problem based robot navigation we find combination two novel approaches performs well problems suggest methods scaling even larger complicated domains
we propose new method construction markov chains given stationary distribution this method based construction auxiliary chain stationary distribution picking elements auxiliary chain suitable number times the proposed method many advantages rivals it easy implement provides simple analysis faster efficient currently available techniques also adapted course simulation we make theoretical numerical comparisons characteristics proposed algorithm mcmc techniques
the problem making optimal decisions uncertain conditions central artificial intelligence if state world known times world modeled markov decision process mdp mdps studied extensively many methods known determining optimal courses action policies the realistic case state information partially observable partially observable markov decision processes pomdps received much less attention the best exact algorithms problems inefficient space time we introduce smooth partially observable value approximation spova new approximation method quickly yield good approximations improve time this method combined reinforcement learning methods combination effective test cases
the katsuno mendelzon km theory belief update proposed reasonable model revising beliefs changing world however semantics update relies information readily available we describe alternative semantical view update observations incorporated belief set explaining observation terms set plausible events might caused observation b predicting consequences explanations we also allow possibility conditional explanations we show picture naturally induces update operator conforming km postulates certain assumptions however argue assumptions always reasonable restrict ability integrate update forms revision reasoning action fl some parts report appeared preliminary form an eventbased abductive model update proc tenth canadian conf ai banff alta
this paper specifies main features brainlike neuronal connectionist models argues need usefulness appropriate successively larger brainlike structures examines parallelhierarchical recognition cone models perception perspective examples structures the anatomy physiology behavior development visual system briefly summarized motivate architecture brainstructured networks perceptual recognition results presented simulations carefully predesigned recognition cone structures perceive objects eg houses digitized photographs a framework perceptual learning introduced including mechanisms generationdiscovery feedbackguided growth new links nodes subject brainlike constraints eg local receptive fields global convergencedivergence the information processing transforms discovered generation finetuned feedbackguided reweighting links some preliminary results presented brainstructured networks learn recognize simple objects eg letters alphabet cups apples bananas feedbackguided generation reweighting these show large improvements networks either lack brainlike structure orand learn reweighting links alone
the ability restructure decision tree efficiently enables variety approaches decision tree induction would otherwise prohibitively expensive two approaches described one incremental tree induction iti nonincremental tree induction using measure tree quality instead test quality dmti these approaches several variants offer new computational classifier characteristics lend particular applications
we consider logistic regression model gaussian prior distribution parameters we show accurate variational techniques used obtain closed form posterior distribution parameters given data thereby yielding posterior predictive model the results readily extended binary belief networks for belief networks also derive closed form posteriors presence missing values finally show dual regression problem gives latent variable density model variational formulation leads exactly solvable em updates
mean field methods provide computationally efficient approximations posterior probability distributions graphical models simple mean field methods make completely factorized approximation posterior unlikely accurate posterior multimodal indeed posterior multimodal one modes captured to improve mean field approximation cases employ mixture models posterior approximations mixture component factorized distribution we describe efficient methods optimizing parameters models
the success evolutionary methods standard control learning tasks created need new benchmarks the classic pole balancing problem longer difficult enough serve viable yardstick measuring learning efficiency systems in paper present difficult version classic problem cart pole move plane we demonstrate neuroevolution system enforced subpopulations esp solve difficult problem without velocity information
this paper introduces explores representational biases efficient learning spatial temporal spatiotemporal patterns connectionist networks cn massively parallel networks simple computing elements it examines learning mechanisms constructively build network structures encode information environmental stimuli successively higher resolutions needed tasks eg perceptual recognition network perform some simple examples presented illustrate basic structures processes used networks ensure parsimony learned representations guiding system focus efforts minimal adequate resolution several extensions basic algorithm efficient learning using multiresolution representations spatial temporal spatiotemporal patterns discussed
qlearning uses tdmethods accelerate qlearning the update complexity previous online q implementations based lookuptables bounded size stateaction space our faster algorithms update complexity bounded number actions the method based observation qvalue updates may postponed needed
massively parallel networks relatively simple computing elements offer attractive versatile framework exploring variety learning structures processes intelligent systems this paper briefly summarizes popular learning structures processes used networks it outlines range potentially powerful alternatives patterndirected inductive learning systems it motivates develops class new learning algorithms massively parallel networks simple computing elements we call class learning processes generative offer set mechanisms constructive adaptive determination network architecture number processing elements connectivity among function experience generative learning algorithms attempt overcome limitations approaches learning networks rely modification weights links within otherwise fixed network topology eg rather slow learning need apriori choice network architecture several alternative designs well range control structures processes used regulate form content internal representations learned networks examined empirical results study generative learning algorithms briefly summarized several extensions refinements algorithms directions future research outlined
the use artificial neural networks domain autonomous vehicle navigation produced promising results alvinn pomerleau shown neural system drive vehicle reliably safely many different types roads ranging paved paths interstate highways even impressive results several areas within neural paradigm autonomous road following still need addressed these include transparent navigation roads different type simultaneous use different sensors generalization road types neural system never seen the system presented addresses issue modular neural architecture uses pretrained alvinn networks connectionist superstructure robustly drive many dif ferent types roads
we introduce constructive incremental learning system regression problems models data means locally linear experts in contrast approaches experts trained independently compete data learning only prediction query required experts cooperate blending individual predictions each expert trained minimizing penalized local cross val dation error using second order methods in way expert able find local distance metric adjusting size shape rece p tive field predictions valid also detect relevant n put features adjusting bias importance individual input dimensions we derive asymptotic results method in variety simulations properties algorithm demonstrated respect interference learning speed prediction accuracy feature detection task oriented incremental learning
the model nonbayesian agent faces repeated game incomplete information nature appropriate tool modeling general agentenvironment interactions in model environment state controlled nature may change arbitrarily feedbackreward function initially unknown the agent bayesian form prior probability neither state selection strategy nature reward function a policy agent function assigns action every history observations actions two basic feedback structures considered in one perfect monitoring case agent able observe previous environment state part feedback imperfect monitoring case available agent reward obtained both settings refer partially observable processes current environment state unknown our main result refers competitive ratio criterion perfect monitoring case we prove existence efficient stochastic policy ensures competitive ratio obtained almost stages arbitrarily high probability efficiency measured terms rate convergence it shown optimal policy exist imperfect monitoring case moreover proved perfect monitoring case exist deterministic policy satisfies long run optimality criterion in addition discuss maxmin criterion prove deterministic efficient optimal strategy exist imperfect monitoring case criterion finally show approach longrun optimality viewed qualitative distinguishes previous work area
we describe polynomialtime algorithm learning axisaligned rectangles q respect product distributions multipleinstance examples pac model here example consists n elements q together label indicating whether n points rectangle learned we assume unknown product distribution d q instances independently drawn according d the accuracy hypothesis measured probability would incorrectly predict whether one n points drawn d rectangle learned our algorithm achieves accuracy probability ffi
we present new machine learning method given set training examples induces definition target concept terms hierarchy intermediate concepts definitions this effectively decomposes problem smaller less complex problems the method inspired boolean function decomposition approach design digital circuits to cope high time complexity finding optimal decomposition propose suboptimal heuristic algorithm the method implemented program hint hierarchy induction tool experimentally evaluated using set artificial realworld learning problems it shown method performs well terms classification accuracy discovery meaningful concept hierarchies
in context inductive learning bayesian approach turned successful estimating probabilities events learning examples the mprobability estimate developed handle situations in paper present mdistribution estimate extension mprobability estimate besides estimation probabilities covers also estimation probability distributions we focus application construction regression trees the theoretical results incorporated system automatic induction regression trees the results applying upgraded system several domains presented compared previous results
realworld learning tasks often involve highdimensional data sets complex patterns missing features in paper review problem learning incomplete data two statistical perspectivesthe likelihoodbased bayesian the goal twofold place current neural network approaches missing data within statistical framework describe set algorithms derived likelihoodbased framework handle clustering classification function approximation incomplete data principled efficient manner these algorithms based mixture modeling make two distinct appeals expectationmaximization em principle dempster et al estimation mixture components coping missing data this report describes research done center biological computational learning artificial intelligence laboratory massachusetts institute technology support center provided part grant national science foundation contract asc support laboratorys artificial intelligence research provided part advanced research projects agency department defense the authors supported part grant atr auditory visual perception research laboratories grant siemens corporation grant iri national science foundation grant nj office naval research zoubin ghahramani supported grant mcdonnellpew foundation michael i jordan nsf presidential young investigator
recently proven dynamics deterministic finitestate automata dfa n states input symbols implemented sparse secondorder recurrent neural network sornn n state neurons omn secondorder weights sigmoidal discriminant functions we investigate constructive algorithm extended faulttolerant neural dfa implementations faults analog implementation neurons weights affect desired network performance we show tolerance weight perturbation achieved easily tolerance weight andor neuron stuckatzero faults however requires duplication network resources this result impact construction neural dfas dense internal representation dfa states
technical report no department statistics university washington october abhijit dasgupta graduate student department biostatistics university washington box seattle wa email address dasguptabiostatwashingtonedu adrian e raftery professor statistics sociology department statistics university washington box seattle wa email address rafterystatwashingtonedu this research supported office naval research grant nj the authors grateful peter guttorp girardeau henderson robert muise helpful discussions
in multiarmed bandit problem gambler must decide arm k nonidentical slot machines play sequence trials maximize reward this classical problem received much attention simple model provides tradeoff exploration trying arm find best one exploitation playing arm believed give best payoff past solutions bandit problem almost always relied assumptions statistics slot machines in work make statistical assumptions whatsoever nature process generating payoffs slot machines we give solution bandit problem adversary rather wellbehaved stochastic process complete control payoffs in sequence t plays prove expected perround payoff algorithm approaches best arm rate ot give improved rate convergence best arm fairly low payoff we also prove general matching lower bound best possible performance algorithm setting in addition consider setting player team experts advising arm play give strategy guarantee expected payoff close best expert finally apply result problem learning play unknown repeated matrix game allpowerful adversary
we show alternative way representing bayesian belief network sensitivities probability distributions this representation equivalent traditional representation conditional probabilities makes dependencies nodes apparent intuitively easy understand we also propose qr matrix representation sensitivities andor conditional probabilities efficient memory requirements computational speed traditional representation computerbased implementations probabilistic inference we use sensitivities show certain class binary networks computation time approximate probabilistic inference positive upper bound error result independent size network finally alternative traditional algorithms use conditional probabilities describe exact algorithm probabilistic inference uses qrrepresentation sensitivities updates probability distributions nodes network according messages neigh bors
the requirement train large neural networks quickly prompted design new massively parallel supercomputer using custom vlsi this design features processing nodes communicating mesh network connected directly processor chip studies show peak performance range billion arithmetic operations per second this paper presents case custom hardware combines neural networkspecific features general programmable machine architecture briefly describes design progress
in many realworld domains like text categorization supervised learning requires large number training examples in paper describe active learning method uses committee learners reduce number training examples required learning our approach similar query committee framework disagreement among committee members predicted label input part example used signal need knowing actual value label our experiments text categorization using committee winnowbased learners demonstrate approach reduce number labeled training examples required used single winnow learner orders magnitude this paper review accepted publication another conference journal acknowledgements the availability reuters corpus reuters stat data manipulation analysis programs perlman greatly assisted research date
covering formalized used extensively in work divideandconquer technique formalized well compared covering technique logic programming framework covering works repeatedly specializing overly general hypothesis iteration focusing finding clause high coverage positive examples divideandconquer works specializing overly general hypothesis focusing discriminating positive negative examples experimental results presented demonstrating cases accurate hypotheses found divideandconquer covering moreover since covering considers alternatives repeatedly tends less efficient divideandconquer never considers alternative twice on hand covering searches larger hypothesis space may result compact hypotheses found technique divideandconquer furthermore divideandconquer contrast covering applicable learn ing recursive definitions
this paper introduces recurrence surface approximation inductive learning method based linear programming predicts recurrence times using censored training examples examples available training output may lower bound right answer this approach augmented feature selection method chooses appropriate feature set within context linear programming generalizer computational results field breast cancer prognosis shown a straightforward translation prediction method artificial neural network model also proposed
minimum message length mml invariant bayesian point estimation technique also consistent efficient we provide brief overview mml inductive inference wallace boulton wallace freeman informationtheoretic bayesian interpretation we outline mml used statistical parameter estimation mml mixture modelling program snob wallace boulton wallace wallace dowe uses message lengths various parameter estimates enable combine parameter estimation selection number components the message length within constant logarithm posterior probability theory so mml theory also regarded theory highest posterior probability snob currently assumes variables uncorrelated permits multivariate data gaussian discrete multistate poisson von mises circular distributions
one challenges models cognitive phenomena development efficient exible interfaces low level sensory information high level processes for visual processing researchers long argued attentional mechanism required perform many tasks required high level vision this thesis presents visit connectionist model covert visual attention used vehicle studying interface the model efficient exible biologically plausible the complexity network linear number pixels effective parallel strategies used minimize number iterations required the resulting system able efficiently solve two tasks particularly difficult standard bottomup models vision computing spatial relations visual search simulations show networks behavior matches much known psychophysical data human visual attention the general architecture model also closely matches known physiological data human attention system various extensions visit discussed including methods learning component modules
a lyapunov function excitatoryinhibitory networks constructed the construction assumes symmetric interactions within excitatory inhibitory populations neurons antisymmetric interactions populations the lyapunov function yields sufficient conditions global asymptotic stability fixed points if conditions violated limit cycles may stable the relations lyapunov function optimization theory classical mechanics revealed the dynamics neural network symmetric interactions provably converges fixed points general assumptions this mathematical result helped establish paradigm neural computation fixed point attractors but reality interactions neurons brain asymmetric furthermore dynamical behaviors seen brain confined fixed point attractors also include oscillations complex nonperiodic behavior these types dynamics realized asymmetric networks may useful neural computation for reasons important understand global behavior asymmetric neural networks the interaction excitatory neuron inhibitory neuron clearly asymmetric here consider class networks incorporates fundamental asymmetry brains microcircuitry networks class distinct populations excitatory inhibitory neurons antisymmetric interactions minimax dissipative hamiltonian forms network dynamics
in bayesian inference bayes factor defined ratio posterior odds versus prior odds posterior odds simply ratio normalizing constants two posterior densities in many practical problems two posteriors different dimensions for cases current monte carlo methods bridge sampling method meng wong path sampling method gelman meng ratio importance sampling method chen shao directly applied in article extend importance sampling bridge sampling ratio importance sampling problems different dimensions then find global optimal importance sampling bridge sampling ratio importance sampling sense minimizing asymptotic relative meansquare errors estimators implementation algorithms asymptotically achieve optimal simulation errors developed two illustrative examples also provided
as knowledge bases used ai systems increase size access relevant information dominant factor cost inference this especially true analogical casebased reasoning ability system perform inference dependent efficient flexible access large base exemplars cases judged likely relevant solving problem hand in chapter discuss novel algorithm efficient associative matching relational structures large semantic networks the structure matching algorithm uses massively parallel hardware search memory knowledge structures matching given probe structure the algorithm built top parka massively parallel knowledge representation system runs connection machine we currently exploring utility algorithm caper casebased planning system
we consider use online stopping rules reduce number training examples needed paclearn rather collect large training sample proved sufficient eliminate bad hypotheses priori idea instead observe training examples oneatatime decide online whether stop return hypothesis continue training the primary benefit approach detect hypothesizer actually converged halt training standard fixedsamplesize bounds this paper presents series sequential learning procedures distributionfree paclearning mistakebounded pac conversion distributionspecific paclearning respectively we analyze worst case expected training sample size procedures show often smaller existing fixed sample size bounds still providing exact worst case pacguarantees we also provide lower bounds show reductions best involve constant possibly log factors however empirical studies show sequential learning procedures actually use many times fewer training examples practice
this paper presents novel approach determine structural similarity guidance adaptation casebased reasoning cbr we advance structural similarity assessment provides single numeric value specific structure two cases common inclusive modification rules needed obtain structure two cases our approach treats retrieval matching adaptation group dependent processes this guarantees retrieval matching similar adaptable cases both together enlarge overall problem solving performance cbr explainability case selection adaptation considerably although approach theoretical nature restricted specific domain give example taken domain industrial building design additionally sketch two prototypical implementations approach
we analyze blameassignment task context experiencebased design redesign physical devices we identify three types blameassignment tasks differ types information take input design achieve desired behavior device design results undesirable behavior specific structural element design misbehaves we describe modelbased approach solving blameassignment task this approach uses structurebehaviorfunction models capture designers comprehension way device works terms causal explanations structure results behaviors we also address issue indexing models memory we discuss three types blameassignment tasks require different types indices accessing models finally describe kritik system implements evaluates modelbased approach blame assignment
we present framework taskdriven knowledge acquisition development design support systems different types knowledge enter knowledge base design support system defined illustrated formal knowledge acquisition vantage point special emphasis placed taskstructure used guide acquisition application knowledge starting knowledge planning steps design augmenting problemsolving knowledge supports design formal integrated model knowledge design constructed based notion knowledge acquisition incremental process give account possibilities problem solving depending knowledge disposal system finally depict different kinds knowledge interact design support system this research supported german ministry research technology bmft within joint project fabel contract iw project partners fabel german national research center computer science gmd sankt augustin bsr consulting gmbh munchen technical university dresden htwk leipzig university freiburg university karlsruhe
the activity sorting like objects classes without help omniscient supervisor known unsupervised classification in ai symbolic connectionist camps study classification the statistical classifiers autoclass snob search theory best explain distribution given data whereas neural network classifiers kohonens networks art use vector quantization principle classifying data previously many studies compared supervised classification algorithms challenging problem comparing unsupervised classifiers largely ignored we performed empirical comparison art autoclass snob we highlight strengths weaknesses various classifiers overall statistical classifiers especially snob perform better neural network counterpart art
ai research casebased reasoning led development many laboratory casebased systems as move towards introducing systems work environments explaining processes casebased reasoning becoming increasingly important issue in paper describe notion metacase illustrating explaining justifying casebased reasoning a metacase contains trace processing problemsolving episode provides explanation problemsolving decisions partial justification solution the language representing problemsolving trace depends model problem solving we describe taskmethod knowledge tmk model problemsolving describe representation metacases tmk language we illustrate explanatory scheme examples interactive kritik computerbased de
statistical decision theory provides principled way estimate amino acid frequencies conserved positions protein family the goal minimize risk function expected squarederror distance estimates true population frequencies the minimumrisk estimates obtained adding optimal number pseudocounts observed data two formulas presented one pseudocounts based marginal amino acid frequencies one pseudocounts based observed data experimental results show profiles constructed using minimalrisk estimates discriminating constructed using existing methods
the purpose paper propose refinement notion innateness if merely identify innateness bias obtain poor characterisation notion since learning device relies bias makes choose given hypothesis instead another we show intuition innateness better captured characteristic bias related isotropy generalist models learning shown rely isotropic bias whereas bias specialised models include specific priori knowledge learned necessarily anisotropic the socalled generalist models however turn specialised way learn symmetrical forms preferentially strictly deficiencies learning ability because learning beings always show two properties generalist models may sometimes ruled bad candidates cognitive modelling
reliable visionbased control autonomous vehicle requires ability focus attention important features input scene previous work autonomous lane following system alvinn pomerleau yielded good results uncluttered conditions this paper presents artificial neural network based learning approach handling difficult scenes confuse alvinn system this work presents mechanism achieving taskspecific focus attention exploiting temporal coherence a saliency map based upon computed expectation contents inputs next time step indicates regions input retina important performing task the saliency map used accentuate features important task deemphasize
production scheduling problem sequentially configuring factory meet forecasted demands critical problem throughout manufacturing industry the requirement maintaining product inventories face unpredictable demand stochastic factory output makes standard scheduling models jobshop inadequate currently applied algorithms simulated annealing constraint propagation must employ adhoc methods frequent replanning cope uncertainty in paper describe markov decision process mdp formulation production scheduling captures stochasticity production demands the solution mdp value function used generate optimal scheduling decisions online a simple example illustrates theoretical superiority approach replanningbased methods we describe industrial application two reinforcement learning methods generating approximate value function domain our results demonstrate deterministic noisy scenarios value function approximation effective technique
in paper investigate new formal model machine learning concept boolean function learned may exhibit uncertain probabilistic behaviorthus input may sometimes classified positive example sometimes negative example such probabilistic concepts pconcepts may arise situations weather prediction measured variables accuracy insufficient determine outcome certainty we adopt valiant model learning demands learning algorithms efficient general sense perform well wide class pconcepts distribution domain in addition giving many efficient algorithms learning natural classes pconcepts study develop detail underlying theory learning pconcepts
this paper highlights phenomenon causes deductively learned knowledge harmful used problem solving the problem occurs deductive problem solvers encounter failure branch search tree the backtracking mechanism problem solvers force program traverse whole subtree thus visiting many nodes twice using deductively learned rule using rules generated learned rule first place we suggest approach called utilization filtering solve problem learners use approach submit problem solver filter function together knowledge acquired the function decides problem whether use learned knowledge part use we tested idea context lemma learning system filter uses probability subgoal failing decide whether turn lemma usage experiments show improvement performance factor this paper concerned particular type harmful redundancy occurs deductive problem solvers employ backtracking search procedure use deductively learned knowledge accelerate search the problem failure branches search tree backtracking mechanism problem solver forces exploration whole subtree thus search procedure visit many states twice using deductively learned rule using search path produced rule first place
fl the authors thank rich yee vijay gullapalli brian pinette jonathan bachrach helping clarify relationships heuristic search control we thank rich sutton chris watkins paul werbos ron williams sharing fundamental insights subject numerous discussions thank rich sutton first making us aware korfs research thoughtful comments manuscript we grateful dimitri bertsekas steven sullivan independently pointing error earlier version article finally thank harry klopf whose insight persistence encouraged interest class learning problems this research supported grants ag barto national science foundation ecs ecs air force office scientific research bolling afb afosr
technical report osucisrc tr abstract one classical topics neural networks winnertakeall wta widely used unsupervised competitive learning cortical processing attentional control because global connectivity wta networks however encode spatial relations input thus support sensory perceptual processing spatial relations important we propose new architecture maintains spatial relations input features this selection network builds legion locally excitatory globally inhibitory oscillator networks dynamics slow inhibition in input scene many objects patterns network selects largest object this system easily adjusted select several largest objects alternate time we show twostage selection network gains efficiency combining selection parallel removal noisy regions the network applied select salient object real images as special case selection network without local excitation gives rise new form oscillatory wta
reinforcement learning rl become central paradigm solving learningcontrol problems robotics artificial intelligence rl researchers focussed almost exclusively problems controller maximize discounted sum payoffs however emphasized schwartz many problems eg optimal behavior limit cycle natural computationally advantageous formulate tasks controllers objective maximize average payoff received per time step in paper i derive new averagepayoff rl algorithms stochastic approximation methods solving system equations associated policy evaluation optimal control questions averagepayoff rl tasks these algorithms analogous popular td qlearning algorithms already developed discountedpayoff case one algorithms derived significant variation schwartzs rlearning algorithm preliminary empirical results presented validate new algorithms
we present algorithms exactly learning unknown environments described deterministic finite automata the learner performs walk target automaton step observes output state chooses labeled edge traverse next state the learner means reset access teacher answers equivalence queries gives learner counterexamples hypotheses we present two algorithms the first case outputs observed learner always correct second case outputs might corrupted random noise the running times algorithms polynomial cover time underlying graph target automaton
exploring mapping unknown environment fundamental problem studied variety contexts many works focused finding efficient solutions restricted versions problem in paper consider model makes limited assumptions environment solve mapping problem general setting we model environment unknown directed graph g consider problem robot exploring mapping g we assume vertices g labeled thus robot hope succeeding unless given means distinguishing vertices for reason provide robot pebble device place vertex use identify vertex later in paper show if robot knows upper bound number vertices learn graph efficiently one pebble if robot know upper bound number vertices n filog log n pebbles necessary sufficient in cases algorithms deterministic
in recent years increasing interest learning bayesian networks data one effective methods learning networks based minimum description length mdl principle previous work shown learning procedure asymptotically successful probability one converge target distribution given sufficient number samples however rate convergence hitherto unknown in work examine sample complexity mdl based learning procedures bayesian networks we show number samples needed learn close approximation terms entropy distance confidence ffi o log ffi log log this means sample complexity loworder polynomial error threshold sublinear confidence bound we also discuss constants term depend complexity target distribution finally address questions asymptotic minimality propose method using sample complexity results speed learning process
almost work averagereward reinforcement learning arl far focused tablebased methods scale domains large state spaces in paper propose two extensions modelbased arl method called hlearning address scaleup problem we extend hlearning learn action models reward functions form bayesian networks approximate value function using local linear regression we test algorithms several scheduling tasks simulated automatic guided vehicle agv show effective significantly reducing space requirement hlearning making converge faster to best knowledge results first apply ing function approximation arl
understanding highdimensional real world data usually requires learning structure data space the structure may contain highdimensional clusters related complex ways methods merge clustering selforganizing maps designed aid visualization interpretation data however methods often fail capture critical structural properties input although selforganizing maps capture highdimensional topology represent cluster boundaries discontinuities merge clustering extracts clusters capture local global topology this paper proposes algorithm combines topologypreserving characteristics selforganizing maps flexible adaptive structure learns cluster bound aries data
although building sophisticated learning agents operate complex environments require learning perform multiple tasks applications reinforcement learning focussed single tasks in paper i consider class sequential decision tasks sdts called composite sequential decision tasks formed temporally concatenating number elemental sequential decision tasks elemental sdts decomposed simpler sdts i consider learning agent learn solve set elemental composite sdts i assume structure composite tasks unknown learning agent the straightforward application reinforcement learning multiple tasks requires learning tasks separately waste computational resources memory time i present new learning algorithm modular architecture learns decomposition composite sdts achieves transfer learning sharing solutions elemental sdts across multiple composite sdts the solution composite sdt constructed computationally inexpensive modifications solutions constituent elemental sdts i provide proof one aspect learning algorithm
existing approaches learning control robot arm rely supervised methods correct behavior explicitly given it difficult learn avoid obstacles using methods however examples obstacle avoidance behavior hard generate this paper presents alternative approach evolves neural network controllers genetic algorithms no inputoutput examples necessary since neuroevolution learns single performance measurement entire task grasping object the approach tested simulation oscar robot arm receives visual sensory input neural networks evolved effectively avoid obstacles various locations reach random target locations
it widely accepted use compact representations lookup tables crucial scaling reinforcement learning rl algorithms realworld problems unfortunately almost theory reinforcement learning assumes lookup table representations in paper address pressing issue combining function approximation rl present function approximator based simple extension state aggregation commonly used form compact representation namely soft state aggregation theory convergence rl arbitrary fixed soft state aggregation novel intuitive understanding effect state aggregation online rl new heuristic adaptive state aggregation algorithm finds improved compact representations exploiting nondiscrete nature soft state aggregation preliminary empirical results also presented
this article introduces class incremental learning procedures specialized predictionthat using past experience incompletely known system predict future behavior whereas conventional predictionlearning methods assign credit means difference predicted actual outcomes new methods assign credit means difference temporally successive predictions although temporaldifference methods used samuels checker player hollands bucket brigade authors adaptive heuristic critic remained poorly understood here prove convergence optimality special cases relate supervisedlearning methods for realworld prediction problems temporaldifference methods require less memory less peak computation conventional methods produce accurate predictions we argue problems supervised learning currently applied really prediction problems sort temporaldifference methods applied advantage
this paper extends previous work dyna class architectures intelligent systems based approximating dynamic programming methods dyna architectures integrate trialanderror reinforcement learning executiontime planning single process operating alternately world learned model world in paper i present show results two dyna architectures the dynapi architecture based dynamic programmings policy iteration method related existing ai ideas evaluation functions universal plans reactive systems using navigation task results shown simple dynapi system simultaneously learns trial error learns world model plans optimal routes using evolving world model the dynaq architecture based watkinss qlearning new kind reinforcement learning dynaq uses less familiar set data structures dynapi arguably simpler implement use we show dynaq architectures easy adapt use changing environments
on large problems reinforcement learning systems must use parameterized function approximators neural networks order generalize similar situations actions in cases strong theoretical results accuracy convergence computational results mixed in particular boyan moore reported last years meeting series negative results attempting apply dynamic programming together function approximation simple control problems continuous state spaces in paper present positive results control tasks attempted one significantly larger the important differences used sparsecoarsecoded function approximators cmacs whereas used mostly global function approximators learned online whereas learned oine boyan moore others suggested problems encountered could solved using actual outcomes rollouts classical monte carlo methods td algorithm however experiments always resulted substantially poorer performance we conclude reinforcement learning work robustly conjunction function approximators little justification present avoiding case general
we consider requirements online learninglearning must done incrementally realtime results learning available soon new example acquired despite abundance methods learning examples used effectively online learning eg components reinforcement learning systems most including radial basis functions cmacs kohonens selforganizing maps developed paper share structure all expand original input representation higher dimensional representation unsupervised way map representation final answer using relatively simple supervised learner perceptron lms rule such structures learn rapidly reliably thought either scale poorly require extensive domain knowledge to contrary researchers rosenblatt gallant smith kanerva prager fallside argued expanded representation chosen largely random good results the main contribution paper develop test hypothesis we show simple randomrepresentation methods perform well nearestneighbor methods suited online learning significantly better backpropagation we find size random representation increase dimensionality problem unreasonably required size reduced substantially using unsupervisedlearning techniques our results suggest randomness useful role play online supervised learning constructive induction
we consider problem dynamically apportioning resources among set options worstcase online framework the model study interpreted broad abstract extension wellstudied online prediction model general decisiontheoretic setting we show multiplicative weightupdate rule littlestone warmuth adapted model yielding bounds slightly weaker cases applicable considerably general class learning problems we show resulting learning algorithm applied variety problems including gambling multipleoutcome prediction repeated games prediction points r n
a new online learning algorithm minimizes statistical dependency among outputs derived blind separation mixed signals the dependency measured average mutual information mi outputs the source signals mixing matrix unknown except number sources the gramcharlier expansion instead edgeworth expansion used evaluating mi the natural gradient approach used minimize mi a novel activation function proposed online learning algorithm equivariant property easily implemented neural network like model the validity new learning algorithm verified computer simulations
overfitting wellknown problem fields symbolic connectionist machine learning it describes deterioration generalisation performance trained model in paper investigate ability novel artificial neural network bpsom avoid overfitting bpsom hybrid neural network combines multilayered feedforward network mfn kohonens selforganising maps soms during training supervised backpropagation learning unsupervised som learning cooperate finding adequate hiddenlayer representations we show bpsom outperforms standard backpropagation also backpropagation weight decay dealing problem overfitting in addition show bpsom succeeds preserving generalisation performance hiddenunit pruning methods fail
we describe model iterated belief revision extends agm theory revision account effect revision conditional beliefs agent in particular model ensures agent makes changes possible conditional component belief set adopting ramsey test minimal conditional revision provides acceptance conditions arbitrary rightnested conditionals we show problem determining acceptance nested conditional reduced acceptance tests unnested conditionals thus iterated revision accomplished virtual manner using uniterated revision
reinforcement learning techniques address problem learning select actions unknown dynamic environments it widely acknowledged use complex domains reinforcement learning techniques must combined generalizing function approximation methods artificial neural networks little however understood theoretical properties combinations many researchers encountered failures practice in paper identify prime source failuresnamely systematic overestimation utility values using watkins qlearning example give theoretical account phenomenon deriving conditions one may expected cause learning fail employing popular function approximators present experimental results support theoretical findings
we derive new selforganising learning algorithm maximises information transferred network nonlinear units the algorithm assume knowledge input distributions defined zeronoise limit under conditions information maximisation extra properties found linear case linsker the nonlinearities transfer function able pick higherorder moments input distributions perform something akin true redundancy reduction units output representation this enables network separate statistically independent components inputs higherorder generalisation principal components analysis we apply network source separation cocktail party problem successfully separating unknown mixtures ten speakers we also show variant network architecture able perform blind deconvolution cancellation unknown echoes reverberation speech signal finally derive dependencies information transfer time delays we suggest information maximisation provides unifying framework problems blind signal processing fl please send comments tonysalkedu this paper appear neural computation the reference version technical report inc february institute neural computation ucsd san diego ca
this paper multidisciplinary review empirical statistical learning graphical model perspective wellknown examples graphical models include bayesian networks directed graphs representing markov chain undirected networks representing markov field these graphical models extended model data analysis empirical learning using notation plates graphical operations simplifying manipulating problem provided including decomposition differentiation manipulation probability models exponential family two standard algorithm schemas learning reviewed graphical framework gibbs sampling expectation maximization algorithm using operations schemas popular algorithms synthesized graphical specification this includes versions linear regression techniques feedforward networks learning gaussian discrete bayesian networks data the paper concludes sketching implications data analysis summarizing popular algorithms fall within framework presented
the utility problem speedup learning describes common behavior machine learning methods eventual degradation performance due increasing amounts learned knowledge the shape learning curve cost using learning method vs number training examples several domains suggests parameterized model relating performance amount learned knowledge mechanism limit amount learned knowledge optimal performance many recent approaches avoiding utility problem speedup learning rely sophisticated utility measures significant numbers training data accurately estimate utility control knowledge empirical results presented elsewhere indicate simple selection strategy retaining control rules derived training problem explanation quickly defines efficient set control knowledge training problems this simple selection strategy provides lowcost alternative exampleintensive approaches improving speed problem solver experimentation illustrates existence minimum representing least cost learning curve reached training examples stress placed controlling amount learned knowledge opposed knowledge an attempt also made relate domain characteristics shape learning curve
we compare kernel estimators single multilayered perceptrons radialbasis functions problems classification handwritten digits speech phonemes by taking two different applications employing many techniques report twodimensional study whereby domainindependent assessment learning methods possible we consider feedforward network one hidden layer as examples local methods use kernel estimators like knearest neighbor knn parzen windows generalized knn grow learn condensed nearest neighbor we also considered fuzzy knn due similarity as distributed networks use linear perceptron pairwise separating linear perceptron multilayer perceptrons sigmoidal hidden units we also tested radialbasis function network combination local distributed networks four criteria taken comparison correct classification test set network size learning time operational complexity we found perceptrons architecture suitable generalize better local memorybased kernel estimators require longer training precise computation local networks simple learn quickly acceptably use memory
in current cbr systems case adaptation usually performed rulebased methods use taskspecific rules handcoded system developer the ability define rules depends knowledge task domain may available priori presenting serious impediment endowing cbr systems needed adaptation knowledge this paper describes ongoing research method address problem acquiring adaptation knowledge experience the method uses reasoning scratch based introspective reasoning requirements successful adaptation build library adaptation cases stored future reuse we describe tenets approach types knowledge requires we sketch initial computer implementation lessons learned open questions study
this position paper sketches framework modeling introspective reasoning discusses relevance framework modeling introspective reasoning memory search it argues effective flexible memory processing rich memories built five types explicitly represented selfknowledge knowledge information needs relationships different types information expectations actual behavior information search process desires ideal behavior representations expectations desires relate actual performance this approach modeling memory search illustration general principles modeling introspective reasoning step towards addressing problem reasoner human machinecan acquire knowledge properties knowledge base
machine learning techniques perceived great potential means acquisition knowledge nevertheless use complex engineering domains still rare most machine learning techniques studied context knowledge acquisition well defined tasks classification learning tasks handled relatively simple algorithms complex domains present difficulties approached combining strengths several complementing learning techniques overcoming weaknesses providing alternative learning strategies this study presents two perspectives macro micro viewing issue multistrategy learning the macro perspective deals decomposition overall complex learning task relatively welldefined learning tasks micro perspective deals designing multistrategy learning techniques supporting acquisition knowledge task the two perspectives discussed context
in order learn effectively reasoner must possess knowledge world able improve knowledge also must introspectively reason performs given task particular pieces knowledge needs improve performance current task introspection requires declarative representations metaknowledge reasoning performed system performance task systems knowledge organization knowledge this chapter presents taxonomy possible reasoning failures occur performance task declarative representations failures associations failures particular learning strategies the theory based metaxps explanation structures help system identify failure types formulate learning goals choose appropriate learning strategies order avoid similar mistakes future the theory implemented computer model introspective reasoner performs multistrategy learning story understanding task
we introduce learning algorithm unsupervised neural networks based ideas statistical mechanics the algorithm derived mean field approximation large layered sigmoid belief networks we show approximately infer statistics networks without resort sampling this done solving mean field equations relate statistics unit markov blanket using statistics target values weights network adapted local delta rule we evaluate strengths weaknesses networks problems statistical pattern recognition
the paper describes selflearning control system mobile robot based sensor information control system provide steering signal way collisions avoided since case examples available system learns basis external reinforcement signal negative case collision zero otherwise we describe adaptive algorithm used discrete coding state space adaptive algorithm learning correct mapping input state vector output steering signal
fl partially supported advanced research projects agency afosr partially supported air force office scientific research afosr fj advanced research projects agency onr nj office naval research onr nj z partially funded air force office scientific research afosr fj office naval research onr nj onr n
the problem approximating smooth l p functions spaces spanned integer translates radially symmetric function well understood in case points translation ffi scattered throughout r approximation problem well understood stationary setting in work treat nonstationary setting assumption ffi small perturbation z our results similar many respects known results case ffi z apply specifically examples gauss kernel generalized multiquadric
in paper initiate investigation generalizations probably approximately correct pac learning model attempt significantly weaken target function assumptions the ultimate goal direction informally termed agnostic learning make virtually assumptions target function the name derives fact designers learning algorithms give belief nature represented target function simple succinct explanation we give number positive negative results provide initial outline possibilities agnostic learning our results include hardness results obvious generalization pac model agnostic setting efficient general agnostic learning method based dynamic programming relationships loss functions agnostic learning algorithm learning problem involves hidden variables
in paper describe design implementation derivation replay framework dersnlpebl derivational snlpebl based within partial order planner dersnlpebl replays previous plan derivations first repeating earlier decisions context new problem situation extending replayed path obtain complete solution new problem when replayed path extended new solution explanationbased learning ebl techniques employed identify features new problem prevent extension these features added censors retrieval stored case to keep retrieval costs low dersnlpebl normally stores plan derivations individual goals replays one derivations solving multigoal problems cases covering multiple goals stored subplans individual goals successfully merged the aim constructing case library predict goal interactions store multigoal case set negatively interacting goals we provide empirical results demonstrating effectiveness dersnlpebl improving planning performance randomlygenerated problems drawn complex domain
previous algorithms supervised sequence learning based dynamic recurrent networks this paper describes alternative class gradientbased systems consisting two feedforward nets learn deal temporal sequences using fast weights the first net learns produce context dependent weight changes second net whose weights may vary quickly the method offers potential stm storage efficiency a single weight instead fullfledged unit may sufficient storing temporal information various learning methods derived two experiments unknown time delays illustrate approach one experiment shows system used adaptive temporary variable binding
evolutionary tree reconstruction important step many biological research problems yet extremely difficult variety computational statistical scientific reasons in particular reconstruction large trees containing significant amounts divergence especially challenging we present paper new tree reconstruction method call diskcovering method used recover accurate estimations evolutionary tree otherwise intractable datasets dcm obtains decomposition input dataset small overlapping sets closely related taxa reconstructs trees subsets using base phylogenetic method choice combines subtrees one tree entire set taxa because subproblems analyzed dcm smaller computationally expensive methods maximum likelihood estimation used without incurring much cost at time taxa within subset closely related even simple methods neighborjoining much likely highly accurate the result dcmboosted methods typically faster accurate compared naive use method in paper describe basic ideas techniques dcm demonstrate advantages dcm experimentally simulating sequence evolution variety trees
automating construction semantic grammars difficult interesting problem machine learning this paper shows semanticgrammar acquisition problem viewed learning searchcontrol heuristics logic program appropriate control rules learned using new firstorder induction algorithm automatically invents useful syntactic semantic categories empirical results show learned parsers generalize well novel sentences outperform previous approaches based connectionist techniques
conventional speculative architectures use branch prediction evaluate likely execution path program execution however certain branches difficult predict one solution problem evaluate paths following conditional branch predicated execution used implement form multipath execution predicated architectures fetch issue instructions associated predicates these predicates indicate instruction commit result predicating branch reduces number branches executed eliminating chance branch misprediction cost executing additional instructions in paper propose restricted form multipath execution called dynamic predication architectures little support predicated instructions instruction set dynamic predication dynamically predicates instruction sequences form branch hammock concurrently executing paths branch a branch hammock short forward branch spans instructions form ifthen ifthenelse construct we mark constructs executable when decode stage detects sequence passes predicated instruction sequence dynamically scheduled execution core our results show dynamic predication accrue speedups
in barn owl selforganization auditory map space external nucleus inferior colliculus icx strongly influenced vision nature interaction unknown in paper biologically plausible minimalistic model icx selforganization proposed icx receives learn signal based owls visual attention when visual attention focused spatial location auditory input learn signal turned map allowed adapt a twodimensional kohonen map used model icx simulations performed evaluate learn signal would affect auditory map when primary area visual attention shifted different spatial locations auditory map shifted corresponding location the shift complete done early development partial done later similar results observed barn owl visual field modified prisms therefore simulations suggest learn signal based visual attention possible explanation auditory plasticity
the place fields hippocampal cells old animals sometimes change animal removed returned environment barnes et al the ensemble correlation two sequential visits environment shows strong bimodality old animals near indicative remapping greater indicative similar representation experiences strong unimodality young animals greater indicative similar representation experiences one explanation multimap hypothesis multiple maps encoded hippocampus old animals may sometimes returning wrong map a theory proposed samsonovich mcnaughton suggests barnes et al experiment implies maps prewired ca region hippocampus here offer alternative explanation orthogonalization properties dentate gyrus dg region hippocampus interact errors selflocalization reset path integrator reentry environment produce bimodality
mit media laboratory perceptual computing section technical report no appeared th ieee intl conference pattern recognition icpr vienna austria abstract we present foveated gesture recognition system guides active camera foveate salient features based reinforcement learning paradigm using vision routines previously implemented interactive environment determine spatial location salient body parts user guide active camera obtain images gestures expressions a hiddenstate reinforcement learning paradigm based partially observable markov decision process pomdp used implement visual attention the attention module selects targets foveate based goal successful recognition uses new multiplemodel qlearning formulation given set target distractor gestures system learn foveate maximally discriminate particular gesture
various extensions genetic algorithm ga attempt find optima search space containing several optima many emulate natural speciation for coevolutionary learning succeed range management control problems learning game strategies methods must find optima however suitable comparison studies rare we compare two similar ga speciation methods fitness sharing implicit sharing using realistic letter classification problem find advantages different circumstances implicit sharing covers optima comprehensively population large enough species form optimum with population large enough fitness sharing find optima larger basins attraction ignore peaks narrow bases implicit sharing easily distracted this indicates speciated ga trying find many nearglobal optima possible implicit sharing works well population large enough this requires prior knowledge many peaks exist
modern knowledge systems design typically employ multiple problemsolving methods turn use different kinds knowledge the construction heterogeneous knowledge system support practical design thus raises two fundamental questions accumulate huge volumes design information support heterogeneous design processing fortunately partial answers questions exist separately legacy databases already contain huge amounts generalpurpose design information in addition modern knowledge systems typically characterize kinds knowledge needed specific problemsolving methods quite precisely this leads us hypothesize methodspecific datatoknowledge compilation potential mechanism integrating heterogeneous knowledge systems legacy databases design in paper first outline general computational architecture called hiped integration then focus specific issue convert data accessed legacy database form appropriate problemsolving method used heterogeneous knowledge system we describe experiment legacy knowledge system called interactive kritik integrated oracle database using idi communication tool the limited experiment indicates computational feasibility methodspecific datatoknowledge compilation also raises additional research issues
this paper investigates advantages disadvantages mixture experts me model introduced connectionist community jjnh applied time series analysis wm two time series dynamics well understood the first series computergenerated series consisting mixture noisefree process quadratic map noisy process composition noisy linear autoregressive hyperbolic tangent there three main results me model produces significantly better results single networks discovers regimes correctly also allows us characterize subprocesses variances due correct matching noise level model data avoids overfitting the second series laser series used santa fe competition me model also obtains excellent outofsample predictions allows analysis shows overfitting
in natural visual experience different views object tend appear close temporal proximity animal manipulates object navigates around we investigated ability attractor network acquire view invariant visual representations associating first neighbors pattern sequence the pattern sequence contains successive views faces ten individuals change pose under network dynamics developed griniasty tsodyks amit multiple views given subject fall basin attraction we use independent component ica representation faces input patterns bell sejnowski the ica representation advantages principal component representation pca viewpointinvariant recognition without attractor network suggesting ica better representation pca object recognition
this paper examines effects relaxed synchronization numerical parallel efficiency parallel genetic algorithms gas we describe coarsegrain geographically structured parallel genetic algorithm our experiments provide preliminary evidence asynchronous versions algorithms lower run time synchronous gas our analysis shows improvement due decreased synchronization costs high numerical efficiency eg fewer function evaluations asynchronous gas this analysis includes critique utility traditional parallel performance measures parallel gas
key ideas statistical learning theory support vector machines generalized decision trees a support vector machine used decision tree the optimal decision tree characterized primal dual space formulation constructing tree proposed the result method generating logically simple decision trees multivariate linear nonlinear decisions the preliminary results indicate method produces simple trees generalize well respect decision tree algorithms single support vector machines
we previously shown regularization principles lead approximation schemes equivalent networks one layer hidden units called regularization networks in particular standard smoothness functionals lead subclass regularization networks well known radial basis functions approximation schemes this paper shows regularization networks encompass much broader range approximation schemes including many popular general additive models neural networks in particular introduce new classes smoothness functionals lead different classes basis functions additive splines well tensor product splines obtained appropriate classes smoothness functionals furthermore generalization extends radial basis functions rbf hyper basis functions hbf also leads additive models ridge approximation models containing special cases breimans hinge functions forms projection pursuit regression several types neural networks we propose use term generalized regularization networks broad class approximation schemes follow extension regularization in probabilistic interpretation regularization different classes basis functions correspond different classes prior probabilities approximating function spaces therefore different types smoothness assumptions in summary different multilayer networks one hidden layer collectively call generalized regularization networks correspond different classes priors associated smoothness functionals classical regularization principle three broad classes radial basis functions generalized hyper basis functions b tensor product splines c additive splines generalized schemes type ridge approximation hinge functions several perceptronlike neural networks onehidden layer this paper appear neural computation vol pages an earlier version
learning inputoutput mapping set examples type many neural networks constructed perform regarded synthesizing approximation multidimensional function solving problem hypersurface reconstruction from point view form learning closely related classical approximation techniques generalized splines regularization theory this paper considers problems exact representation detail approximation linear nonlinear mappings terms simpler functions fewer variables kolmogorovs theorem concerning representation functions several variables terms functions one variable turns almost irrelevant context networks learning we develop theoretical framework approximation based regularization techniques leads class threelayer networks call generalized radial basis functions grbf since mathematically related wellknown radial basis functions mainly used strict interpolation tasks grbf networks equivalent generalized splines also closely related pattern recognition methods parzen windows potential functions several neural network algorithms kanervas associative memory backpropagation kohonens topology preserving map they also interesting interpretation terms prototypes synthesized optimally combined learning stage the paper introduces several extensions applications technique discusses intriguing analogies neurobiological data c fl massachusetts institute technology this paper describes research done within center biological information processing department brain cognitive sciences artificial intelligence laboratory this research sponsored grant office naval research onr cognitive neural sciences division artificial intelligence center hughes aircraft corporation alfred p sloan foundation national science foundation support a i laboratorys artificial intelligence research provided advanced research projects agency department defense army contract dacac part onr contract nk
this article describes reasoner improve understanding incompletely understood domain application already knows novel problems domain casebased reasoning process using past experiences stored reasoners memory understand novel situations solve novel problems however process assumes past experiences well understood provide good lessons used future situations this assumption usually false one learning novel domain since situations encountered previously domain might understood completely furthermore reasoner may even case adequately deals new situation may able access case using existing indices we present theory incremental learning based revision previously existing case knowledge response experiences situations the theory implemented casebased story understanding program learn new case situations case already exists b learn index case memory c incrementally refine understanding case using reason new situations thus evolving better understanding domain experience this research complements work casebased reasoning providing mechanisms case library automatically built use casebased reasoning program
we present statistical model genes dna a generalized hidden markov model ghmm provides framework describing grammar legal parse dna sequence stormo haussler probabilities assigned transitions states ghmm generation nucleotide base given particular state machine learning techniques applied optimize probabilities using standardized training set given new candidate sequence best parse deduced model using dynamic programming algorithm identify path model maximum probability the ghmm flexible modular new sensors additional states inserted easily in addition provides simple solutions integrating cardinality constraints reading frame constraints indels homology searching the description results implementation genefinding model called genie presented the exon sensor codon frequency model conditioned windowed nucleotide frequency preceding codon two neural networks used brunak engelbrecht knudsen splice site prediction we show simple model performs quite well for crossvalidated standard test set genes ftpwwwhgclblgovpubgenesets human dna genefinding system identified proteincoding bases correctly specificity exons exactly identified specificity genie shown perform favorably compared several genefinding systems
we examine questions optimality domination repeated stage games one players may draw strategies perhaps different computationally bounded sets we also consider optimality domination bounded convergence rates infinite payoff we develop notion grace period handle problem vengeful strategies
we study problem efficiently learning play game optimally unknown adversary chosen computationally bounded class we contribute line research playing games finite automata expand scope research considering new classes adversaries we introduce natural notions games recent history adversaries whose current action determined simple boolean formula recent history play games statistical adversaries whose current action determined simple function statistics entire history play in cases give efficient algorithms learning play pennymatching difficult game called contract we also give powerful positive result date learning play finite automata efficient algorithm learning play game finite automata probabilistic actions low cover time
morgan integrated system finding genes vertebrate dna sequences morgan uses variety techniques accomplish task distinctive decision tree classifier the decision tree system combined new methods identifying start codons donor sites acceptor sites brought together framesensitive dynamic programming algorithm finds optimal segmentation dna sequence coding noncoding regions exons introns the optimal segmentation dependent separate scoring function takes subsequence assigns score reflecting probability sequence exon the scoring functions morgan sets decision trees combined give probability estimate experimental results database vertebrate dna sequences show morgan excellent performance many different measures on separate test set achieves overall accuracy correlation coefficient sensitivity specificity coding bases in addition morgan identifies coding exons exactly ie beginning end coding regions predicted correctly this paper describes morgan system including decision tree routines algorithms site recognition performance benchmark database vertebrate dna
in paper report use backpropagation based neural networks implement phase computational intelligence process pythia expert system supporting numerical simulation applications modelled partial differential equations pdes pythia exemplar based reasoning system provides advice method parameters use simulation specified pde based application when advice requested characteristics given model matched characteristics previously seen classes models the performance various solution methods previously seen similar classes models used basis predicting method use thus major step reasoning process pythia involves analysis categorization models classes models based characteristics in study demonstrate use neural networks identify class predefined models whose characteristics match ones specified pde based application
a method described reduces hypotheses space efficient easily interpretable reduction criteria called reduction a learning algorithm described based reduction analyzed using probability approximate correct learning results the results obtained reducing rule set equivalent set kdnf formulas the goal learning algorithm induce compact rule set describing basic dependencies within set data the reduction based criterion exible gives semantic interpretation rules fulfill criteria comparison syntactical hypotheses reduction show reduction improves search smaller probability missclassification
we identify three principle factors affecting performance learning networks localized units unit noise sample density structure target function we analyze effect unit receptive field parameters factors use analysis propose new learning algorithm dynamically alters receptive field properties learning
semimarkov decision problems continuous time generalizations discrete time markov decision problems a number reinforcement learning algorithms developed recently solution markov decision problems based ideas asynchronous dynamic programming stochastic approximation among td qlearning realtime dynamic programming after reviewing semimarkov decision problems bellmans optimality equation context propose algorithms similar named adapted solution semimarkov decision problems we demonstrate algorithms applying problem determining optimal control simple queueing system we conclude discussion circumstances algorithms may usefully ap plied
there recently widespread interest use multiple models classification regression statistics neural networks communities the hierarchical mixture experts hme successful number regression problems yielding significantly faster training use expectation maximisation algorithm in paper extend hme classification results reported three common classification benchmark tests exclusiveor ninput parity two spirals
one important factor determining computa tional complexity evaluating probabilistic network cardinality state spaces nodes by varying granularity state spaces one trade accuracy result computational efficiency we present time procedure approximate evaluation probabilistic networks based idea on application simple networks proce dure exhibits smooth improvement approxi mation quality computation time increases this suggests statespace abstraction one useful control parameter designing real time probabilistic reasoners
existing complexity measures contemporary learning theory conveniently applied specific learning problems eg training sets moreover typically nongeneric ie necessitate making assumptions way learner operate the lack satisfactory generic complexity measure learning problems poses difficulties researchers various areas present paper puts forward idea may help alleviate it shows supervised learning problems fall two generic complexity classes one associated computational tractability by determining class particular problem belongs thus effectively evaluate degree generic difficulty
the paper investigates statistical effects may need exploited supervised learning it notes effects classified according conditionality order proposes learning algorithms typically form bias towards particular classes effect it presents results empirical study statistical bias backpropagation the study involved applying algorithm wide range learning problems using variety different internal architectures the results study revealed backpropagation specific bias general direction statistical rather relational effects the paper shows existence bias effectively constitutes weakness algorithms ability discount noise
segmentation preliminary results abstract scatterpartitioning radial basis function rbf networks increase number degrees freedom complexity inputoutput mapping estimated basis supervised training data set due superior expressive power scatterpartitioning gaussian rbf grbf model termed supervised growing neural gas sgng selected literature sgng employs onestage errordriven learning strategy capable generating removing hidden units synaptic connections a slightly modified sgng version tested function estimator training surface fitted image ie d signal whose size finite the relationship generation learning system disjointed maps hidden units presence image pictorially homogeneous subsets segments investigated unfortunately examined sgng version performs poorly function estimator image segmenter this may due intrinsic inadequacy onestage errordriven learning strategy adjust structural parameters output weights simultaneously consistently in framework rbf networks studies investigate combination twostage errordriven learning strategies synapse generation removal criteria internal report paper entitled image segmentation scatterpartitioning rbf networks a feasibility study presented conference applications science neural networks fuzzy systems evolutionary computation part spies international symposium optical science engineering instrumentation july san diego ca
a distinct advantage symbolic learning algorithms artificial neural networks typically concept representations form easily understood humans one approach understanding representations formed neural networks extract symbolic rules trained networks in paper describe investigate approach extracting rules networks uses nofm extraction algorithm network training method soft weightsharing previously nofm algorithm successfully applied knowledgebased neural networks our experiments demonstrate extracted rules generalize better rules learned using c system in addition accurate extracted rules also reasonably comprehensible
our experience showed us exibility expressing parallel algorithm simulating neural networks desirable even possible obtain efficient solution single training algorithm we believe advantages clear easy understand program predominates disadvantages approaches allowing specific machine neural network algorithm we currently investigate neural network models worth parallelized resulting parallel algorithms composed common basic building blocks logarithmic tree efficient communication structure connections connections d ackley g hinton t sejnowski a learning algorithm boltzmann machines cognitive science pp b m forrest et al implementing neural network models parallel computers the computer journal vol w giloi latency hiding message passing architectures international parallel processing symposium april cancun mexico ieee computer society press t nordstrm b svensson using and designing massively parallel computers artificial neural networks journal of parallel and distributed computing vol pp a kramer a vincentelli efficient parallel learning algorithms neural networks advances neural information processing systems i d touretzky ed pp t kohonen selforganization associative memory springerverlag berlin d a pomerleau g l gusciora d l touretzky h t kung neural network simulation warp speed how we got million connections per second ieee intern conf neural networks july a rbel dynamic selection training patterns neural networks a new method control generalization technical report technical university berlin d e rumelhart d e hinton r j williams learning internal representations error propagation rumelhart mcclelland eds parallel distributed processing explorations microstructure cognition vol i pp bradford booksmit press cambridge ma w schiffmann m joost r werner comparison optimized backpropagation algorithms proc european symposium artificial neural networks esann brussels pp j schmidhuber accelerated learning backpropagation nets connectionism perspective elsevier science publishers bv northholland pp m taylor p lisboa eds techniques applications neural networks ellis horwood m witbrock m zagha an implementation backpropagation learning gf large simd parallel computer parallel computing vol pp x zhang m mckenna j p mesirov d l waltz the backpropagation algorithm grid hypercube architectures parallel computing vol pp
in order learn effectively system must possess knowledge world able improve knowledge also must introspectively reason performs given task particular pieces knowledge needs improve performance current task introspection requires declaratflive representation reasoning performed system performance task this paper presents taxonomy possible reasoning failures occur task declarative representations associations particular learning strategies we propose theory metaxps explanation structures help system identify failure types choose appropriate learning strategies order avoid similar mistakes future a program called metaaqua embodies theory processes examples domain drug smuggling
although artificial neural networks applied variety realworld scenarios remarkable success often criticized exhibiting low degree human comprehensibility techniques compile compact sets symbolic rules artificial neural networks offer promising perspective overcome obvious deficiency neural network representations this paper presents approach extraction ifthen rules artificial neural networks its key mechanism validity interval analysis generic tool extracting symbolic knowledge propagating rulelike knowledge backpropagationstyle neural networks empirical studies robot arm domain illustrate appropriateness proposed method extracting rules networks realvalued distributed representations
in paper examine method feature subset selection based information theory initially framework defining theoretically optimal computationally intractable method feature subset selection presented we show goal eliminate feature gives us little additional information beyond subsumed remaining features in particular case irrelevant redundant features we give efficient algorithm feature selection computes approximation optimal feature selection criterion the conditions approximate algorithm successful examined empirical results given number data sets showing algorithm effectively han dles datasets large numbers features
in paper address problem casebased learning presence irrelevant features we review previous work attribute selection present new algorithm oblivion carries greedy pruning oblivious decision trees effectively store set abstract cases memory we hypothesize approach efficiently identify relevant features even interact parity concepts we report experimental results artificial domains support hypothesis experiments natural domains show improvement cases others in closing discuss implications experiments consider additional work irrelevant features outline directions future research
learning plays vital role development situated agents in paper explore use reinforcement learning shape robot perform predefined target behavior we connect simulated real robots a lecsys parallel implementation learning classifier system extended genetic algorithm after classifying different kinds animatlike behaviors explore effects learning different types agents architecture monolithic flat hierarchical training strategies in particular hierarchical architecture requires agent learn coordinate basic learned responses we show best results achieved agents architecture training strategy match structure behavior pattern learned we report results number experiments carried simulated real environments show results simulations carry smoothly real robots while experiments deal simple reactive behavior one demonstrate use simple general memory mechanism as whole experimental activity demonstrates classifier systems genetic algorithms practically employed develop autonomous agents
although probabilistic inference general bayesian belief network nphard problem inference computation time reduced practical cases exploiting domain knowledge making appropriate approximations knowledge representation in paper introduce property similarity states new method approximate knowledge representation based property we define two states node similar likelihood ratio probabilities depend instantiations nodes network we show similarity states exposes redundancies joint probability distribution exploited reduce computational complexity probabilistic inference networks multiple similar states for example show bno networka two layer networks often used diagnostic problemscan reduced close network multiple similar states probabilistic inference new network done polynomial time respect size network results queries practical importance close results obtained exponential time original network the error introduced reduction converges zero faster exponentially respect degree polynomial describing resulting computational complexity
multilayer architectures used bayesian belief networks helmholtz machines provide powerful framework representing learning higher order statistical relations among inputs because exact probability calculations models often intractable much interest finding approximate algorithms we present algorithm efficiently discovers higher order structure using em gibbs sampling the model interpreted stochastic recurrent network ambiguity lowerlevel states resolved feedback higher levels we demonstrate performance algorithm bench mark problems
in paper study extension distributionfree model learning introduced valiant also known probably approximately correct pac model allows presence malicious errors examples given learning algorithm such errors generated adversary unbounded computational power access entire history learning algorithms computation thus study worstcase model errors our results include general methods bounding rate error tolerable learning algorithm efficient algorithms tolerating nontrivial rates malicious errors equivalences problems learning errors standard combinatorial optimization problems
we investigate problem computing posterior probability model class given data sample prior distribution possible parameter settings by model class mean group models share parametric form in general posterior may hard compute highdimensional parameter spaces usually case realworld applications in literature several methods computing posterior approximately proposed quality approximations may depend heavily size available data sample in work interested testing well approximative methods perform realworld problem domains in order conduct study chosen model family finite mixture distributions with certain assumptions able derive model class posterior analytically model family we report series model class selection experiments realworld data sets true posterior approximations compared the empirical results support hypothesis approximative techniques provide good estimates true posterior especially sample size grows large
email firstnamelastnamecshelsinkifi report c university helsinki department computer science abstract in paper explore use finite mixture models building decision support systems capable sound probabilistic inference finite mixture models many appealing properties computationally efficient prediction reasoning phase universal sense approximate problem domain distribution handle multimodality well we present formulation model construction problem bayesian framework finite mixture models describe bayesian inference performed given model the model construction problem seen missing data estimation describe realization expectationmaximization em algorithm finding good models to prove feasibility approach report crossvalidated empirical results several publicly available classification problem datasets compare results corresponding results obtained alternative techniques neural networks decision trees the comparison based best results reported literature datasets question it appears using theoretically sound bayesian framework suggested reported results outperformed relatively small effort
one application models reasoning behavior allow reasoner introspectively detect repair failures reasoning process we address issues transferability models versus specificity knowledge kinds knowledge needed selfmodeling knowledge structured evaluation introspective reasoning systems we present robbie system implements model planning processes improve planner response reasoning failures we show robbies hierarchical model balances model generality access implementationspecific details discuss qualitative quantitative measures used evaluating introspective component
we consider multicriteria sequential decision making problems criteria ordered according importance structural properties problems touched reinforcement learning algorithms learn asymptotically optimal decisions derived computer experiments confirm theoretical results provide insight learning processes
acyclic digraphs adgs widely used describe dependences among variables multivariate distributions in particular likelihood functions adg models admit convenient recursive factorizations often allow explicit maximum likelihood estimates well suited building bayesian networks expert systems there may however many adgs determine dependence markov model thus family adgs given set vertices naturally partitioned markovequivalence classes class associated unique statistical model statistical procedures model selection model averaging fail take account equivalence classes may incur substantial computational inefficiencies recent results shown markovequivalence class uniquely determined single chain graph essential graph markovequivalent simultaneously adgs equivalence class here propose two stochastic bayesian model averaging selection algorithms essential graphs apply analysis three discretevariable data sets
given set samples unknown probability distribution study problem constructing good approximative bayesian network model probability distribution question this task viewed search problem goal find maximal probability network model given data in work make attempt learn arbitrarily complex multiconnected bayesian network structures since resulting models unsuitable practical purposes due exponential amount time required reasoning task instead restrict special class simple treestructured bayesian networks called bayesian prototype trees polynomial time algorithm bayesian reasoning exists we show probability given bayesian prototype tree model evaluated given data evaluation criterion used stochastic simulated annealing algorithm searching model space the simulated annealing algorithm provably finds maximal probability model provided sufficient amount time used
we use simple illustrative example expose main ideas evidential probability specifically show use acceptance rule naturally leads use intervals represent probabilities change opinion due experience facilitated probabilities concerning compound experiments events computed given proper knowledge underlying distributions
this paper presents utree reinforcement learning algorithm uses selective attention shortterm memory simultaneously address intertwined problems large perceptual state spaces hidden state by combining advantages work instancebased memorybased learning work robust statistical tests separating noise task structure method learns quickly creates taskrelevant state distinctions handles noise well utree uses treestructured representation related work prediction suffix trees ron et al partigame moore galgorithm chapman kaelbling variable resolution dynamic programming moore it builds utile suffix memory mccallum c used shortterm memory selective perception the algorithm demonstrated solving highway driving task agent weaves around slower faster traffic the agent uses active perception simulated eye movements the environment hidden state time pressure stochasticity world states percepts from environment sensory system agent uses utile distinction test build tree represents depththree memory necessary internal statesfar fewer states would resulted fixedsized historywindow ap proach
feature selection problem choosing subset relevant features in general exhaustive search bring optimal subset with monotonic measure exhaustive search avoided without sacrificing optimality unfortunately error distancebased measures monotonic a new measure employed work monotonic fast compute the search relevant features according measure guaranteed complete exhaustive experiments conducted verification
this paper describes novel method dialogue agent learn choose optimal dialogue strategy while widely agreed dialogue strategies formulated terms communicative intentions little work automatically optimizing agents choices multiple ways realize communicative intention our method based combination learning algorithms empirical evaluation techniques the learning component method based algorithms reinforcement learning dynamic programming qlearning the empirical component uses paradise evaluation framework walker et al identify important performance factors provide performance function needed learning algorithm we illustrate method dialogue agent named elvis email voice interactive system supports access email phone we show elvis learn choose among alternate strategies agent initiative reading messages summarizing email folders
this paper outlines problems may occur reduced error pruning inductive logic programming notably efficiency thereafter new method incremental reduced error pruning proposed attempts address problems experiments show many noisy domains method much efficient alternative algorithms along slight gain accuracy however experiments show well use algorithm recommended domains specific concept description
eeg analysis played key role modeling brains cortical dynamics relatively little effort devoted developing eeg limited means communication if several mental states reliably distinguished recognizing patterns eeg paralyzed person could communicate device like wheelchair composing sequences mental states eeg pattern recognition difficult problem hinges success finding representations eeg signals patterns distinguished in article report study comparing three eeg representations unprocessed signals reduceddimensional representation using karhunenloeve transform frequencybased representation classification performed twolayer neural network implemented cnaps server processor simd architecture adaptive solutions inc execution time comparisons show hundredfold speed sun sparc the best classification accuracy untrained samples using frequencybased representation
this paper surveys field reinforcement learning computerscience perspective it written accessible researchers familiar machine learning both historical basis field broad selection current work summarized reinforcement learning problem faced agent learns behavior trialanderror interactions dynamic environment the work described resemblance work psychology differs considerably details use word reinforcement the paper discusses central issues reinforcement learning including trading exploration exploitation establishing foundations field via markov decision theory learning delayed reinforcement constructing empirical models accelerate learning making use generalization hierarchy coping hidden state it concludes survey implemented systems assessment practical utility current methods reinforcement learning
we add internal memory xcs classifier system we test xcs internal memory named xcsm nonmarkovian environments two four aliasing states experimental results show xcsm easily converge optimal solutions simple environments moreover xcsms performance stable respect size internal memory involved learning however results present evidence complex nonmarkovian environments xcsm may fail evolve optimal solution our results suggest happens exploration strategies currently employed xcs adequate guarantee convergence optimal policy xcsm complex nonmarkovian environments
simple modification standard hill climbing optimization algorithm taking account learning features discussed basic concept approach socalled probability vector single entries determine probabilities appearance entries nbit vectors this vector used random generation nbit vectors form neighborhood specified given probability vector within neighborhood best solutions smallest functional values minimized function recorded the feature learning introduced probability vector updated formal analogue hebbian learning rule wellknown theory artificial neural networks the process repeated probability vector entries close either zero one the resulting probability vector unambiguously determines nbit vector may interpreted optimal solution given optimization task resemblance genetic algorithms discussed effectiveness proposed method illustrated example looking global minima highly multimodal function
this paper describes efficient methods exact approximate implementation minfeatures bias prefers consistent hypotheses definable features possible this bias useful learning domains many irrelevant features present training data we first introduce focus new algorithm exactly implements minfeatures bias this algorithm empirically shown substantially faster focus algorithm previously given almuallim dietterich we introduce mutualinformationgreedy simplegreedy weightedgreedy algorithms apply efficient heuristics approximating minfeatures bias these algorithms employ greedy heuristics trade optimality computational efficiency experimental studies show learning performance id greatly improved algorithms used preprocess training data eliminating irrelevant features ids consideration in particular weightedgreedy algorithm provides excellent efficient approximation min
a statistical approach decision tree modeling described in approach decision tree modeled parametrically process output generated input sequence decisions the resulting model yields likelihood measure goodness fit allowing ml map estimation techniques utilized an efficient algorithm presented estimate parameters tree the model selection problem presented several alternative proposals considered a hidden markov version tree described data sequences temporal dependencies
a binary matrix our task infer given z a given assumptions statistical properties n this problem arises decoding noisy communication z transmitted using errorcorrecting code based parity checks original signal inference sequence linear feedback shift register lfsr noisy observation sequence p zja i assume decoders aim find probable for large n exhaustive search n possible sequences feasible one way attack combinatorial problem create related continuous optimization problem discrete variables replaced real variables here i derive continuous representation terms free energy approximation awkward posterior distribution
an intelligent system capable adapting constantly changing environment it therefore ought capable learning perceptual interactions surroundings this requires certain amount plasticity structure any attempt model perceptual capabilities living system matter construct synthetic system comparable abilities must therefore account plasticity variety developmental learning mechanisms this paper examines results neuroanatomical morphological well behavioral studies development visual perception integrates computational framework suggests several interesting experiments computational models yield insights development visual perception in order understand development information processing structures brain one needs knowledge changes undergoes birth maturity context normal environment however knowledge development aberrant settings also extremely useful reveals extent development function environmental experience opposed genetically determined prewiring accordingly consider development visual system normal restricted rearing conditions the role experience early development sensory systems general visual system particular widely studied variety experiments involving carefully controlled manipulation environment presented animal extensive reviews results found mitchell movshon hirsch boothe singer some examples manipulation visual experience total pattern deprivation eg dark rearing selective deprivation certain class patterns eg vertical lines monocular deprivation animals binocular vision etc extensive studies involving behavioral deficits resulting total visual pattern deprivation indicate deficits arise primarily result impairment visual information processing brain the results experiments suggest specific developmental learning mechanisms may operating various stages development different levels system we discuss hhhhhhhhhhhhhhh this working draft all comments especially constructive criticism suggestions improvement appreciated i indebted prof james dannemiller introducing literature infant development prof leonard uhr helpful comments initial draft paper numerous researchers whose experimental work provided basis model outlined paper this research partially supported grants national science foundation university wisconsin graduate school
this paper studies problem ergodicity transition probability matrices markovian models hidden markov models hmms makes difficult task learning represent longterm context sequential data this phenomenon hurts forward propagation longterm context information well learning hidden state representation represent longterm context depends propagating credit information backwards time using results markov chain theory show problem diffusion context credit reduced transition probabilities approach ie transition probability matrices sparse model essentially deterministic the results found paper apply learning approaches based continuous optimization gradient descent baumwelch algorithm
modern industry today needs flexible adaptive faulttolerant methods information processing several applications shown neural networks fulfill requirements in paper application areas neural networks successfully used presented then kind check list described mentioned different steps applying neural networks the paper finished discussion neural networks projects done research group interactive planning research center computer science fzi
gas oil pipelines need inspected corrosion defects regular intervals for application pipetronix gmbh ptx karlsruhe developed special ultrasonic based probe based recorded wall thicknesses called pipe pig research center computer science fzi developed cooperation ptx automatic inspection system called neuropipe neuropipe task detect defects like metal loss the kernel inspection tool neural classifier trained using manually collected defect examples the following paper focus aspects successfull use learning methods industrial application
we construct mixture locally linear generative models collection pixelbased images digits use recognition different models given digit used capture different styles writing new images classified evaluating loglikelihoods model we use embased algorithm mstep computationally straightforward principal components analysis pca incorporating tangentplane information expected local deformations requires adding tangent vectors sample covariance matrices pca demonstrably improves performance
in international journal neural systems p url paper ftpftpcscoloradoedupubtimeseriesmypapersexpertspsz httpwwwcscoloradoeduandreastimeseriesmypapersexpertspsz university colorado computer science technical report cucs in analysis prediction realworld systems two key problems nonstationarityoften form switching regimes overfitting particularly serious noisy processes this article addresses problems using gated experts consisting nonlinear gating network several also nonlinear competing experts each expert learns predict conditional mean expert adapts width match noise level regime the gating network learns predict probability expert given input this article focuses case gating network bases decision information inputs this contrasted hidden markov models decision based previous states ie output gating network previous time step well averaging several predictors in contrast gated experts softpartition input space this article discusses underlying statistical assumptions derives weight update rules compares performance gated experts standard methods three time series computergenerated series obtained randomly switching two nonlinear processes time series santa fe time series competition light intensity laser chaotic state daily electricity demand france realworld multivariate problem structure several time scales the main results gating network correctly discovers different regimes process widths associated expert important segmentation task used characterize subprocesses less overfitting compared single networks homogeneous multilayer perceptrons since experts learn match variances local noise levels this viewed matching local complexity model local complexity data
this paper describes approach modelling drug activity using machine learning tools some experiments modelling quantitative structureactivity relationship qsar using standard hansch method machine learning system golem already reported literature the paper describes results applying two machine learning systems magnus assistant retis data the results achieved machine learning systems better results hansch method therefore machine learning tools considered promising solving kind problems the given results also illustrate variations performance different machine learning systems applied drug design problem
in paper describe one aspect research project called hiped addressed problem performing design engineering devices accessing heterogeneous databases the front end hiped system consisted interactive kritik multimodal reasoning system combined case based model based reasoning solve design problem this paper focuses backend processing five types queries received front end evaluated mapping appropriately using facts schemas underlying databases rules establish correspondance among data databases terms relationships equivalence overlap set containment the uniqueness approach stems fact mapping process forgiving query received front end evaluated respect large number possibilities these possibilities encoded form rules consider various ways tokens given query may match relation names attrribute names values underlying tables the approach implemented using coral deductive database system rule processing engine
temporal difference methods solve temporal credit assignment problem reinforcement learning an important subproblem general reinforcement learning learning achieve dynamic goals although existing temporal difference methods q learning applied problem take advantage special structure this paper presents dglearning algorithm learns efficiently achieve dynamically changing goals exhibits good knowledge transfer goals in addition paper shows traditional relaxation techniques applied problem finally experimental results given demonstrate superiority dg learning q learning moderately large synthetic nondeterministic domain
in paper prove intractability learning several classes boolean functions distributionfree model also called probably approximately correct pac model learning examples these results representation independent hold regardless syntactic form learner chooses represent hypotheses our methods reduce problems cracking number wellknown publickey cryptosys tems learning problems we prove polynomialtime learning algorithm boolean formulae deterministic finite automata constantdepth threshold circuits would dramatic consequences cryptography number theory particular algorithm could used break rsa cryptosystem factor blum integers composite numbers equivalent modulo detect quadratic residues the results hold even learning algorithm required obtain slight advantage prediction random guessing the techniques used demonstrate interesting duality learning cryptography we also apply results obtain strong intractability results approximating gener alization graph coloring fl this research conducted author harvard university supported at t bell laboratories scholarship supported grants onrnk nsfdcr nsfccr daalk darpa afosr serc
we present new method determining consensus sequence dna fragment assemblies the new method traceevidence directly incorporates aligned abi trace information consensus calculations via previously described representation tracedata classifications the new method extracts sums evidence indicated representation determine consensus calls using traceevidence method results automatically produced consensus sequences accurate less ambiguous produced standard majority voting methods additionally improvements achieved less coverage required standard methods using traceevidence coverage three error rates low coverage ten sequences
to learned morphology natural language capacity recognize produce words consisting novel combinations familiar morphemes most recent work acquisition morphology takes perspective production receptive morphology comes first child this paper presents connectionist model acquisition capacity recognize morphologically complex words the model takes sequences phonetic segments inputs maps onto output units representing meanings lexical grammatical morphemes it consists simple recurrent network separate hiddenlayer modules tasks recognizing root grammatical morphemes input word experiments artificial language stimuli demonstrate model generalizes novel words morphological rules one major types found natural languages version network unassigned hiddenlayer modules learn assign output recognition tasks efficient manner i also argue rules involving reduplication copying portions root network requires separate recurrent subnetworks sequences larger units syllables the network learn develop syllable representations support recognition reduplication also provide basis learning produce well recognize morphologically complex words the model makes many detailed predictions learning difficulty particular morphological rules
this paper presents algorithm combines traditional ebl techniques recent developments inductive logic programming learn effective clause selection rules prolog programs when control rules incorporated original program significant speedup may achieved the algorithm shown improvement competing ebl approaches several domains additionally algorithm capable automatically transforming intractable algorithms ones run polynomial time
neurons ventral stream primate visual system exhibit responses images objects invariant respect natural transformations translation size view anatomical neurophysiological evidence suggests achieved series hierarchical processing areas in attempt elucidate manner representations established constructed model cortical visual processing seeks parallel many features system specifically multistage hierarchy topologically constrained convergent connectivity each stage constructed competitive network utilising modified hebblike learning rule called trace rule incorporates previous well current neuronal activity the trace rule enables neurons learn whatever invariant short time periods eg representation objects objects transform real world the trace rule enables neurons learn statistical invariances objects transformations associating together representations occur close together time we show using trace rule training algorithm model indeed learn produce transformation invariant responses natural stimuli faces
in paper propose recurrent neural networks feedback input units handling two types data analysis problems on one hand scheme used static data input variables missing on hand also used sequential data input variables missing available different frequencies unlike case probabilistic models eg gaussian missing variables network attempt model distribution missing variables given observed variables instead discriminant approach fills missing variables sole purpose minimizing learning criterion eg minimize output error
many neural networks derived optimization dynamics suitable objective functions we show networks designed repeated transformations one objective another fixpoints we exhibit collection algebraic transformations reduce network cost increase set objective functions neurally implementable the transformations include simplification products expressions functions one two expressions sparse matrix products may interpreted legendre transformations also minimum maximum set expressions these transformations introduce new interneurons force network seek saddle point rather minimum other transformations allow control network dynamics reconciling lagrangian formalism need fixpoints we apply transformations simplify number structured neural networks beginning standard reduction winnertakeall network on connections on also susceptible inexact graphmatching random dot matching convolutions coordinate transformations sorting simulations show fixpointpreserving transformations may applied repeatedly elaborately example networks still robustly converge
the use casebased reasoning process model design involves subtasks recalling previously known designs memory adapting design cases subcases fit current design context the development process model particular design domain proceeds parallel development representation cases case memory organisation design knowledge needed addition specific designs the selection particular representational paradigm types information details use particular problemsolving domain depend intended use information represented project information available well nature domain in paper describe development implementation four casebased design systems casecad cadsyn win demex each system described terms content organisation source case memory implementation case recall case adaptation a comparison systems considers relative advantages disadvantages implementations
we describe biologically plausible model dynamic recognition learning visual cortex based statistical theory kalman filtering optimal control theory the model utilizes hierarchical network whose successive levels implement kalman filters operating successively larger spatial temporal scales each hierarchical level network predicts current visual recognition state lower level adapts recognition state using residual error prediction actual lowerlevel state simultaneously network also learns internal model spatiotemporal dynamics input stream adapting synaptic weights hierarchical level order minimize prediction errors the kalman filter model respects key neuroanatomical data reciprocity connections visual cortical areas assigns specific computational roles interlaminar connections known exist neurons visual cortex previous work elucidated usefulness model explaining neurophysiological phenomena endstopping related extraclassical receptive field effects in paper addition providing detailed exposition model present variety experimental results demonstrating ability model perform robust spatiotemporal segmentation recognition objects image sequences presence varying amounts occlusion background clutter noise
individual lifetime learning guide evolving population areas high fitness genotype space evolutionary phenomenon known baldwin effect baldwin hinton nowlan it accepted wisdom guiding speeds rate evolution by highlighting another interaction learning evolution termed hiding effect argued depends measure evolutionary speed one adopts the hiding effect shows learning reduce selection pressure individuals hiding genetic differences there thus tradeoff baldwin effect hiding effect determine learnings influence evolution two factors contribute tradeoff cost learning landscape epis tasis investigated experimentally
this paper focuses optimization hyperparameters function approximators we describe kind racing algorithm continuous optimization problems spends less time evaluating poor parameter settings time honing estimates promising regions parameter space the algorithm able automatically optimize parameters function approximator less computation time we demonstrate algorithm problem finding good parameters memory based learner show tradeoffs involved choosing right amount computation spend evaluation
based analysis experiments using realworld datasets find greediness forward feature selection algorithms severely corrupt accuracy function approximation using selected input features improves efficiency significantly hence propose three greedier algorithms order enhance efficiency feature selection processing we provide empirical results linear regression locally weighted regression knearestneighbor models we also propose use algorithms develop offline chinese japanese handwriting recognition system auto matically configured local models
this paper considers aspect mixture modelling significantly overlapping distributions require data parameters accurately estimated well separated distributions for example two gaussian distributions considered significantly overlap means within three standard deviations if insufficient data available single component distribution estimated although data originates two component distributions we consider much data required distinguish two component distributions one distribution mixture modelling using minimum message length mml criterion first perform experiments show mml criterion performs well relative bayesian criteria second make two improvements existing mml estimates improve performance overlapping distributions
in paper present performance prediction model indicating performance range mimd parallel processor systems neural network simulations the model expresses total execution time simulation function execution times small number kernel functions measured one processor one physical communication link the functions depend type neural network geometry decomposition connection structure mimd machine using model execution time speedup scalability efficiency large mimd systems predicted the model validated quantitatively applying two popular neural networks backpropagation kohonen selforganizing feature map decomposed gcel transputer system measurements taken network simulations decomposed via dataset network decomposition techniques agreement model measurements within estimates given performances expected new t transputer systems the presented method also used application areas image processing
with goal reducing computational costs without sacrificing accuracy describe two algorithms find sets prototypes nearest neighbor classification here term prototypes refers reference instances used nearest neighbor computation instances respect similarity assessed order assign class new data item both algorithms rely stochastic techniques search space sets prototypes simple implement the first monte carlo sampling algorithm second applies random mutation hill climbing on four datasets show three four prototypes sufficed give predictive accuracy equal superior basic nearest neighbor algorithm whose runtime storage costs approximately times greater we briefly investigate random mutation hill climbing may applied select features prototypes simultaneously finally explain performance sampling algorithm datasets terms statistical measure extent clustering displayed target classes
we present new selforganizing neural network model two variants the first variant performs unsupervised learning used data visualization clustering vector quantization the main advantage existing approaches eg kohonen feature map ability model automatically find suitable network structure size this achieved controlled growth process also includes occasional removal units the second variant model supervised learning method results combination abovementioned selforganizing network radial basis function rbf approach in model possible contrast earlier approaches toperform positioning rbf units supervised training weights parallel therefore current classification error used determine insert new rbf units this leads small networks generalize well results twospirals benchmark vowel classification problem presented better results previously published fl submitted publication
i present first results columbus autonomous mobile robot columbus operates initially unknown structured environments its task explore model environment efficiently avoiding collisions obstacles columbus uses instancebased learning technique modeling environment realworld experiences generalized via two artificial neural networks encode characteristics robots sensors well characteristics typical environments robot assumed face once trained networks allow knowledge transfer across different environments robot face lifetime columbus models represent expected reward confidence expectations exploration achieved navigating low confidence regions an efficient dynamic programming method employed background find minimalcost paths executed robot maximize exploration columbus operates realtime it operating successfully office building environment periods hours
we analyze performance genetic algorithm ga call culling variety algorithms problem refer additive search problem asp asp closely related several previously well studied problems game mastermind additive fitness functions we show problem learning ising perceptron reducible noisy version asp culling efficient asp highly noise tolerant best known approach regimes noisy asp first problem aware genetic type algorithm bests known competitors standard gas contrast perform much poorly asp hillclimbing approaches even though schema theorem holds asp we generalize asp kasp study whether gas achieve implicit parallelism problem many schemata gas fail achieve implicit parallelism describe algorithm call explicitly parallel search succeeds we also compute optimal culling point selective breeding turns independent fitness function population distribution we also analyze mean field theoretic algorithm performing similarly culling many problems these results provide insight gas beat competing methods
many extensions proposed help instancebased learning algorithms perform better wide variety realworld applications however trivial decide parameters options use applying instancebased learning algorithm particular problem traditionally crossvalidation used choose parameters k k nearest neighbor classifier this paper points cross validation often provide enough information allow finetuning classifier confidence levels used break ties common crossvalidation used it proposes fuzzy instance based learning fibl algorithm uses distanceweighted voting parameters set via combination crossvalidation confidence levels in experiments datasets fibl higher average generalization accuracy using majority voting using crossvalidation alone determine parameters
this paper describes formulation reinforcement learning enables learning noisy dynamic environemnts complex concurrent multirobot learning domain the methodology involves minimizing learning space use behaviors conditions dealing credit assignment problem shaped reinforcement form heterogeneous reinforcement functions progress estimators we experimentally validate ap proach group four mobile robots learning foraging task
most existing decision tree systems use greedy approach induce trees locally optimal splits induced every node tree although greedy approach suboptimal believed produce reasonably good trees in current work attempt verify belief we quantify goodness greedy tree induction empirically using popular decision tree algorithms c cart we induce decision trees thousands synthetic data sets compare corresponding optimal trees turn found using novel map coloring idea we measure effect greedy induction variables underlying concept complexity training set size noise dimensionality our experiments show among things expected classification cost greedily induced tree consistently close optimal tree
report sycon abstract previous results input state stabilizability shown hold even systems linear controls provided general type feedback allowed applications certain stabilization problems coprime factorizations well comparisons results input state stability also briefly discussed
attractor networks map continuous input space discrete output space useful pattern completion cleaning noisy missing features input however designing net given set attractors notoriously tricky training procedures cpu intensive often produce spurious attractors illconditioned attractor basins these difficulties occur connection network participates encoding multiple attractors we describe alternative formulation attractor networks encoding knowledge local distributed although localist attractor nets similar dynamics distributed counterparts much easier work interpret we propose statistical formulation localist attractor net dynamics yields convergence proof mathematical interpretation model parameters we present simulation experiments explore behavior localist attractor nets showing produce gang effectthe presence attractor enhances attractor basins neighboring attractorsand spurious attractors occur points symmetry state space
according wolperts nofreelunch nfl theorems generalisation absence domain knowledge necessarily zerosum enterprise good generalisation performance one situation always offset bad performance another wolpert notes theorems demonstrate effective generalisation logical impossibility merely learners bias assumption set key importance
learning limited modification parameters limited scope capability modify system structure also needed get wider range learnable in case artificial neural networks learning iterative adjustment synaptic weights succeed network designer predefines appropriate network structure ie number hidden layers units size shape receptive projective fields this paper advocates view network structure usually done determined trialanderror computed learning algorithm incremental learning algorithms modify network structure addition andor removal units andor links a survey current connectionist literature given line thought grow learn gal new algorithm learns association oneshot due incremental using local representation during socalled sleep phase units previously stored longer necessary due recent modifications removed minimize network complexity the incrementally constructed network later finetuned offline improve performance another method proposed greatly increases recognition accuracy train number networks vote responses the algorithm variants tested recognition handwritten numerals seem promising especially terms learning speed this makes algorithm attractive online learning tasks eg robotics the biological plausibility incremental learning also discussed briefly earlier part work realized laboratoire de microinformatique ecole polytechnique federale de lausanne supported fonds national suisse de la recherche scientifique later part realized supported international computer science institute a number people helped guiding stimulating discussions questions subutai ahmad peter clarke jerry feldman christian jutten pierre marchal jean daniel nicoud steve omohondro leon personnaz
this paper introduces probability model mixture trees account sparse dynamically changing dependence relationships we present family efficient algorithms use em minimum spanning tree algorithm find ml map mixture trees variety priors including dirichlet mdl priors
two fundamental problems analyzing dna sequences locating regions dna sequence encode proteins determining reading frame region we investigate using artificial neural networks anns find coding regions determine reading frames detect frameshift errors e coli dna sequences we describe adaptation approach used uberbacher mural identify coding regions human dna compare performance anns several conventional methods predicting reading frames our experiments demonstrate anns outperform conventional approaches
the paper describes selflearning control system mobile robot based sensor information control system provide steering signal way collisions avoided since case examples available system learns basis external reinforcement signal negative case collision zero otherwise rules temporal difference learning used find correct mapping discrete sensor input space steering signal we describe algorithm learning correct mapping input state vector output steering signal algorithm used discrete coding input state space
work currently underway devise learning methods better able transfer knowledge one task another the process knowledge transfer usually viewed logically separate inductive procedures ordinary learning however paper argues seperatist view leads number conceptual difficulties it offers task analysis situates transfer process inside generalised inductive protocol it argues transfer viewed subprocess within induction independent procedure transporting knowledge learning trials
this chapter describes three studies address question neural network learning improved via incorporation information extracted networks this general problem call network transfer encompasses many types relationships source target networks our focus utilization weights source networks solve subproblem target network task goal speeding learning target task we demonstrate approach described improve learning speed ten times learning starting random weights
alvinn autonomous land vehicle neural net backpropagation trained neural network capable autonomously steering vehicle road highway environments although alvinn fairly robust one problems time takes train as vehicle capable online learning driver drive car minutes network capable autonomous operation one reason use backprop in report describe original alvinn system look three alternative training methods quickprop cascade correlation cascade we run series trials using quickprop cascade correlation cascade compare backprop baseline finally hidden unit analysis performed determine network learning applying advanced learning algorithms alvinn
the work discussed paper motivated need building decision support systems realworld problem domains our goal use systems tool supporting bayes optimal decision making action maximizing expected utility respect predicted probabilities possible outcomes selected for reason models used need probabilistic nature output model probability distribution set numbers for model family chosen set simple discrete finite mixture models advantage computationally efficient in work describe bayesian approach constructing finite mixture models sample data our approach based twophase unsupervised learning process used exploratory analysis model construction in first phase selection model class ie number parameters performed calculating cheesemanstutz approximation model class evidence in second phase map parameters selected class estimated em algorithm in framework overfitting problem common many traditional learning approaches avoided learning process automatically regulates complexity model this paper focuses model class selection phase approach validated presenting empirical results natural synthetic data
we introduce analyze new algorithm linear classification combines rosenblatts perceptron algorithm helmbold warmuths leaveoneout method like vapniks maximalmargin classifier algorithm takes advantage data linearly separable large margins compared vapniks algorithm however much simpler implement much efficient terms computation time we also show algorithm efficiently used high dimensional spaces using kernel functions we performed experiments using algorithm variants classifying images handwritten digits the performance algorithm close good performance maximalmargin classifiers problem
a version paper appear acm transactions computer systems august permission make digital copies part work personal classroom use grantedwithout fee provided copies made distributed profit commercial advantage copies bear notice full citation first page copyrights components work owned others acm must honored abstracting credit permitted to copy otherwise republish post servers redistribute lists requires prior specific permission andor fee abstract to achieve high performance contemporary computer systems rely two forms parallelism instructionlevel parallelism ilp threadlevel parallelism tlp wideissue superscalar processors exploit ilp executing multiple instructions single program single cycle multiprocessors mp exploit tlp executing different threads parallel different processors unfortunately parallelprocessing styles statically partition processor resources thus preventing adapting dynamicallychanging levels ilp tlp program with insufficient tlp processors mp idle insufficient ilp multipleissue hardware superscalar wasted this paper explores parallel processing alternative architecture simultaneous multithreading smt allows multiple threads compete share processors resources every cycle the compelling reason running parallel applications smt processor ability use threadlevel parallelism instructionlevel parallelism interchangeably by permitting multiple threads share processors functional units simultaneously processor use ilp tlp accommodate variations parallelism when program single thread smt processors resources dedicated thread tlp exists parallelism compensate lack
in paper present framework building probabilistic automata parameterized contextdependent probabilities gibbs distributions used model state transitions output generation parameter estimation carried using em algorithm mstep uses generalized iterative scaling procedure we discuss relations certain classes stochastic feedforward neural networks geometric interpretation parameter estimation simple example statistical language model constructed using methodology
in regression context boosting bagging techniques build committee regressors may superior single regressor we use regression trees fundamental building blocks bagging committee machines boosting committee machines performance analyzed three nonlinear functions boston housing database in cases boosting least equivalent cases better bagging terms prediction error
current expert systems properly handle imprecise incomplete information on hand neural networks perform pattern recognition operations even noisy environments against background implemented neural expert system shell neula whose computational mechanism processes imprecisely incompletely given information means approximate probabilistic reasoning
coevolution give rise red queen effect interacting populations alter others fitness landscapes the red queen effect significantly complicates measurement coevolutionary progress introducing fitness ambiguities improvements performance coevolved individuals appear decline stasis usual measures evolutionary progress unfortunately appropriate measures fitness given red queen effect developed artificial life theoretical biology population dynamics evolutionary genetics we propose set appropriate performance measures based genetic behavioral data illustrate use simulation coevolution genetically specified continuoustime noisy recurrent neural networks generate pursuit evasion behaviors autonomous agents
inferences measurement error models sensitive modeling assumptions specifically model incorrect estimates inconsistent to reduce sensitivity modeling assumptions yet still retain efficiency parametric inference propose use flexible parametric models accommodate departures standard parametric models we use mixtures normals purpose we study two cases detail linear errorsinvariables model changepoint berkson model fl raymond j carroll professor statistics nutrition toxicology department statistics texas am university college station tx kathryn roeder associate professor larry wasserman professor department statistics carnegiemellon university pittsburgh pa carrolls research supported grant national cancer institute ca roeders research supported nsf grant dms wassermans research supported nih grant roca nsf grants dms dms
in paper investigate phenomenon multiparent reproduction ie study recombination mechanisms arbitrary n gt number parents participate creating children in particular discuss scanning crossover generalizes standard uniform crossover diagonal crossover generalizes point crossover study effects different number parents ga behavior we conduct experiments tough function optimization problems observe multiparent operators performance gas enhanced significantly we also give theoretical foundation showing operators work distributions
technical report no department statistics gn university washington seattle washington usa susan l rosenkranz pew health policy postdoctoral fellow institute health policy studies box university california san francisco san francisco ca adrian e raftery professor statistics sociology department statistics gn university washington seattle wa rosenkranzs research supported national research service award tca national cancer institute the authors grateful paula diehr kevin cain helpful discussions
draft a brief introduction neural networks richard d de veaux lyle h ungar williams college university pennsylvania abstract artificial neural networks used increasing frequency high dimensional problems regression classification this article provides tutorial overview neural networks focusing back propagation networks method approximating nonlinear multivariable functions we explain statisticians vantage point neural networks might attractive compare modern regression techniques keywords nonparametric regression function approximation backpropagation introduction networks mimic way brain works computer programs actually learn patterns forecasting without know statistics these many claims attractions artificial neural networks neural networks henceforth drop term artificial unless need distinguish biological neural networks seem everywhere days least advertising able statistics without fuss bother anything except buy piece software neural networks successfully used many different applications including robotics chemical process control speech recognition optical character recognition credit card fraud detection interpretation chemical spectra vision autonomous navigation vehicles pointers literature given end article in article attempt explain one particular type neural network feedforward networks sigmoidal activation functions backpropagation networks actually works trained compares well known statistical techniques as example someone would want use neural network consider problem recognizing hand written zip codes letters this classification problem
to apply algorithm classification assign class separate set codebook gaussians each set trained patterns single class after trained codebook gaussians set provides estimate probability function one class parzen window estimation take estimate pattern distribution average gaussians set classification pattern may done calculating probability class respective sample point assigning pattern class highest probability hence whole codebook plays role classification patterns this case regular classification schemes using codebooks we tested classification scheme several classification tasks including two spiral problem we compared algorithm various classification algorithms came second best algorithm applications parzen window estimation however computing time memory parzen window estimation excessive compared algorithm hence practical situations algorithm preferred we developed fast algorithm combines attractive properties parzen window estimation vector quantization the scale parameter tuned adaptively therefore set ad hoc manner it allows classification strategy codebook vectors taken account this yields better results standard vector quantization techniques an interesting topic research use radially nonsymmetric gaussians
predictions lifetimes dynamically allocated objects used improve time space efficiency dynamic memory management computer programs barrett zorn used simple lifetime predictor demonstrated improvement variety computer programs in paper use decision trees lifetime prediction programs show significantly better prediction our method also advantage training use large number features let decision tree automatically choose relevant subset
evolutionary systems used variety applications turbine design scheduling problems the basic algorithms similar applications representation always problem specific unfortunately search time evolutionary systems much depends efficient codings using problem specific domain knowledge reduce size search space this paper describes approach user specifies general basic coding used larger variety problems the system learns efficient problem specific coding to evolutionary system variable length coding used while system optimizes example problem meta process identifies successful combinations genes population combines higher level evolved genes the extraction repeated iteratively allowing genes evolve high level complexity encode high number original basic genes this results continuous restructuring search space allowing potentially successful solutions found much shorter search time the evolved coding used solve related problems while excluding potentially desirable solutions evolved coding makes knowledge example problem available new problem
the coverage learning algorithm number concepts learned algorithm samples given size this paper asks whether good learning algorithms designed maximizing coverage the paper extends previous upper bound coverage boolean concept learning algorithm describes two algorithmsmultiballs largeballwhose coverage approaches upper bound experimental measurement coverage id fringe algorithms shows coverage far bound further analysis largeball shows although learns many concepts seem interesting concepts hence coverage maximization alone appear yield practicallyuseful learning algorithms the paper concludes definition coverage within bias suggests way coverage maximization could applied strengthen weak preference biases
markov decision processes mdps recently applied problem modeling decisiontheoretic planning while traditional methods solving mdps often practical small states spaces effectiveness large ai planning problems questionable we present algorithm called structured policy iteration spi constructs optimal policies without explicit enumeration state space the algorithm retains fundamental computational steps commonly used modified policy iteration algorithm exploits variable propositional independencies reflected temporal bayesian network representation mdps the principles behind spi applied structured representation stochastic actions policies value functions algorithm used conjunction cent approximation methods
this paper reviews features new class multilayer connectionist architectures known asocs adaptive selforganizing concurrent systems asocs similar decisionmaking neural network models attempts learn adaptive set arbitrary vector mappings however differs dramatically mechanisms asocs based networks adaptive digital elements selfmodify using local information function specification entered incrementally use rules rather complete inputoutput vectors processing network able extract critical features large environment give output parallel fashion learning also uses parallelism selforganization new rule completely learned time linear depth network the model guarantees learning arbitrary mapping boolean inputoutput vectors the model also stable learning erase previously learned mappings except explicitly contradicted
maximum working likelihood mwl inference presence missing data quite challenging intractability associated marginal likelihood this problem exacerbated number parameters involved large we propose using markov chain monte carlo mcmc first obtain mwl estimator working fisher information matrix second using monte carlo quadrature obtain remaining components correct asymptotic mwl variance evaluation marginal likelihood needed we demonstrate consistency asymptotic normality number independent identically distributed data clusters large likelihood may incorrectly specified an analysis longitudinal ordinal data given example key words convergence posterior distributions maximum likelihood metropolis
natural images contain characteristic statistical regularities set apart purely random images understanding regularities enable natural images coded efficiently in paper describe forms structure contained natural images show related response properties neurons early stages visual system many important forms structure require higherorder ie linear pairwise statistics characterize makes models based linear hebbian learning principal components analysis inappropriate finding efficient codes natural images we suggest good objective efficient coding natural scenes maximize sparseness representation show network learns sparse codes natural scenes succeeds developing localized oriented bandpass receptive fields similar primate striate cortex
in paper develop empirical methodology studying behavior evolutionary algorithms based problem generators we describe three generators used study effects epistasis performance eas finally illustrate use ideas preliminary exploration effects epistasis simple gas
traditionally genetic algorithms relied upon point crossover operators many recent empirical studies however shown benefits higher numbers crossover points some intriguing recent work focused uniform crossover involves average l crossover points strings length l theoretical results suggest view hyperplane sampling disruption uniform crossover redeeming features however growing body experimental evidence suggests otherwise in paper attempt reconcile opposing views uniform crossover present framework understanding virtues
conditional logics introduced lewis stalnaker utilized artificial intelligence capture broad range phenomena in paper examine complexity several variants discussed literature we show general deciding satisfiability pspacecomplete formulas arbitrary conditional nesting npcomplete formulas bounded nesting conditionals however provide several exceptions rule of particular note results showing assuming uniformity ie worlds agree worlds possible decision problem becomes exptimecomplete even formulas bounded nesting b assuming absoluteness ie worlds agree conditional statements decision problem npcomplete mulas arbitrary nesting
an incremental higherorder nonrecurrent network combines two properties found useful learning sequential tasks higherorder connections incremental introduction new units the network adds higher orders needed adding new units dynamically modify connection weights since new units modify weights next timestep information previous step temporal tasks learned without use feedback thereby greatly simplifying training furthermore theoretically unlimited number units added reach arbitrarily distant past experiments reber grammar demonstrated speedups two orders magnitude recurrent networks
i propose novel general principle unsupervised learning distributed nonredundant internal representations input patterns the principle based two opposing forces for representational unit adaptive predictor tries predict unit remaining units in turn unit tries react environment minimizes predictability this encourages unit filter abstract concepts environmental input concepts statistically independent upon units focus i discuss various simple yet potentially powerful implementations principle aim finding binary factorial codes barlow et al ie codes probability occurrence particular input simply product probabilities corresponding code symbols such codes potentially relevant segmentation tasks speeding supervised learning novelty detection methods finding factorial codes automatically implement occams razor finding codes using minimal number units unlike previous methods novel principle potential removing linear also nonlinear output redundancy illustrative experiments show algorithms based principle predictability minimization practically feasible the final part paper describes entirely local algorithm potential learning unique representations extended input sequences
in paper study learning pac model valiant example oracle used learning may faulty one two ways either misclassifying example distorting distribution examples we first consider models examples misclassified kearns recently showed efficient learning new model using statistical queries sufficient condition pac learning classification noise we show efficient learning statistical queries sufficient learning pac model malicious error rate proportional required statistical query accuracy one application result new lower bound tolerable malicious error learning monomials k literals this first bound independent number irrelevant attributes n we also use statistical query model give sufficient conditions using distribution specific algorithms distributions outside prescribed domains a corollary result expands class distributions weakly learn monotone boolean formulae we also consider new models learning examples chosen according distribution learner tested we examine three variations distribution noise give necessary sufficient conditions polynomial time learning noise we show containments separations various models faulty oracles finally examine hypothesis boosting algorithms context learning distribution noise show schapires result regarding strength weak learnability sense tight requiring weak learner nearly distribution free
this paper describes first stage study evolution learning abilities we use simple maze exploration problem designed r sutton task individual encode inherent learning parameters genome the learning architecture use one step qlearning using lookup table inherent parameters initial qvalues learning rate discount rate rewards exploration rate under fitness measure proportioning number times achieves goal later half life learners evolve genetic algorithm the results computer simulation indicated learning ability emerge environment changes every generation inherent map optimal path acquired environment doesnt change these results suggest emergence learning ability needs environmental change faster alternate generation
we examine problem performing exact dynamicprogramming updates partially observable markov decision processes pomdps computational complexity viewpoint dynamicprogramming updates crucial operation wide range pomdp solution methods find intractable perform updates piecewiselinear convex value functions general pomdps we offer new algorithm called witness algorithm compute updated value functions efficiently restricted class pomdps number linear facets great we compare witness algorithm existing algorithms analytically empirically find fastest algorithm wide range pomdp sizes
this paper examines limits instruction level parallelism found programs particular spec benchmark suite apart using recent version spec benchmark suite differs earlier studies removing nonessential true dependencies occur result compiler employing stack subroutine linkage this subtle limitation parallelism readily evident appears true dependency stack pointer other methods used employ stack remove dependency in paper show removal exposes far parallelism seen previously we refer type parallelism parallelism distance requires impossibly large instruction windows detection we conclude two observations single instruction window characteristic superscalar machines inadequate detecting parallelism distance order take advantage parallelism compiler must involved separate threads must explicitly programmed
in paper present framework building probabilistic automata parameterized contextdependent probabilities gibbs distributions used model state transitions output generation parameter estimation carried using em algorithm mstep uses generalized iterative scaling procedure we discuss relations certain classes stochastic feedforward neural networks geometric interpretation parameter estimation simple example statistical language model constructed using methodology
models unsupervised correlationbased hebbian synaptic plasticity typically unstable either synapses grow reaches maximum allowed strength synapses decay zero strength a common method avoiding outcomes use constraint conserves limits total synaptic strength cell we study dynamical effects constraints two methods enforcing constraint distinguished multiplicative subtractive for otherwise linear learning rules multiplicative enforcement constraint results dynamics converge principal eigenvector operator determining unconstrained synaptic development subtractive enforcement contrast typically leads final state almost synaptic strengths reach either maximum minimum allowed value this final state often dominated weight configurations principal eigenvector unconstrained operator multiplicative enforcement yields graded receptive field mutually correlated inputs represented whereas subtractive enforcement yields receptive field sharpened subset maximallycorrelated inputs if two equivalent input populations eg two eyes innervate common target multiplicative enforcement prevents segregation ocular dominance segregation two populations weakly correlated whereas subtractive enforcement allows segregation circumstances these results may used understand constraints output cells input cells a variety rules implement constrained dynamics discussed
this project supported part grant mcdonnellpew foundation grant atr human information processing research laboratories grant siemens corporation grant nj office naval research the project also supported nsf grant asc support center biological computational learning mit including funds provided darpa hpcc program michael i jordan nsf presidential young investigator
learning made efficient actively select particularly salient data points within bayesian learning framework objective functions discussed measure expected informativeness candidate measurements three alternative specifications want gain information lead three different criteria data selection all criteria depend assumption hypothesis space correct may prove main weakness
knowledge clusters relations important understanding highdimensional input data unknown distribution ordinary feature maps fully connected fixed grid topology properly reflect structure clusters input spacethere cluster boundaries map incremental feature map algorithms nodes connections added deleted map according input distribution overcome problem however far algorithms limited maps drawn d case dimensional input space in approach proposed paper nodes added incrementally regular dimensional grid drawable times irrespective dimensionality input space the process results map explicitly represents cluster structure highdimensional input
lattice conditional independence lci models multivariate normal data recently introduced analysis nonmonotone missing data patterns nonnested dependent linear regression models seemingly unrelated regressions it shown class lci models coincides subclass class graphical markov models determined acyclic digraphs adgs namely subclass transitive adg models an explicit graph theoretic characterization adgs markov equivalent transitive adg obtained this characterization allows one determine whether specific adg d markov equivalent transitive adg hence lci model polynomial time without exhaustive search exponentially large equivalence class d these results require existence positivity joint densities
in paper describe method improving geneticalgorithmbased optimization using casebased learning the idea utilize sequence points explored search guide exploration the proposed method particularly suitable continuous spaces expensive evaluation functions arise engineering design empirical results two engineering design domains across different representations demonstrate proposed method significantly improve efficiency reliability ga optimizer moreover results suggest modification makes genetic algorithm less sensitive poor choices tuning parameters muta tion rate
genetic algorithms gas extensively used means performing global optimization simple yet reliable manner however realistic engineering design optimization domains simple classical implementation ga based binary encoding bit mutation crossover often inefficient unable reach global optimum in paper describe ga continuous designspace optimization uses new ga operators strategies tailored structure properties engineering design domains empirical results domains supersonic transport aircraft supersonic missile inlets demonstrate newly formulated ga significantly better classical ga efficiency reliability
d e rumelhart g e hinton r j williams learning internal representations error propagation d e rumelhart j l mcclelland eds parallel distributed processing explorations microstructure cognition vol mit press
evolutionary trees frequently used underlying model design algorithms optimization criteria software packages multiple sequence alignment msa in paper reexamine suitability trees universal model msa light broad range biological questions msas used address a tree model consists tree topology model accepted mutations along branches after surveying major applications msa examples molecular biology literature used illustrate situations tree model fails this occurs relationship residues column described tree example structural functional applications msa it also occurs situations lateral gene transfer entire gene modeled unique tree in cases nonparsimonous data convergent evolution may difficult find consistent mutational model we hope survey promote dialogue biologists computer scientists leading biologically realistic research msa
selective suppression transmission feedback synapses learning proposed mechanism combining associative feedback selforganization feedforward synapses experimental data demonstrates cholinergic suppression synaptic transmission layer i feedback synapses lack suppression layer iv feedforward synapses a network feature uses local rules learn mappings linearly separable during learning sensory stimuli desired response simultaneously presented input feedforward connections form selforganized representations input suppressed feedback connections learn transpose feedforward connectivity during recall suppression removed sensory input activates selforganized representation activity generates learned response
technical report no department statistics university toronto abstract one way sample distribution sample uniformly region plot density function a markov chain converges uniform distribution constructed alternating uniform sampling vertical direction uniform sampling horizontal slice defined current vertical position variations slice sampling methods easily implemented univariate distributions used sample multivariate distribution updating variable turn this approach often easier implement gibbs sampling may efficient easilyconstructed versions metropolis algorithm slice sampling therefore attractive routine markov chain monte carlo applications use software automatically generates markov chain sampler model specification one also easily devise overrelaxed versions slice sampling sometimes greatly improve sampling efficiency suppressing random walk behaviour random walks also avoided slice sampling schemes simultaneously update variables
markov decision problems mdps provide foundations number problems interest ai researchers studying automated planning reinforcement learning in paper summarize results regarding complexity solving mdps running time mdp solution algorithms we argue although mdps solved efficiently theory study needed reveal practical algorithms solving large problems quickly to encourage future research sketch alternative methods analysis rely struc ture mdps
learning reinforcements promising approach creating intelligent agents however reinforcement learning usually requires large number training episodes we present evaluate design addresses shortcoming allowing connectionist qlearner accept advice given time natural manner external observer in approach advicegiver watches learner occasionally makes suggestions expressed instructions simple imperative programming language based techniques knowledgebased neural networks insert programs directly agents utility function subsequent reinforcement learning integrates refines advice we present empirical evidence investigates several aspects approach show given good advice learner achieve statistically significant gains expected reward a second experiment shows advice improves expected reward regardless stage training given another study demonstrates subsequent advice result gains reward finally present experimental results indicate method powerful naive technique making use advice
this paper presents mathematical foundations dirichlet mixtures used improve database search results homologous sequences variable number sequences protein family domain known we present method condensing information protein database mixture dirichlet densities these mixtures designed combined observed amino acid frequencies form estimates expected amino acid probabilities position profile hidden markov model statistical model these estimates give statistical model greater generalization capacity remotely related family members reliably recognized model dirichlet mixtures shown outperform substitution matrices methods computing expected amino acid distributions database search resulting fewer false positives false negatives families tested this paper corrects previously published formula estimating expected probabilities contains complete derivations dirichlet mixture formulas methods optimizing mixtures match particular databases suggestions efficient implementation
derivational analogy technique reusing problem solving experience improve problem solving performance this research addresses issue common problem solvers use derivational analogy overcoming mismatches past experiences new problems impede reuse first research describes variety mismatches arise proposes new approach derivational analogy uses appropriate adaptation strategies second compares approach seven others common domain this empirical study shows derivational analogy almost always efficient problem solving scratch amount contributes depends ability overcome mismatches
pollack demonstrated secondorder recurrent neural networks act dynamical recognizers formal languages trained positive negative examples observed phase transitions learning ifslike fractal state sets followon work focused mainly extraction minimization finite state automaton fsa trained network however networks capable inducing languages regular therefore equivalent fsa indeed may simpler small network fit training data inducing nonregular language but networks language regular in paper using low dimensional network capable learning tomita data sets present empirical method testing whether language induced network regular we also provide detailed machine analysis trained networks regular nonregular languages
coins technical report january abstract this article presents algorithm inducing multiclass decision trees multivariate tests internal decision nodes each test constructed training linear machine eliminating variables controlled manner empirical results demonstrate algorithm builds small accurate trees across variety tasks
funes p pollack j computer evolution buildable objects fourth european conference artificial life p husbands i harvey eds mit press pp knowledge program would result familiar structures provided algorithm model physical reality purely utilitarian fitness function thus supplying measures feasibility functionality in way evolutionary process runs environment unnecessarily constrained we added however requirement computability reject overly complex structures took long simulations evaluate the results encouraging the evolved structures surprisingly alien look based common knowledge build brick toys instead computer found ways evolutionary search process we able assemble final designs manually confirm accomplish objectives introduced fitness functions after background related problems describe physical simulation model twodimensional lego structures representation encoding applying evolution we demonstrate feasibility work photos actual objects result particular optimizations finally discuss future work draw conclusions in order evolve morphology behavior autonomous mechanical devices manufactured one must simulator operates several constraints resultant controller adaptive enough cover gap simulated real world eral space mechanisms conservative simulation never perfect preserve margin safety efficient quicker test simulation physical production test buildable results convertible simula tion real object computer evolution buildable objects abstract the idea coevolution bodies brains becoming popular little work done evolution physical structure lack general framework evolution creatures simulation constrained reality gap implies resultant objects usually buildable the work present takes step problem body evolution applying evolutionary techniques design structures assembled parts evolution takes place simulator designed computes forces stresses predicts failure dimensional lego structures the final printout program schematic assembly built physically we demonstrate functionality several different evolved entities
in paper concerned problem acquiring knowledge integration our aim construct integrated knowledge base several separate sources the need merge knowledge bases arise example knowledge bases acquired independently interactions several domain experts as opinions different domain experts may differ knowledge bases constructed way normally differ a similar problem also arise whenever separate knowledge bases generated learning algorithms the objective integration construct one system exploits knowledge available good performance the aim paper discuss methodology knowledge integration describe implemented system integ present concrete results demonstrate advantages method
many arthropods particularly insects exhibit sophisticated visually guided behaviours yet cases behaviours guided input hundreds thousands pixels ie ommatidia compound eye inspired observation several years exploring possibilities visually guided robots lowbandwidth vision rather design robot controllers hand use artificial evolution form extended genetic algorithm automatically generate architectures artificial neural networks generate effective sensorymotor coordination controlling mobile robots analytic techniques drawn neuroethology dynamical systems theory allow us understand evolved robot controllers function predict behaviour environments used evolutionary process initial experiments performed simulation techniques successfully transferred work variety real physical robot platforms this chapter reviews past work concentrating analysis evolved controllers gives overview current research we conclude discussion application evolutionary techniques problems biological vision
the major implementational problem reversible jump mcmc commonly natural way choose jump proposals since euclidean structure guide choice in paper consider mechanism guiding proposal choice analysis acceptance probabilities jumps essentially method involves approximation acceptance probability around certain canonical jumps we illustrate procedure using example reversible jump mcmc application involving bayesian analysis graphical gaussian models
instancebased learning methods explicitly remember data receive they usually training phase prediction time perform computation then take query search database similar datapoints build online local model local average local regression predict output value in paper review advantages instance based methods autonomous systems also note ensuing cost hopelessly slow computation database grows large we present evaluate new way structuring database new algorithm accessing maintains advantages instancebased learning earlier attempts combat cost instancebased learning sacrificed explicit retention data applicable instancebased predictions based small number near neighbors reintroduce explicit training phase form interpolative data structure our approach builds multiresolution data structure summarize database experiences resolutions interest simultaneously this permits us query database exibility conventional linear search greatly reduced computational cost
in standard online model learning algorithm tries minimize total number mistakes made series trials on trial learner sees instance either accepts rejects instance told appropriate response we define natural variant model apple tasting learner gets feedback instance accepted we use two transformations relate apple tasting model enhanced standard model false acceptances counted separately false rejections we present strategy trading false acceptances false rejections standard model from one perspective strategy exactly optimal including constants we apply results obtain good general purpose apple tasting algorithm well nearly optimal apple tasting algorithms variety standard classes conjunctions disjunctions n boolean variables we also present analyze simpler transformation useful instances drawn random rather selected adversary
in paper describe algorithm exploits error distribution generated learning algorithm order break domain approximated piecewise learnable partitions traditionally error distribution neglected favor lump error measure rms by however lose lot important information the error distribution tells us algorithm badly exists ridge errors also tells us partition space one part space interfere learning another the algorithm builds variable arity kd tree whose leaves contain partitions using tree new points predicted using correct partition traversing tree we instantiate algorithm using memory based learners crossvalidation
preens parallel research execution environment neural systems distributed neurosimulator targeted networks workstations transputer systems as current applications neural networks often contain large amounts data neural networks involved tasks vision large high requirements memory computational resources imposed target execution platforms preens executed distributed environment ie tools neural network simulation programs running machine connectable via tcpip using approach larger tasks data examined using efficient coarse grained parallelism furthermore design preens allows neural networks running high performance mimd machine transputer system in paper different features design concepts preens discussed these also used applications like image processing
it well known standard learning classifier systems applied many different domains exhibit number problems payoff oscillation difficult regulate interplay reward system background genetic algorithm ga rule chains instability default hierarchies instability alecsys parallel version standard learning classifier system cs suffers problems in paper propose innovative solutions problems we introduce following original features mutespec new genetic operator used specialize potentially useful classifiers energy quantity introduced measure global convergence order apply genetic algorithm system close steady state dynamical adjustment classifiers set cardinality order speed performance phase algorithm we present simulation results experiments run simulated twodimensional world simple agent learns follow light source
supervised neural networks generalize well much less information weights output vectors training cases so learning important keep weights simple penalizing amount information contain the amount information weight controlled adding gaussian noise noise level adapted learning optimize tradeoff expected squared error network amount information weights we describe method computing derivatives expected squared error amount information noisy weights network contains layer nonlinear hidden units provided output units linear exact derivatives computed efficiently without timeconsuming monte carlo simulations the idea minimizing amount information required communicate weights neural network leads number interesting schemes encoding weights
there many applications desirable order rather classify instances here consider problem learning order given feedback form preference judgments ie statements effect one instance ranked ahead another we outline twostage approach one first learns conventional means preference function form prefu v indicates whether advisable rank u v new instances ordered maximize agreements learned preference function we show problem finding ordering agrees best preference function npcomplete even restrictive assumptions nevertheless describe simple greedy algorithm guaranteed find good approximation we discuss online learning algorithm based hedge algorithm finding good linear combination ranking experts we use ordering algorithm combined online learning algorithm find combination search experts domainspecific query expansion strategy www search engine present experimental results demonstrate merits approach
technical report csrp march abstract evolutionary algorithms powerful techniques optimisation whose operation principles inspired natural selection genetics in paper discuss relation evolutionary techniques numerical classical search methods show methods instances single general search strategy call evolutionary computation cookbook by combining features classical evolutionary methods different ways new instances general strategy generated ie new evolutionary classical algorithms designed one algorithm ga fl described
we present neural net architecture discover hierarchical recursive structure symbol strings to detect structure multiple levels architecture capability reducing symbols substrings single symbols makes use external stack memory in terms formal languages architecture learn parse strings lr contextfree grammar given training sets positive negative exemplars architecture trained recognize many different grammars the architecture one layer modifiable weights allowing many cognitive domains involve complex sequences contain hierarchical recursive structure eg music natural language parsing event perception to illustrate spider ate hairy fly noun phrase containing embedded noun phrase hairy fly understanding multilevel structures requires forming reduced descriptions hinton string symbols states hairy fly reduced single symbolic entity noun phrase we present neural net architecture learns encode structure symbol strings via reduction transformations the difficult problem extracting multilevel structure complex extended sequences studied mozer ring rohwer schmidhuber among others while previous efforts made straightforward interpretation behavior
selforganizing feature maps usually implemented abstracting lowlevel neural parallel distributed processes an external supervisor finds unit whose weight vector closest euclidian distance input vector determines neighborhood weight adaptation the weights changed proportional euclidian distance in biologically plausible implementation similarity measured scalar product neighborhood selected lateral inhibition weights changed redistributing synaptic resources the resulting selforganizing process quite similar abstract case however process somewhat hampered boundary effects parameters need carefully evolved it also necessary add redundant dimension input vectors
the application decision making learning algorithms multiagent systems presents many interestingresearch challenges opportunities among ability agents learn act observing imitating agents we describe algorithm iqalgorithm integrates imitation qlearning roughly qlearner uses observations made expert agent bias exploration promising directions this algorithm goes beyond previous work direction relaxing oftmade assumptions learner observer expert observed agent share objectives abilities our preliminary experiments demonstrate significant transfer agents using iqmodel many cases reductions training time
faces represent complex multidimensional meaningful visual stimuli developing computational model face recognition difficult we present hybrid neural network solution compares favorably methods the system combines local image sampling selforganizing map neural network convolutional neural network the selforganizing map provides quantization image samples topological space inputs nearby original space also nearby output space thereby providing dimensionality reduction invariance minor changes image sample convolutional neural network provides partial invariance translation rotation scale deformation the convolutional network extracts successively larger features hierarchical set layers we present results using karhunenloeve transform place selforganizing map multilayer perceptron place convolutional network the karhunenloeve transform performs almost well error versus the multilayer perceptron performs poorly error versus the method capable rapid classification requires fast approximate normalization preprocessing consistently exhibits better classification performance eigenfaces approach database considered number images per person training database varied with images per person proposed method eigenfaces result error respectively the recognizer provides measure confidence output classification error approaches zero rejecting examples we use database images individuals contains quite high degree variability expression pose facial details we analyze computational complexity discuss new classes could added trained recognizer
we present new algorithm solving markov decision problems extends modified policy iteration algorithm puterman shin two important ways the new algorithm asynchronous allows values states updated arbitrary order need consider actions state updating policy the new algorithm converges general initial conditions required modified policy iteration specifically set initial policyvalue function pairs algorithm guarantees convergence strict superset set modified policy iteration converges this generalization obtained making simple easily implementable change policy evaluation operator used updating value function both asynchronous nature algorithm convergence general conditions expand range problems algorithm applied
recently markov chain monte carlo mcmc sampling methods become widely used determining properties posterior distribution alternative gibbs sampler elaborate hitandrun sampler generalization blackbox sampling scheme generate timereversible markov chain posterior distribution the proof convergence applications bayesian computation constrained parameter spaces provided comparisons mcmc samplers made in addition propose importance weighted marginal density estimation iwmde method an iwmde obtained averaging many dependent observations ratio full joint posterior densities multiplied weighting conditional density w the asymptotic properties iwmde guidelines choosing weighting conditional density w also considered the generalized version iwmde estimating marginal posterior densities full joint posterior density contains analytically intractable normalizing constants developed furthermore develop monte carlo methods based kullbackleibler divergences comparing marginal posterior density estimators this article summary authors phd thesis presented savage award session
in paper characterize complexity noisetolerant learning pac model specifically show general lower bound logffi number examples required pac learning presence classification noise combined result simon effectively show sample complexity pac learning presence classification noise vcf furthermore demonstrate optimality general lower bound providing noisetolerant learning algorithm class symmetric boolean functions uses sample size within constant factor bound finally note general lower bound compares favorably various general upper bounds pac learning presence classification noise
it recently realized parasite virulence harm caused parasites hosts adaptive trait selection particular level virulence happen either level betweenhost tradeoffs result shortsighted withinhost competition this paper describes simulations study effect modifier genes changes mutation rate suppressing shortsighted development virulence investigates interaction simplified model im mune clearance
much work qualitative physics involves constructing models physical systems using functional descriptions flow monotonically increases pressure semiquantitative methods improve model precision adding numerical envelopes monotonic functions ad hoc methods normally used determine envelopes this paper describes systematic method computing bounding envelope multivariate monotonic function given stream data the derived envelope computed determining simultaneous confidence band special neural network guaranteed produce monotonic functions by composing envelopes complex systems simulated using semiquantitative methods
in paper describe application memorybased learning problem prepositional phrase attachment disambiguation we compare memorybased learning stores examples memory generalizes using intelligent similarity metrics number recently proposed statistical methods well suited large numbers features we evaluate methods common benchmark dataset show method compares favorably previous methods wellsuited incorporating various unconventional representations word patterns value difference metrics lexical space
hierarchically structured mixture models studied context data analysis inference neural synaptic transmission characteristics mammalian central nervous systems mixture structures arise due uncertainties stochastic mechanisms governing responses electrochemical stimulation individual neurotransmitter release sites nerve junctions models attempt capture scientific features sensitivity individual synaptic transmission sites electrochemical stimuli extent electrochemical responses stimulated this done via suitably structured classes prior distributions parameters describing features such priors may structured permit assessment currently topical scientific hypotheses fundamental neural function posterior analysis implemented via stochastic simulation several data analyses described illustrate approach resulting neurophysiological insights recently generated experimental contexts further developments open questions neurophysiological statistical noted research partially supported nsf grants dms dms dms this work represents part collaborative project dr dennis a turner duke university medical center durham va data provided dr turner dr howard v wheal southampton university a slightly revised version paper published journal american statistical association vol pp modified title hierarchical mixture models neurological transmission analysis the author recipient mitchell prize bayesian analysis substantive concrete problem based work reported paper
the need software modules performing natural language processing nlp tasks growing these modules perform efficiently accurately time rapid development often mandatory recent work indicated machine learning techniques general memorybased learning mbl particular offer tools meet ends we present examples modules trained mbl three nlp tasks texttospeech conversion ii partofspeech tagging iii phrase chunking we demonstrate three modules display high generalization accuracy argue mbl applicable similarly well large class nlp tasks
we present membership query ie interpolation algorithm exactly identifying class readonce formulas basis boolean threshold functions using generic transformation angluin hellerstein karpinski gives algorithm using membership equivalence queries exactly identifying class readonce formulas basis boolean threshold functions negation we also present series generic transformations used convert algorithm one learning model algorithm different model
we study time series model viewed decision tree markov temporal structure the model intractable exact calculations thus utilize variational approximations we consider three different distributions approximation one markov calculations performed exactly layers decision tree decoupled one decision tree calculations performed exactly time steps markov chain decoupled one viterbilike assumption made pick single likely state sequence we present simulation results artificial data bach chorales accepted oral presentation nips
stochastic simulation algorithms likelihood weighting often give fast accurate approximations posterior probabilities probabilistic networks methods choice large networks unfortunately special characteristics dynamic probabilistic networks dpns used represent stochastic temporal processes mean standard simulation algorithms perform poorly in essence simulation trials diverge reality process observed time in paper present simulation algorithms use evidence observed time step push set trials back towards reality the first algorithm evidence reversal er restructures time slice dpn evidence nodes slice become ancestors state variables the second algorithm called survival fittest sampling sof repopulates set trials time step using stochastic reproduction rate weighted likelihood evidence according trial we compare performance algorithm likelihood weighting original network also investigate benefits combining er sof methods the ersof combination appears maintain bounded error independent number time steps simulation
simulated annealing search technique single trial solution modified random an energy defined represents good solution the goal find best solution minimising energy changes lead lower energy always accepted increase probabilistically accepted the probability given expek b t where e change energy k b constant t temperature initially temperature high corresponding liquid molten state large changes possible progressively reduced using cooling schedule allowing smaller changes system solidifies low energy solution
systems learn examples often create disjunctive concept definition the disjuncts concept definition cover training examples referred small disjuncts the problem small disjuncts error prone large disjuncts may necessary achieve high level predictive accuracy holte acker porter this paper extends previous work done problem small disjuncts investigating reasons small disjuncts error prone large disjuncts evaluating impact small disjuncts inductive learning this paper shows attribute noise missing attributes class noise training set size cause small disjuncts error prone large disjuncts this paper also evaluates impact factors learning small disjuncts ie error rate it shows two artificial domains low levels attribute noise applied training set ability learn correct noisefree concept evaluated small disjuncts primarily responsible making learning difficult
a number efficient learning algorithms achieve exact identification unknown function class using membership equivalence queries using standard transformation algorithms easily converted online learning algorithms use membership queries under transformation number equivalence queries made query algorithm directly corresponds number mistakes made online algorithm in paper consider several natural classes known learnable setting investigate minimum number equivalence queries accompanying counterexamples equivalently minimum number mistakes online model made learning algorithm makes polynomial number membership queries uses polynomial computation time we able reduce number equivalence queries used previous algorithms often prove matching lower bounds as example consider class dnf formulas n variables k olog n terms previously algorithm blum rudich br provided best known upper bound ok log n minimum number equivalence queries needed exact identification we greatly improve upper bound showing exactly k counterexamples needed learner knows k priori exactly k counterexamples needed learner know k priori this exactly matches known lower bounds bc for many results obtain complete characterization tradeoff number membership equivalence queries needed exact identification the classes consider monotone dnf formulas horn sentences olog nterm dnf formulas readk satj dnf formulas readonce formulas various bases deterministic finite automata
we present learning algorithm rulebased concept representations called rippledown rule sets rippledown rule sets allow us deal exceptions rule separately introducing exception rules exception rules exception rule etc constant depth these local exception rules contrast decision lists exception rules must placed global ordering rules the localization exceptions makes possible represent concepts decision list representation on hand decision lists constant number alternations rules different classes represented constant depth rippledown rule sets polynomial increase size our algorithm occam algorithm constant depth rippledown rule sets hence pac learning algorithm it based repeatedly applying greedy approximation method weighted set cover problem find good exception rule sets
we present algorithm learning sets rules organized k levels each level contain arbitrary number rules c l l class associated level c concept given class basic concepts the rules higher levels precedence rules lower levels used represent exceptions as basic concepts use boolean attributes infinite attribute space model certain concepts defined terms substrings given sample examples algorithm runs polynomial time produces consistent concept representation size olog k n k n size smallest consistent representation k levels rules this implies algorithm learns pac model the algorithm repeatedly applies greedy heuristics weighted set cover the weights obtained approximate solutions previous set cover problems
in paper investigate representational methodological issues attractor network model mapping orthography semantics based plaut we find contrary psycholinguistic studies response time concrete words represented bits output pattern slower abstract words this model also predicts response times words dense semantic neighborhood faster words semantically similar neighbors language this conceptually consistent neighborhood effect seen mapping orthography phonology seidenberg mcclelland plaut et al patterns many neighbors faster pathways since regularity random mapping used clear cause effect different previous experiments we also report rather distressing finding reaction time model measured time takes network settle presented new input when criterion used determine network settled changed include testing hidden units results reported change direction effect abstract words slower words dense semantic neighborhoods since independent reasons exclude hidden units stopping criterion done common practice believe phenomenon interest mostly neural network practitioners however provide insight interaction hidden output units settling
casebased reasoning cbr used form caching solved problems speedup later problem solving using cached cases brings additional costs due retrieval time case adaptation time also storage space simply storing cases result situation retrieving trying adapt old cases take time average caching this means caching must applied selectively build case memory actually useful this form utility problem the approach taken construct cost model system used predict effect changes system in paper describe utility problem associated caching cases construction cost model we present experimental results demonstrate model used predict effect certain changes case memory
a genetic algorithmic ga approach vector quantizer design combines conventional generalized lloyd algorithm gla presented we refer hybrid genetic generalized lloyd algorithm ggla it works briefly follows a finite number codebooks called chromosomes selected each codebook undergoes iterative cycles reproduction we perform experiments various alternative design choices using gaussianmarkov processes speech image source data signaltonoise ratio snr performance measure in cases ggla showed performance improvements respect gla we also compare results zadorgersho formula
in casebased planning cbp previously generated plans stored cases memory reused solve similar planning problems future cbp save considerable time planning scratch generative planning thus offering potential heuristic mechanism handling intractable problems one drawback cbp systems need highly structured memory requires significant domain engineering complex memory indexing schemes enable efficient case retrieval in contrast cbp system caper based massively parallel framebased ai language extremely fast retrieval complex cases large unindexed memory the ability fast frequent retrievals many advantages indexing unnecessary large casebases used memory probed numerous alternate ways allowing specific retrieval stored plans better fit target problem less adaptation fl preliminary version article appearing ieee expert february pp this paper extended version
we present efficient method assigning number processors tasks associated cells rectangular uniform grid load balancing equipartition constraints observed approximately minimizing total perimeter partition corresponds amount interprocessor communication this method based upon decomposition grid stripes optimal height we prove mild assumptions problem size grows large parameters error bound associated feasible solution approaches zero we also present computational results high level parallel genetic algorithm utilizes method make comparisons methods on network workstations algorithm solves within minutes instances problem would require one billion binary variables quadratic assignment formulation
finding bayesian balance exploration exploitation adaptive optimal control general intractable this paper shows compute suboptimal estimates based certainty equivalence approximation arising form dual control this systematizes extends existing uses exploration bonuses reinforcement learning sutton the approach two components statistical model uncertainty world way turning exploratory behaviour
this paper deals nonlinear leastsquares problems involving fitting data parameterized analytic functions for generic regression data general result establishes countability stronger assumptions finiteness set functions giving rise critical points quadratic loss function in special case usually called singlehidden layer neural networks built upon standard sigmoidal activation tanhx equivalently e x rough upper bound cardinality provided well
this research funded part nsf grant no iri part onr grant no nj we thank john clement use protocol transcript james greeno contribution developing constructive modeling interpretation ryan tweney helpful comments todd w griffith nancy j nersessian ashok goel abstract we hypothesize generic models central conceptual change science this hypothesis origins two theoretical sources the first source constructive modeling derives philosophical theory synthesizes analyses historical conceptual changes science investigations reasoning representation cognitive psychology the theory constructive modeling posits generic mental models productive conceptual change the second source adaptive modeling derives computational theory creative design both theories posit situation independent domain abstractions ie generic models using constructive modeling interpretation reasoning exhibited protocols collected john clement problem solving session involving conceptual change employ resources theory adaptive modeling develop new computational model torque here describe piece analysis protocol illustrate synthesis two theories used develop system articulating testing torque the results research show generic modeling plays central role conceptual change they also demonstrate interdisciplinary synthesis provide significant insights scientific reasoning
this paper discusses design neural networks solve specific problems adaptive control in particular investigates influence typical problems arising realworld control tasks well techniques solution exist framework neurocontrol based investigation systematic design method developed the method exemplified development adaptive force controller robot manipulator
we present informationtheoretic derivation learning algorithm clusters unlabelled data linear discriminants in contrast methods try preserve information input patterns maximize information gained observing output robust binary discriminators implemented sigmoid nodes we derive local weight adaptation rule via gradient ascent objective demonstrate dynamics simple data sets relate approach previous work suggest directions may extended
this paper presents asocs adaptive selforganizing concurrent system model massively parallel processing incrementally defined rule systems areas adaptive logic robotics logical inference dynamic control an asocs adaptive network composed many simple computing elements operating asynchronously parallel this paper focuses adaptive algorithm aa details architecture learning algorithm aa significant memory knowledge maintenance advantages previous asocs models an asocs operate either data processing mode learning mode during learning mode asocs given new rule expressed boolean conjunction the aa learning algorithm incorporates new rule distributed fashion short bounded time during data processing mode asocs acts parallel hardware circuit
this paper presents method analyzing coupled time series using markov models domain state space immense to make parameter estimation tractable large state space represented cartesian product smaller state spaces paradigm known factorial markov models the transition matrix model represented mixture transition matrices underlying dynamical processes this formulation know mixed memory markov models using framework analyze daily exchange rates five currencies british pound canadian dollar deutsch mark japanese yen swiss franc measured us dollar
this work explores use machine learning methods extracting knowledge simulations complex systems in particular use genetic algorithms learn rulebased strategies used autonomous robots the evaluation given strategy may require several executions simulation produce meaningful estimate quality strategy as consequence evaluation single individual genetic algorithm requires fairly substantial amount computation such system suggests sort largegrained parallelism available network workstations we describe implementation parallel genetic algorithm present case studies resulting speedup two robot learning tasks
most artificial neural networks anns fixed topology learning often suffer number shortcomings result variations anns use dynamic topologies shown ability overcome many problems this paper introduces locationindependent transformations lits general strategy implementing distributed feedforward networks use dynamic topologies dynamic anns efficiently parallel hardware a lit creates set locationindependent nodes node computes part network output independent nodes using local information this type transformation allows efficient support adding deleting nodes dynamically learning in particular paper presents lit dynamic backpropagation networks single hidden layer the complexity learning execution algorithms onplogm single pattern nis number inputs p number outputs number hidden nodes original network keywords neural networks backpropagation implementation design dynamic topologies reconfigurable architectures
hard combinatorial problems sequencing scheduling led recently research genetic algorithms canonical coding symmetric tsp modified coding njob mmachine flowshop problem configurates solution space different way we show well known genetic operators act intelligently coding scheme they implecitely prefer subset solutions contain probably best solutions respect objective we conjecture every new problem needs determination necessary condition genetic algorithm work e proof experiment we implemented asynchronous parallel genetic algorithm unixbased computer network computational results new heuristic discussed
this paper presents vlsi implementation priority adaptive selforganizing concurrent system pasocs learning model built using multichip module mcm substrate many current hardware implementations neural network learning models direct implementations classical neural network structuresa large number simple computing nodes connected dense number weighted links pasocs one class asocs adaptive selforganizing concurrent system connectionist models whose overall goal classical neural networks models whose functional mechanisms differ significantly this model potential application areas pattern recognition robotics logical inference dynamic control
the application adaptive optimization strategies scheduling manufacturing systems recently become research topic broad interest population based approaches scheduling predominantly treat static data models whereas realworld scheduling tends dynamic problem this paper briefly outlines application genetic algorithm dynamic job shop problem arising production scheduling first sketch genetic algorithm handle release times jobs in second step preceding simulation method used improve performance algorithm finally job shop regarded nondeterministic optimization problem arising occurrence job releases temporal decomposition leads scheduling control interweaves simulation time genetic search
neural network pruning methods level individual network parameters eg connection weights improve generalization shown empirical study however open problem pruning methods known today obd obs autoprune epsiprune selection number parameters removed pruning step pruning strength this work presents pruning method lprune automatically adapts pruning strength evolution weights loss generalization training the method requires algorithm parameter adjustment user results statistical significance tests comparing autoprune lprune static networks early stopping given based extensive experimentation different problems the results indicate training pruning often significantly better rarely significantly worse training early stopping without pruning furthermore lprune often superior autoprune superior obd diagnosis tasks unless severe pruning early training process required
casebased problemsolving systems rely similarity assessment select stored cases whose solutions easily adaptable fit current problems however widelyused similarity assessment strategies evaluation semantic similarity poor predictors adaptability as result systems may select cases difficult impossible adapt even easily adaptable cases available memory this paper presents new similarity assessment approach couples similarity judgments directly case library containing systems adaptation knowledge it examines approach context casebased planning system learns new plans new adaptations empirical tests alternative similarity assessment strategies show approach enables better case selection increases benefits accrued learned adaptations
the casebased reasoning process depends multiple overlapping knowledge sources provides opportunity learning exploiting opportunities requires determining learning mechanisms use individual knowledge source also different learning mechanisms interact combined utility this paper presents case study examining relative contributions costs involved learning processes three different knowledge sourcescases case adaptation knowledge similarity informationin casebased planner it demonstrates importance interactions different learning processes identifies promising method integrating multiple learning methods improve casebased reasoning
casebased reasoning depends multiple knowledge sources beyond case library including knowledge case adaptation criteria similarity assessment because hand coding knowledge accounts large part knowledge acquisition burden developing cbr systems appealing acquire learning cbr promising learning method apply this observation suggests developing casebased cbr systems cbr systems whose components use cbr however despite early interest casebased approaches cbr method received comparatively little attention open questions include casebased components cbr system designed amount knowledge acquisition effort require effectiveness this paper investigates questions case study issues addressed methods used results achieved casebased planning system uses cbr guide case adaptation similarity assessment the paper discusses design considerations presents empirical results support usefulness casebased cbr point potential problems tradeoffs directly demonstrate overlapping roles different cbr knowledge sources the paper closes general lessons casebased cbr areas future research
a linear support vector machine formulation used generate fast finitelyterminating linearprogramming algorithm discriminating two massive sets ndimensional space number points orders magnitude larger n the algorithm creates succession sufficiently small linear programs separate chunks data time the key idea small number support vectors corresponding linear programming constraints positive dual variables carried successive small linear programs containing chunk data we prove procedure monotonic terminates finite number steps exact solution leads globally optimal separating plane entire dataset numerical results fully dense publicly available datasets numbering million points dimensional space confirm theoretical results demonstrate ability handle large problems
in sejnowski rosenberg developed famous nettalk system english texttospeech this chapter describes machine learning approach texttospeech builds upon extends initial nettalk work among many extensions nettalk system following different learning algorithm wider input window errorcorrecting output coding righttoleft scan word pronounced results decision influencing subsequent decisions addition several useful input features these changes yielded system performs much better original nettalk system after training words system achieves correct pronunciation individual phonemes correct pronunciation whole words pronunciation must exactly match dictionary pronunciation correct based judgements three human participants blind assessment study system estimated serious error rate whole words compared error rate dectalk rulebase
the problem minimizing number misclassified points plane attempting separate two point sets intersecting convex hulls ndimensional real space formulated linear program equilibrium constraints lpec this general lpec converted exact penalty problem quadratic objective linear constraints a frankwolfetype algorithm proposed penalty problem terminates stationary point global solution novel aspects approach include a linear complementarity formulation step function counts misclassifications ii exact penalty formulation without boundedness nondegeneracy constraint qualification assumptions iii an exact solution extraction sequence minimizers penalty function finite value penalty parameter general lpec explicitly exact solution lpec uncoupled constraints iv a parametric quadratic programming formulation lpec associated misclassification minimization problem
planning analogical reasoning learning method consists storage retrieval replay planning episodes planning performance improves accumulation reuse library planning cases retrieval driven domaindependent similarity metrics based planning goals scenarios in complex situations multiple goals retrieval may find multiple past planning cases jointly similar new planning situation this paper presents issues implications involved replay multiple planning cases opposed single one multiple case plan replay involves adaptation merging annotated derivations planning cases several merge strategies replay introduced process various forms eagerness differences past new situations annotated justifications planning cases in particular introduce effective merging strategy considers plan step choices especially appropriate interleaving planning plan execution we illustrate discuss effectiveness merging strategies specific domains
mixedinitiative planning envisions framework automated human planners interact jointly construct plans satisfy specific objectives in paper report work engineering robust mixedinitiative planning system human planners rely strongly past planning experience generate new plans format casebased system supports human planning accumulation userbuilt plans querydriven browsing past plans several plan functionality analysis primitives prodigyanalogy automated ai planner combines generative casebased planning stored plans annotated plan rationale reuse involves adaptation driven rationale our system micbp integrates format prodigyanalogy realtime messagepassing mixedinitiative planning system the main technical approach consists allowing user specify link objectives enable system capture reuse plan rationale we present micbp concrete application domain military force deployment planning this synergistic system increases planning efficiency human planners automated suggestion similar past plans plausible plan modifications
the primary goal inductive learning generalize well induce function accurately produces correct output future inputs hansen salamon showed certain assumptions combining predictions several separately trained neural networks improve generalization one key assumptions individual networks independent errors produce in standard way performing backpropagation assumption may violated standard procedure initialize network weights region weight space near origin this means backpropagations gradientdescent search may reach small subset possible local minima in paper present approach initializing neural networks uses competitive learning intelligently create networks originally located far origin weight space thereby potentially increasing set reachable local minima we report experiments two realworld datasets combinations networks initialized method generalize better combina tions networks initialized traditional way
we present two algorithms inducing structural equation models data assuming latent variables models causal interpretation parameters may estimated linear multiple regression our algorithms comparable pc ic rely conditional independence we present algorithms empirical comparisons pc ic
we investigate neural network based approximation methods these methods depend locality basis functions after discussing local global basis functions propose multiresolution hierarchical method the various resolutions stored various levels tree at root tree global approximation kept leafs store learning samples intermediate nodes store intermediate representations in order find optimal partitioning input space selforganising maps soms used the proposed method implementational problems reminiscent encountered manyparticle simulations we investigate parallel implementation method using parallel hierarchical meth ods manyparticle simulations starting point
orthogonal incremental learning oil new approach incremental training feedforward network single hidden layer oil based idea describe output weights hidden nodes set orthogonal basis functions hidden nodes treated orthogonal representation network output weights domain we proved separate training hidden nodes conflict previously optimized nodes described special relationship orthogonal backpropagation obp rule an advantage oil existing algorithms extremely fast learning this approach also easily extended buildup incrementally arbitrary function linear composition adjustable functions necessarily orthogonal oil tested twospirals net talk benchmark problems
todays potential users machine learning technology faced nontrivial problem choosing large everincreasing number available tools one appropriate particular task to assist often noninitiated users desirable model selection process automated using experience base level learning researchers proposed metalearning possible solution historically predictive accuracy de facto criterion work metalearning focusing discovery rules match applications models based accuracy although predictive accuracy clearly important criterion also case number criteria could often ought considered learning model selection this paper presents number criteria discusses impact metalevel approaches model selection
one approach invariant object recognition employs recurrent neural network associative memory in standard depiction networks state space memories objects stored attractive fixed points dynamics i argue modification picture object continuous family instantiations represented continuous attractor this idea illustrated network learns complete patterns to perform task filling missing information network develops continuous attractor models manifold patterns drawn from statistical viewpoint pattern completion task allows formulation unsupervised a classic approach invariant object recognition use recurrent neural network associative memory in spite intuitive appeal biological plausibility approach largely abandoned practical applications this paper introduces two new concepts could help resurrect object representation continuous attractors learning attractors pattern completion in models associative memory memories stored attractive fixed points discrete locations state space discrete attractors may appropriate patterns continuous variability like images threedimensional object different viewpoints when instantiations object lie continuous pattern manifold appropriate represent objects attractive manifolds fixed points continuous attractors to make idea practical important find methods learning attractors examples a naive method train network retain examples shortterm memory this method deficient prevent network storing spurious fixed points unrelated examples a superior method train network restore examples corrupted learns complete patterns filling missing information learning terms regression rather density estimation
in paper consider problem independent constraint handling mechanism stepwise adaptation weights saw show working graph coloring problems sawing technically belongs penalty function based approaches amounts modifying penalty function search we show twofold benefit first proves rather insensitive technical parameters thereby providing general problem independent way handle constrained problems second leads superior ea performance in extensive series comparative experiments show sawing ea outperforms powerful graph coloring heuristic algorithm dsatur hardest graph instances linear scaleup behaviour
recently several neural algorithms introduced independent component analysis here approach problem point view single neuron first simple hebbianlike learning rules introduced estimating one independent components sphered data some learning rules used estimate independent component negative kurtosis others estimate component positive kurtosis next twounit system introduced estimate independent component kurtosis the results generalized estimate independent components nonsphered raw mixtures to separate several independent components system several neurons linear negative feedback used the convergence learning rules rigorously proven without unnecessary hypotheses distributions independent components
in paper define task place learning describe one approach problem the framework represents distinct places using evidence grids probabilistic description occupancy place recognition relies casebased classification augmented registration process correct translations the learning mechanism also similar casebased systems involving simple storage inferred evidence grids experimental studies physical simulated robots suggest approach improves place recognition experience handle significant sensor noise scales well increasing numbers places previous researchers studied evidence grids place learning combined two powerful concepts used experimental methods machine learning evaluate methods abilities
in constructive induction ci learners problem representation modified normal part learning process this useful initial representation inadequate inappropriate in paper i argue distinction constructive nonconstructive methods unclear i propose theoretical model allows clean distinction made b process ci properly motivated i also show although constructive induction used almost exclusively context supervised learning reason form part unsupervised regime
when designing deductive database designer decide predicate relation whether defined extensionally intensionally definition look like an intelligent system presented assist designer task it starts example database predicates defined extensionally it tries compact database transforming extensionally defined predicates intensionally defined ones the intelligent system employs techniques area inductive logic programming
when work information multiple sources formalism employs handle uncertainty may uniform in order able combine knowledge bases different formats need first establish common basis characterizing evaluating different formalisms provide semantics combined mechanism a common framework provide infrastructure building integrated system essential understand behavior we present unifying framework based ordered partition possible worlds called partition sequences corresponds intuitive notion biasing towards certain possible scenarios uncertain actual situation we show existing formalisms namely default logic autoepistemic logic probabilistic conditioning thresholding generalized conditioning possibility theory incorporated general framework
this paper investigates technique creating sparsely connected feedforward neural networks may capable producing networks large input output layers the architecture appears particularly suited tasks involve sparse training data able take advantage sparseness reduce training time some initial results presented based tests bit compression problem
in paper propose method calculate posterior probability nondecomposable graphical gaussian model our proposal based new device sample wishart distributions conditional graphical constraints as result methodology allows bayesian model selection within whole class graphical gaussian models including nondecomposable ones
for absorbing markov chain reinforcement transition bertsekas gives simple example function learned td depends bertsekas showed approximation optimal respect leastsquares error value function approximation obtained td method poor respect metric with respect error values td approximates function better td however respect error differences values td approximates function better td td better td respect former metric rather latter in addition direct td weights errors unequally residual gradient methods baird harmon baird klopf weight errors equally for case control simple markov decision process presented direct td residual gradient td learn optimal policy td learns suboptimal policy these results suggest example differences state values significant state values td preferable td
lazy learning methods provide useful representations training algorithms learning complex phenomena autonomous adaptive control complex systems this paper surveys ways locally weighted learning type lazy learning applied us control tasks we explain various forms control tasks take affects choice learning paradigm the discussion section explores interesting impact explicitly remembering previous experiences problem learning control
genetic programming gp variant genetic algorithms data structures handled trees this makes gp especially useful evolving functional relationships computer programs represented trees symbolic regression determination function dependence gx approximates set data points x in paper feasibility symbolic regression gp demonstrated two examples taken different domains furthermore several suggested methods literature compared intended improve gp performance readability solutions taking account introns redundancy occurs trees keeping size trees small the experiments show gp elegant useful tool derive complex functional dependencies numerical data
we report study mixture modeling problems arising assessment chemical structureactivity relationships drug design discovery pharmaceutical research laboratories developing test compounds screening synthesize many related candidate compounds linking together collections basic molecular building blocks known monomers these compounds tested biological activity feeding screening analysis drug design the tests also provide data relating compound activity chemical properties aspects structure associated monomers focus studying relationships aid future monomer selection the level chemical activity compounds based geometry chemical binding test compounds target binding sites receptor compounds screening tests unable identify binding configurations hence potentially critical covariate information missing natural latent variable resulting statistical models mixed respect missing information complicating data analysis inference this paper reports study twomonomer twobinding site framework associated data we build structured mixture models mix linear regression models predicting chemical effectiveness respect sitebinding selection mechanisms we discuss aspects modeling analysis including problems pitfalls describe results analyses simulated real data set in modeling real data led critical model extensions introduce hierarchical random effects components adequately capture heterogeneities site binding mechanisms resulting levels effectiveness compounds bound comments current potential future directions conclude report
we give analysis generalization error cross validation terms two natural measures difficulty problem consideration approximation rate accuracy target function ideally approximated function number hypothesis parameters estimation rate deviation training generalization errors function number hypothesis parameters the approximation rate captures complexity target function respect hypothesis model estimation rate captures extent hypothesis model suffers overfitting using two measures give rigorous general bound error cross validation the bound clearly shows tradeoffs involved making fl fraction data saved testing large small by optimizing bound respect fl argue combination formal analysis plotting controlled experimentation following qualitative properties cross validation behavior quite robust significant changes underlying model selection problem
model selection eg considered problem choosing hypothesis language provides optimal balance low empirical error high structural complexity in abstract discuss intuition new efficient approach model selection our approach inherently bayesian eg instead using priors target functions hypotheses talk priors error values leads us new mathematical characterization expected true error in setting classification learning learner given sample drawn according unknown distribution labeled instances returns empirical minimizer hypothesis least empirical error certain unknown true error if process carried repeatedly true error empirical minimizer vary run run empirical minimizer depends randomly drawn sample this induces distribution true errors empirical minimizers possible samples drawn according unknown distribution if distribution would known one could easily derive expected true error empirical minimizer model integrating distribution this would immediately lead optimal model selection algorithm enumerate models calculate expected error model integrating error distribution select model least expected error pac theory vc framework provide worstcase bounds chance drawing sample true error minimizer exceeds worstcase meaning hold distribution instances concept given class by contrast focus determine distribution fixed given learning problem specified assumptions unlike worstcase bound depends size vcdimension hypothesis space actual error distribution depends hypothesis space unknown distribution labeled instances however prove certain assumption independence hypotheses distribution true errors hence expected true error expressed function distribution empirical errors uniformly drawn hypotheses thought prior error values the latter distribution always onedimensional estimated fixedsized initial portion training data fixedsized set randomly drawn hypotheses this estimate distribution leads us estimate expected true error empirical minimizer model turn leads highly efficient model selection algorithm we study behavior approach several controlled experiments our results show accuracy error estimate least comparable accuracy estimate obtained fold crossvalidation provided prior error values estimated using least examples but cv requires ten invocations learner per model time algorithm requires assess model constant size model we also study robustness algorithm violations independence assumptions we observe bias predictions hypotheses space size four less when hypothesis space size dependencies diluted violations assumptions negligible incur significant error the full paper available httpkicstuberlindeschefferpaperseedreportps
in area inductive learning generalization main operation usual definition induction based logical implication recently rising interest clausal representation knowledge machine learning almost inductive learning systems perform generalization clauses use relation subsumption instead implication the main reason wellknown simple technique compute least general generalizations subsumption implication however generalization subsumption inappropriate learning recursive clauses crucial problem since recursion basic program structure logic programs we note implication clauses undecidable therefore introduce stronger form implication called timplication decidable clauses we show every finite set clauses exists least general generalization timplication we describe technique reduce generalizations implication clause generalizations subsumption call expansion original clause moreover show every nontautological clause exists tcomplete expansion means every generalization timplication clause reduced generalization subsumption expansion
this paper argues bayesian probability theory general method machine learning from two wellfounded axioms theory capable accomplishing learning tasks incremental nonincremental supervised unsupervised it learn different types data regardless whether noisy perfect independent facts behaviors unknown machine these capabilities partially demonstrated paper uniform application theory two typical types machine learning incremental concept learning unsupervised data classification the generality theory suggests process learning may many different types currently held method oldest may best
this paper focuses bias variance decomposition analysis local learning algorithm nearest neighbor classifier extended error correcting output codes this extended algorithm often considerably reduces ie classification error comparison nearest neighbor ricci aha the analysis presented reveals performance improvement obtained drastically reducing bias cost increasing variance we also show even classification problems classes extending codeword length beyond limit assures column separation yields error reduction this error reduction variance due voting mechanism used errorcorrecting output codes also bias
we integrated distributed search genetic programming gp based systems collective memory form collective adaptation search method such system significantly improves search problem complexity increased since pure gp approach scale well problem complexity natural question two components actually contributing search process we investigate collective memory search utilizes random search engine find significantly outperforms gp based search engine we examine solution space show problem complexity search space grow collective adaptive system perform better collective memory search employing random search engine
the document presents approach judging relevance retrieved information based novel approach similarity assessment contrary systems define relevance measures context similarity query time this necessary since without context similarity one guarantee similar items also relevant
this paper presents selfimproving reactive control system autonomous robotic navigation the navigation module uses schemabased reactive control system perform navigation task the learning module combines casebased reasoning reinforcement learning continuously tune navigation system experience the casebased reasoning component perceives characterizes systems environment retrieves appropriate case uses recommendations case tune parameters reactive control system the reinforcement learning component refines content cases based current experience together learning components perform online adaptation resulting improved performance reactive control system tunes environment well online learning resulting improved library cases capture environmental regularities necessary perform online adaptation the system extensively evaluated simulation studies using several performance metrics system configurations
a model onsite learning presented the system learns querying hard patterns classifying easy ones this model related querybased filtering methods takes account addition labelling filtering data cost a simple policies introduced analyzed simple problem d high low game in addition querybycommittee algorithm seung et al suggested good approximator model space realworld domains results using algorithm synthesized problem realworld ocr task using backpropagation network nearest neighbor classifier show onsite learner perform well classifier trained offsite achieving significant cost reduction
the standard method obtaining response treebased genetic programming take value returned root node in nontree representations alternate methods explored one alternative treat specific location indexed memory response value program terminates the purpose paper explore applicability technique treestructured programs explore intron effects studies bring light this papers experimental results support finding memorybased program response technique improvement problems in addition papers experimental results support finding contrary past research speculation addition even facilitation introns seriously degrade search performance genetic programming
we discuss implications holtes recentlypublished article demonstrated commonly used data simple classification rules almost accurate decision trees produced quinlans c we consider particular significance holtes results future topdown induction decision trees to extent holte questioned sense research multilevel decision tree learning we go detail parts holtes study we try put results perspective we argue absolute terms small difference accuracy r c witnessed holte still significant we claim c possesses additional accuracyrelated advantages r in addition discuss representativeness databases used holte we compare empirically optimal accuracies multilevel onelevel decision trees observe significant differences we point several deficien cies limitedcomplexity classifiers
we describe approach graphemetophoneme conversion languageindependent dataoriented given set examples spelling words associated phonetic representation language graphemetophoneme conversion system automatically produced language takes input spelling words produces output phonetic transcription according rules implicit training data we describe design system compare performance knowledgebased alternative dataoriented approaches
no finite sample sufficient determine density therefore entropy signal directly some assumption either functional form density smoothness necessary both amount prior space possible density functions by far common approach assume density parametric form by contrast derive differential learning rule called emma optimizes entropy way kernel density estimation entropy derivative calculated sampling density estimate the resulting parameter update rule surprisingly simple efficient we show emma used detect correct corruption magnetic resonance images mri this application beyond scope existing parametric entropy models
a satisficing search problem consists set probabilistic experiments performed order without repetitions satisfying configuration successes failures reached the cost performing experiments depends order chosen earlier work concentrated finding optimal search strategies special cases model search trees andor graphs cost function success probabilities experiments given in contrast study complexity learning approximately optimal search strategy success probabilities known outset working fully general model show n number unknown probabilities c maximum cost performing experiments
we present method calculating phase diagrams highdimensional variant selforganizing map som the method requires ansatz tesselation data space induced map explicit state map using method analyze two recently proposed models development orientation ocular dominance column maps the phase transition condition orientation map turns different form corresponding lowdimensional map
we study process multiagent reinforcement learning context load balancing distributed system without use either central coordination explicit communication we first define precise framework study adaptive load balancing important features stochastic nature purely local information available individual agents given framework show illuminating results interplay basic adaptive behavior parameters effect system efficiency we investigate properties adaptive load balancing heterogeneous populations address issue exploration vs exploitation context finally show naive use communication may improve might even harm system efficiency
brendan j frey geoffrey e hinton efficient stochastic source coding application bayesian network source model the computer journal in paper introduce new algorithm called bitsback coding makes stochastic source codes efficient for given onetomany source code show algorithm actually efficient algorithm always picks shortest codeword optimal efficiency achieved codewords chosen according boltzmann distribution based codeword lengths it turns commonly used technique determining parameters maximum likelihood estimation actually minimizes bitsback coding cost codewords chosen according boltzmann distribution a tractable approximation maximum likelihood estimation generalized expectation maximization algorithm minimizes bitsback coding cost after presenting binary bayesian network model assigns exponentially many codewords symbol show tractable approximation boltzmann distribution used bitsback coding we illustrate performance bitsback coding using using nonsynthetic data binary bayesian network source model produces possible codewords input symbol the rate bitsback coding nearly one half obtained picking shortest codeword symbol
agents learn agents exploit information possess distinct advantage competitive situations games provide stylized adversarial environments study agent learning strategies researchers developed game playing programs learn play better experience we developed learning program learn play better learns identify exploit weaknesses particular opponent repeatedly playing several games we propose scheme learning opponent action probabilities utility maximization framework exploits learned opponent model we show proposed expected utility maximization strategy generalizes traditional maximin strategy allows players benefit taking calculated risks avoided maximin strategy experiments popular board game connect show learning player consistently outperforms nonlearning player pitted another automated player using weaker heuristic though proposed mechanism improve skill level computer player improve ability play effectively weaker opponent
many real world learning problems best characterized interaction multiple independent causes factors discovering causal structure data focus paper based zemel hintons cooperative vector quantizer cvq architecture unsupervised learning algorithm derived expectationmaximization em framework due combinatorial nature data generation process exact estep computationally intractable two alternative methods computing estep proposed gibbs sampling meanfield approximation promising empirical results presented
this paper deals problem blind identification source separation consists estimation mixing matrix andor separation mixture stochastically independent sources without priori knowledge mixing matrix the method propose estimates mixture matrix recurrent inputoutput io identification using inputs nonlinear transformation estimated sources herein nonlinear transformation distortion consists constraining modulus inputs ioidentification device constant in contrast existing approaches covariance additive noise need modeled estimated regular parameter needed the proposed approach implemented using multilayer neural networks order improve performance separation new associated online unsupervised adaptive learning rules also developed the effectiveness proposed method illustrated computer simulations
source separation consists recovering set n independent signals n observed instantaneous mixtures signals possibly corrupted additive noise many source separation algorithms use second order information whitening operation reduces non trivial part separation determining unitary matrix most show kind invariance property exploited predict general results performance our first contribution exhibit lower bound performance terms accuracy separation this bound independent algorithm iid case distribution source signals second show performance invariant algorithms depends mixing matrix noise level specific way a consequence low noise levels performance depend mixture distribution sources via function characteristic given source separation algorithm
in paper neural network approach reconstruction natural highly correlated images linear additive mixture proposed a multilayer architecture local online learning rules developed solve problem blind separation sources the main motivation using multilayer network instead singlelayer one improve performance robustness separation applying simple local learning rule biologically plausible moreover architecture onchip learning relatively easy implementable using vlsi electronic circuits furthermore enables extraction source signals sequentially one starting strongest signal finishing weakest one the experimental part focuses separating highly correlated human faces mixture additive noise unknown number sources
we study online learning algorithms predict combining predictions several subordinate prediction algorithms sometimes called experts these simple algorithms belong multiplicative weights family algorithms the performance algorithms degrades logarithmically number experts making particularly useful applications number experts large however applications text categorization often natural experts abstain making predictions instances we show transform algorithms assume experts always awake algorithms require assumption we also show derive corresponding loss bounds our method general applied large family online learning algorithms we also give applications various prediction models including decision graphs switching experts
when dealing classification problems current ilp systems often lag behind stateoftheart attributional learners part blame ascribed much larger hypothesis space therefore thoroughly explored however sometimes due fact ilp systems take account probabilistic aspects hypotheses classifying unseen examples this paper proposes we developed naive bayesian classifier within ilpr first order learner the learner uses clever relief based heuristic able detect strong dependencies within literal space dependencies exist we conducted series experiments artificial realworld data sets the results show combination ilpr together naive bayesian classifier sometimes significantly improves classification unseen instances measured classification accuracy average information score
process simulation emerged valuable tool process design analysis operation in work extend capabilities iterated linear programming lp dealing problems encountered dynamic nonsmooth process simulation a previously developed lp method refined addition new descent strategy combines line search trust region approach this adds stability efficiency method the lp method advantage naturally dealing profile bounds well this demonstrated avoid computational difficulties arise iterates going physically unrealistic regions a new method treatment discontinuities occurring dynamic simulation problems also presented paper the method ensures event occurred within time interval consideration detected one event occurs detected one indeed earliest one a specific class implicitly discontinuous process simulation problems phase equilibrium calculations also looked a new formulation introduced solve multiphase problems fl to correspondence addressed emailbieglercmuedu
a frequently observed difficulty application genetic algorithms domain optimization arises premature convergence in order preserve genotype diversity develop new model autoadaptive behavior individuals in model population member active individual assumes sociallike behavior patterns different individuals living population assume different patterns by moving hierarchy social states individuals change behavior changes social state controlled arguments plausibility these arguments implemented rule set massivelyparallel genetic algorithm computational experiments largescale job shop benchmark problems show results new approach dominate ordinary genetic algorithm significantly
proben collection problems neural network learning realm pattern classification function approximation plus set rules conventions carrying benchmark tests similar problems proben contains data sets different domains all datasets represent realistic problems could called diagnosis tasks one consist real world data the datasets presented simple format using attribute representation directly used neural network training along datasets proben defines set rules conduct document neural network benchmarking the purpose problem rule collection give researchers easy access data evaluation algorithms networks make direct comparison published results feasible this report describes datasets benchmarking rules it also gives basic performance measures indicating difficulty various problems these measures used baselines comparison
this paper presents neurochess program learns play chess final outcome games neurochess learns chess board evaluation functions represented artificial neural networks it integrates inductive neural network learning temporal differencing variant explanationbased learning performance results illustrate strengths weaknesses approach
some important factors play major role determining performances cbr casebased reasoning system complexity accuracy retrieval phase both flat memory inductive approaches suffer serious drawbacks in first approach search time increases dealing large scale memory base second one modification case memory becomes complex sophisticated architecture in paper show construct simple efficient indexing system structure the idea construct case hierarchy two levels memory lower level contains cases organised groups similar cases upper level contains prototypes prototype represents one group cases this smaller memory used retrieval phase prototype construction achieved means incremental prototypebased nn neural network we show mode cbrnn coupling preprocessing one neural network serves indexing system
developing ability recognize landmark visual image robots current location fundamental problem robotics we consider problem paclearning concept class geometric patterns target geometric pattern configuration k points real line each instance configuration n points real line labeled according whether visually resembles target pattern to capture notion visual resemblance use hausdorff metric informally two geometric patterns p q resemble hausdorff metric every point one pattern close point pattern we relate concept class geometric patterns landmark recognition problem present polynomialtime algorithm paclearns class onedimensional geometric patterns we also present experimental results algorithm performs
the concept measure functions generalization performance suggested this concept provides alternative way selecting evaluating learned models classifiers in addition makes possible state learning problem computational problem the known prior metaknowledge problem domain captured measure function possible combination training set classifier assigns value describing good classifier the computational problem find classifier maximizing measure function we argue measure functions great value practical applications besides tool model selection force us make explicit relevant prior knowledge learning problem hand ii provide deeper understanding existing algorithms iii help us construction problemspecific algorithms we illustrate last point suggesting novel algorithm based incremental search classifier optimizes given measure function
recurrent attractor networks offer many advantages feedforward networks modeling psychological phenomena their dynamic nature allows capture time course cognitive processing learned weights may often easily interpreted soft constraints representational components perhaps significant feature networks however ability facilitate generalization enforcing well formedness constraints intermediate output representations attractor networks learn systematic regularities well formed representations exposure small number examples said possess articulated attractors this paper investigates conditions articulated attractors arise recurrent networks trained using variants backpropagation the results computational experiments demonstrate structured attractors spontaneously appear emergence systematicity appropriate error signal presented directly recurrent processing elements we show however distal error signals backpropagated intervening weights pose serious problems networks kind we present simulation results discuss reasons difficulty suggest directions future attempts surmount
induced decision trees extensivelyresearched solution classification tasks for many practical tasks trees produced treegeneration algorithms comprehensible users due size complexity although many tree induction algorithms shown produce simpler comprehensible trees data structures derived trees good classification accuracy tree simplification usually secondary concern relative accuracy attempt made survey literature perspective simplification we present framework organizes approaches tree simplification summarize critique approaches within framework the purpose survey provide researchers practitioners concise overview treesimplification approaches insight relative capabilities in final discussion briefly describe empirical findings discuss application tree induction algorithms case retrieval casebased reasoning systems
in paper propose monitor markov chain sampler using cusum path plot chosen dimensional summary statistic we argue cusum path plot bring effectively sequential plot aspects markov sampler tell user quickly slowly sampler moving around sample space direction summary statistic the proposal illustrated four examples represent situations cusum path plot works well well moreover rigorous analysis given one examples we conclude cusum path plot effective tool convergence diagnostics markov sampler comparing different markov samplers
this paper gives precise easy compute bounds convergence time gibbs sampler used bayesian image reconstruction for sampling gibbs distribution without presence external field bounds n number pixels obtained proportionality constant easy calculate some key words bayesian image restoration convergence gibbs sampler ising model markov chain monte carlo
c flmit media lab perceptual computing learning common sense technical report nov revised jun abstract we present methods coupling hidden markov models hmms model systems multiple interacting processes the resulting models multiple state variables temporally coupled via matrices conditional probabilities we introduce deterministic ot cn approximation maximum posterior map state estimation enables fast classification parameter estimation via expectation maximization an nheads dynamic programming algorithm samples highest probability paths compact state trellis minimizing upper bound cross entropy full combinatoric dynamic programming problem the complexity ot cn c chains n states apiece observing t data points compared ot n c naive cartesian product exact state clustering stochastic monte carlo methods applied inference problem in several experiments examining training time model likelihoods classification accuracy robustness initial conditions coupled hmms compared favorably conventional hmms energybased approaches coupled inference chains we demonstrate compare algorithms synthetic real data including interpretation video
summary the paper describes bayesian analysis agricultural field experiments topic received little previous attention despite vast frequentist literature adoption bayesian paradigm simplifies interpretation results especially ranking selection also complex formulations analyzed comparative ease using markov chain monte carlo methods a key ingredient approach need spatial representations unobserved fertility patterns this discussed detail problems caused outliers jumps fertility tackled via hierarchicalt formulations may find use contexts the paper includes three analyses variety trials yield one example involving binary data none entirely straightforward some comparisons frequentist analyses made the datasets available httpwwwstatdukeeduhigdontrialsdatahtml
we show paper continuous state space markov chains rigorously discretized finite markov chains the idea subsample continuous chain renewal times related small sets control discretization once finite markov chain derived mcmc output general convergence properties finite state spaces exploited convergence assessment several directions our choice based divergence criterion derived kemeny snell first evaluated parallel chains stopping time implemented efficiently two parallel chains using birkhoffs pointwise ergodic theorem stopping rules the performance criterion illustrated three standard examples
markov chain monte carlo mcmc samplers proved remarkably popular tools bayesian computation however problems arise application density interest high dimensional strongly correlated in circumstances sampler may slow traverse state space mixing poor in article offer partial solution problem the state space markov chain augmented accommodate multiple chains parallel updates individual chains based around genetic style crossover operator acting parent states drawn population chains this process makes efficient use gradient information implicitly encoded within distribution states across population empirical studies support claim crossover operator acting parallel population chains improves mixing this illustrated example sampling high dimensional posterior probability density complex predictive model by adopting latent variable approach methodology extended deal variable selection model averaging high dimensions this illustrated example knot selection spline interpolant
mit computational cognitive science technical report abstract we describe variational approximation methods efficient probabilistic reasoning applying methods problem diagnostic inference qmrdt database the qmrdt database largescale belief network based statistical expert knowledge internal medicine the size complexity network render exact probabilistic diagnosis infeasible small set cases this hindered development qmr dt network practical diagnostic tool hindered researchers exploring critiquing diagnostic behavior qmr in paper describe variational approximation methods applied qmr network resulting fast diagnostic inference we evaluate accuracy methods set standard diagnostic cases compare stochastic sampling methods
the effects neural networks topology performance well known yet question finding optimal configurations automatically remains largely open this paper proposes solution problem rbf networks a self optimising approach driven evolutionary strategy taken the algorithm uses output information computationally efficient approximation rbf networks optimise kmeans clustering process coevolving two determinant parameters networks layout number centroids centroids positions empirical results demonstrate promise
this paper describes hybrid methodology integrates genetic algorithms decision tree learning order evolve useful subsets discriminatory features recognizing complex visual concepts a genetic algorithm ga used search space possible subsets large set candidate discrimination features candidate feature subsets evaluated using c decisiontree learning algorithm produce decision tree based given features using limited amount training data the classification performance resulting decision tree unseen testing data used fitness underlying feature subset experimental results presented show increasing amount learning significantly improves feature set evolution difficult visual recognition problems involving satellite facial image data in addition also report extent subtle aspects baldwin effect exhibited system
in paper examine behavior humancomputer system crisis response as one instance crisis management describe task responding spills fires involving hazardous materials we describe inca intelligent assistant planning scheduling domain relation human users we focus incas strategy retrieving case case library seeding initial schedule helping user adapt seed we also present three hypotheses behavior mixedinitiative system experiments designed test the results suggest approach leads faster response development usergenerated automaticallygenerated schedules without sacrificing solution quality
given adequate simulation model task environment payoff function measures quality partially successful plans competitionbased heuristics genetic algorithms develop high performance reactive rules interesting sequential decision tasks we previously described implemented system called samuel learning reactive plans shown system successfully learn rules laboratory scale tactical problem in paper describe method deriving explanations justify success empirically derived rule sets the method consists inferring plausible subgoals explaining reactive rules trigger sequence actions ie stra tegy satisfy subgoals
machine learning valuable tool improving flexibility efficiency robot applications many approaches applying machine learning robotics known some approaches enhance robots highlevel processing planning capabilities other approaches enhance lowlevel processing control basic actions in contrast approach presented paper uses machine learning enhancing link lowlevel representations sensing action highlevel representation planning the aim facilitate communication robot human user a hierarchy concepts learned route records mobile robot perception action combined every level ie concepts perceptually anchored the relational learning algorithm grdt developed completely searches hypothesis space restricted rule schemata user defines terms grammars
we motivate use convergence diagnostic techniques markov chain monte carlo algorithms review various methods proposed mcmc literature a common notation established method discussed particular emphasis implementational issues possible extensions the methods compared terms interpretability applicability recommendations provided particular classes problems
for target tracking task handheld camera anthropomorphic oscarrobot manipulator track object moves arbitrarily table the desired camerajoint mapping approximated feedforward neural network through use time derivatives position object manipulator controller inherently predict next position moving target object in paper several anticipative controllers described successfully applied track moving object
covariance information help algorithm search predictive causal models estimate strengths causal relationships this information discarded conditional independence constraints identified usual contemporary causal induction algorithms our fbd algorithm combines covariance information effective heuristic build predictive causal models we demonstrate fbd accurate efficient in one experiment assess fbds ability find best predictors variables another compare performance using many measures pearl vermas ic algorithm and although fbd based multiple linear regression cite evidence performs well problems difficult regression algorithms
the problem learning decision rules sequential tasks addressed focusing problem learning tactical decision rules simple flight simulator the learning method relies notion competition employs genetic algorithms search space decision policies several experiments presented address issues arising differences simulation model learning occurs target environment decision rules ultimately tested
the inductive learning problem consists learning concept given examples nonexamples concept to perform learning task inductive learning algorithms bias learning method here discuss biasing learning method use previously learned concepts domain these learned concepts highlight useful information concepts domain we describe transference bias present mfocl horn clause relational learning algorithm utilizes bias learn multiple concepts we provide preliminary empirical evaluation show effects biasing previous information noisefree noisy data
choosing architecture neural network one important problems making neural networks practically useful accounts applications usually sweep details carpet how many hidden units needed should weight decay used much what type output units chosen and we address issues within framework statistical theory model this paper principally concerned architecture selection issues feedforward neural networks also known multilayer perceptrons many issues arise selecting radial basis function networks recurrent networks widely these problems occur much wider context within statistics applied statisticians selecting combining models decades two recent discussions references discuss neural networks statistical perspective choice provides number workable approximate answers
we propose modeltheoretic definition causation show contrary common folklore genuine causal influences distinguished spurious covariations following standard norms inductive reasoning we also establish complete characterization conditions distinction possible finally provide prooftheoretical procedure inductive causation show large class data structures effective algorithms exist uncover direction causal influences defined
this study deals alltoall broadcast cns we determine lower bound run time present algorithm meeting bound since study points bottleneck network interface also analyze performance alternative interface designs our analyses based run time model network
automated decision making often complicated complexity knowledge involved much complexity arises contextsensitive variations underlying phenomena we propose framework representing descriptive contextsensitive knowledge our approach attempts integrate categorical uncertain knowledge network formalism this paper outlines basic representation constructs examines expressiveness efficiency discusses potential applications framework
we discuss number methods estimating standard error predicted values multilayer perceptron these methods include delta method based hessian bootstrap estimators sandwich estimator the methods described compared number examples we find bootstrap methods perform best partly capture variability due choice starting weights
discrete mixtures normal distributions widely used modeling amplitude fluctuations electrical potentials synapses human animal nervous systems the usual framework independent data values j arising j j x n j means j come discrete prior g unknown x n j observed x j j n gaussian noise terms a practically important development associated statistical methods issue nonnormality noise terms often norm rather exception neurological context we recently developed models based convolutions dirichlet process mixtures problems explicitly model noise data values x j arising dirichlet process mixture normals addition modeling location prior g dirichlet process this induces dirichlet mixture mixtures normals whose analysis may developed using gibbs sampling techniques we discuss models analysis illustrate context neurological response analysis
neural controllers able position handheld camera dof anthropomorphic oscarrobot manipulator object arbitrary placed table the desired camerajoint mapping approximated feedforward neural networks however object moving manipulator lags behind required time preprocess visual information move manipulator through use time derivatives position object manipulator controller inherently predict next position object in paper several predictive controllers proposed successfully applied track moving object
this paper overviews aa adaptive algorithm model asocs adaptive self organizing concurrent systems approach it also presents promising empirical generalization results aa actual data aa topologically dynamic network grows fit problem learned aa generalizes selforganizing fashion network seeks find features discriminate concepts convergence training set guaranteed bounded linearly time
this communication deals source separation problem consists separation noisy mixture independent sources without priori knowledge mixture coefficients in paper consider maximum likelihood ml approach discrete source signals known probability distributions an important feature ml approach gaussian noise covariance matrix additive noise treated parameter hence necessary know model spatial structure noise another striking feature offered case discrete sources mild assumptions possible separate sources sensors in paper consider maximization likelihood via expectationmaximization em algorithm
if robust statistical model developed classify health system wellknown taylor series approximation technique forms basis diagnosticrecovery procedure initiated systems health degrades fails altogether this procedure determines ranked set probable causes degraded health state used prioritized checklist isolating system anomalies quantifying corrective action the diagnosticrecovery procedure applicable classifier known robust applied neural network traditional parametric pattern classifiers generated supervised learning procedure empirical riskbenefit measure optimized we describe procedure mathematically demonstrate ability detect diagnose causes faults nasas deep space communications complex goldstone california
case combination difficult problem case based reasoning subcases often exhibit conflicts merged together in previous work formalized case combination representing case constraint satisfaction problem used minimum conflicts algorithm systematically synthesize global solution however also found instances problem minimum conflicts algorithm perform case combination efficiently in paper describe situations initially retrieved cases easily adaptable propose method improve case adaptability genetic algorithm we introduce fitness function maintains much retrieved case information possible also perturbing subsolution allow subsequent case combination proceed efficiently
the dynamic constraint satisfaction problem dcsp formalism gaining attention valuable often necessary extension static csp framework dynamic constraint satisfaction enables csp techniques applied extensively since applied domains set constraints variables involved problem evolves time at time casebased reasoning cbr community working techniques reuse existing solutions solving new problems we observed dynamic constraint satisfaction matches closely casebased reasoning process case adaptation these observations emerged previous work combining cbr csp achieve constraintbased adaptation this paper summarizes previous results describes similarity challenges facing dcsp case adaptation shows csp cbr together begin address chal lenges
prior knowledge bias regarding concept speed task learning probably approximately correct pac learning mathematical model concept learning used quantify speed due different forms bias learning thus far pac learning mostly used analyze syntactic bias limiting concepts conjunctions boolean prepositions this paper demonstrates pac learning also used analyze semantic bias domain theory concept learned the key idea view hypothesis space pac learning consistent prior knowledge syntactic semantic in particular paper presents pac analysis determinations type relevance knowledge the results analysis reveal crisp distinctions relations among different determinations illustrate usefulness analysis based pac model
computational models natural systems often contain free parameters must set optimize predictive accuracy models this process called calibrationcan viewed form supervised learning presence prior knowledge in view fixed aspects model constitute prior knowledge goal learn values free parameters we report series attempts learn parameter values global vegetation model called mapss mapped atmosphereplantsoil system developed collaborator ron neilson standard machine learning methods work mapss constraints introduced structure model create difficult nonlinear optimization problem we developed new divideandconquer approach subsets parameters calibrated others held constant this approach succeeds possible select training examples exercise portions model
the paper considers situation learners testing set contains close approximations cases appear training set such cases considered virtual seens since approximately seen learner generalisation measures take account frequency virtual seens may misleading the paper shows nn algorithm used derive normalising baseline generalisation statistics the normalisation process demonstrated though application holtes study generalisation performance r algorithm tested c commonly used datasets
initial results abstract conversational casebased reasoning cbr systems incrementally extract query description userdirected conversation advertised ease use however designing large case libraries good performance ie precision querying efficiency difficult cbr vendors provide guidelines designing libraries manually guidelines difficult apply we describe automated inductive approach revises conversational case libraries increase conformance design guidelines revision increased performance three conversational case libraries
diagnosis process identifying disorders machine patient considering history symptoms signs starting possible initial information new information requested sequential manner diagnosis made precise it thus missing data problem since everything known we model joint probability distribution data case database mixture models model parameters estimated em algorithm gives additional benefit missing data database also handled correctly request new information refine diagnosis performed using maximum utility principle decision theory since system based machine learning domain independent an example using heart disease database presented
we give example neural net without hidden layers sigmoid transfer function together training set binary vectors sum squared errors regarded function weights local minimum global minimum the example consists set training instances four weights threshold learnt we know substantially smaller binary examples exist
the multiple extension problem arises default theory use different subsets defaults propose different mutually incompatible answers queries this paper presents algorithm uses set observations learn credulous version default theory essentially optimally accurate in detail associate given default theory set related credulous theories r fr g r uses total ordering defaults determine single answer return query our goal select credulous theory highest expected accuracy r expected accuracy probability answer produces query correspond correctly world unfortunately theorys expected accuracy depends distribution queries usually known moreover task identifying optimal r opt r even given distribution information intractable this paper presents method optacc sidesteps problems using set samples estimate unknown distribution hillclimbing local optimum in particular given parameters ffi gt optacc produces r oa r whose expected accuracy probability least ffi within local optimum appeared ecai workshop theoretical foundations knowledge representation reasoning
compression information important concept theory learning we argue hypothesis inherent compression pressure towards short elegant general solutions genetic programming system variable length evolutionary algorithms this pressure becomes visible size complexity solutions measured without noneffective code segments called introns the built parsimony pressure effects complex fitness functions crossover probability generality maximum depth length solutions explicit parsimony granularity fitness function initialization depth length modularization some effects positive negative in work provide basis analysis effects suggestions overcome negative implications order obtain balance needed successful evolution an empirical investigation supports hypothesis also presented
wilsons recent xcs classifier system forms complete mappings payoff environment reinforcement learning tradition thanks accuracy based fitness according wilsons generalization hypothesis xcs tendency towards generalization with xcs optimality hypothesis i suggest xcs systems evolve optimal populations representations populations accurately map inputaction pairs payoff predictions using smallest possible set nonoverlapping classifiers the ability xcs evolve optimal populations boolean multiplexer problems demonstrated using condensation technique evolutionary search suspended setting crossover mutation rates zero condensation automatically triggered selfmonitoring performance statistics entire learning process terminated autotermination combined techniques allow classifier system evolve optimal representations boolean functions without form supervision
current rule induction systems eg cn typically rely separate conquer strategy learning rule stilluncovered examples this results dwindling number examples available learning successive rules adversely affecting systems accuracy an alternative learn rules simultaneously using entire training set this approach implemented rise system empirical comparison rise cn suggests conquering without separating performs similarly counterpart simple domains achieves increasingly substantial gains accuracy domain difficulty grows
a genetic programming method investigated optimizing architecture connection weights multilayer feedforward neural networks the genotype network represented tree whose depth width dynamically adapted particular application specifically defined genetic operators the weights trained nextascent hillclimbing search a new fitness function proposed quantifies principle occams razor it makes optimal tradeoff error fitting ability parsimony network we discuss results two problems differing complexity study convergence scaling properties algorithm
the performance neural network categorizes facial expressions compared human subjects set experiments using interpolated imagery the experiments human subjects neural networks make use interpolations facial expressions pictures facial affect database ekman friesen the difference materials used human subjects experiments young et al materials manner interpolated images constructed imagequality morphs versus pixel averages nevertheless neural network accurately captures categorical nature human responses showing sharp transitions labeling images along interpolated sequence crucially demonstration categorical perception harnad model shows highest discrimination transition images crossover point the model also captures shape reaction time curves human subjects along sequences finally network matches human subjects judgements expressions mixed images the main failing model intrusions neutral responses transitions seen human subjects we attribute difference difference pixel average stimuli image quality morph stimuli these results show simple neural network classifier access biological constraints presumably imposed human emotion processor whose access surrounding culture category labels placed american subjects facial expressions nevertheless simulate fairly well human responses emotional expressions
the article hand discusses tool automatic generation structured models complex dynamic processes means genetic programming in contrast techniques use genetic programming find appropriate arithmetic expression order describe inputoutput behaviour process tool based block oriented approach transparent description signal paths a short survey techniques computer based system identification given basic concept smog structured model generator described furthermore latest extensions system presented detail including automatically defined submodels quali tative fitness criteria
we examine role hyperplane ranking search performed simple genetic algorithm we also develop metric measuring degree ranking exists respect static measurements taken directly function well measurement dynamic ranking hyperplanes genetic search we show degree dynamic ranking induced simple genetic algorithm highly correlated degree static ranking inherent function especially initial genera tions search
genetic algorithms rely two genetic operators crossover mutation although exists large body conventional wisdom concerning roles crossover mutation roles captured theoretical fashion for example never theoretically shown mutation sense less powerful crossover vice versa this paper provides answers questions theoretically demonstrating important characteristics operator captured
in recent paper friedman geiger goldszmidt introduced classifier based bayesian networks called tree augmented naive bayes tan outperforms naive bayes performs competitively c stateoftheart methods this classifier several advantages including robustness polynomial computational complexity one limitation tan classifier applies discrete attributes thus continuous attributes must prediscretized in paper extend tan deal continuous attributes directly via parametric eg gaussians semiparametric eg mixture gaussians conditional probabilities the result classifier represent combine discrete continuous attributes in addition propose new method takes advantage modeling language bayesian networks order represent attributes discrete continuous form simultaneously use versions classification this automates process deciding form attribute relevant classification task it also avoids commitment either discretized semiparametric form since different attributes may correlate better one version our empirical results show latter method usually achieves classification performance good better either purely discrete purely continuous tan models
this paper considers problem representing complex systems evolve stochastically time dynamic bayesian networks provide compact representation stochastic processes unfortunately often unwieldy since explicitly model complex organizational structure many real life systems fact processes typically composed several interacting subprocesses turn decomposed we propose hierarchically structured representation language extends dynamic bayesian networks objectoriented bayesian network framework show language allows us describe systems natural modular way our language supports natural representation certain system characteristics hard capture using traditional frameworks for example allows us represent systems processes evolve different rate others systems processes interact intermittently we provide simple inference mechanism representation via translation bayesian networks suggest ways inference algorithm exploit additional structure encoded representation
it often difficult predict optimal neural network size particular application constructive destructive methods add subtract neurons layers connections etc might offer solution problem we prove one method recurrent cascade correlation due topology fundamental limitations representation thus learning capabilities it represent monotone ie sigmoid hardthreshold activation functions certain finite state automata we give preliminary approach get around limitations devising simple constructive training method adds neurons training still preserving powerful fullyrecurrent structure we illustrate approach simulations learn many examples regular grammars
indexing cases important topic memorybased reasoningmbr one key problem assign weights attributes cases although several weighting methods proposed methods handle numeric attributes directly necessary discretize numeric values classification furthermore existing methods theoretical background little said optimality we propose new weighting method based statistical technique called quantification method ii it handle numeric symbolic attributes framework generated attribute weights optimal sense maximize ratio variance classes variance cases experiments several benchmark tests show many cases method obtains higher accuracies weighting methods the results also indicate distinguish relevant attributes irrelevant ones tolerate noisy data
a general result stabilization linear systems using bounded controls abstract we present two constructions controllers globally stabilize linear systems subject control saturation we allow essentially arbitrary saturation functions the conditions imposed system obvious necessary ones namely eigenvalues uncontrolled system positive real part standard stabilizability rank condition hold one constructions terms neuralnetwork type onehidden layer architecture one terms cascades linear maps saturations
this paper proposes classification scheme based integration multiple ensembles anns it demonstrated classification problem seismic signals natural earthquakes must distinguished seismic signals artificial explosions a redundant classification environment consists several ensembles neural networks created trained bootstrap sample sets using various data representations architectures the anns within ensembles aggregated bagging ensembles integrated nonlinearly signal adaptive manner using posterior confidence measure based agreement variance within ensembles the proposed integrated classification machine achieved correct classifications seismic test data cross validation evaluations comparisons indicate integration collection anns ensembles robust way handling high dimensional problems complex nonstationary signal space current seismic classification problem
this first draft chapter bayesian biostatistics edited donald a berry darlene k strangl adrian e raftery professor statistics sociology department statistics gn university washington seattle wa usa sylvia richardson directeur de recherche inserm unite avenue paul vaillant couturier villejuif cedex france rafterys research supported onr contract nj ministere de la recherche et de lespace paris universite de paris vi inria rocquencourt france raftery thanks latter two institutions paul deheuvels gilles celeux hearty hospitality paris sabbatical part chapter written the authors grateful christine montfort excellent research assistance mariette gerber michel chavance david madigan helpful discussions
productionmanufacturing scheduling typically involves acquisition user optimization preferences the illstructuredness problem space desired objectives make practical scheduling problems difficult formalize costly solve especially problem configurations user optimization preferences change time this paper advocates incremental revision framework improving schedule quality incorporating user dynamically changing preferences casebased reasoning our implemented system called cabins records situationdependent tradeoffs consequences result schedule revision guide schedule improvement the preliminary experimental results show cabins able effectively capture user static dynamic preferences known system exist implicitly extensional manner case base
realization autonomous behavior mobile robots using fuzzy logic control requires formulation rules collectively responsible necessary levels intelligence such collection rules conveniently decomposed efficiently implemented hierarchy fuzzybehaviors this article describes done using behaviorbased architecture a behavior hierarchy mechanisms control decisionmaking described in addition approach behavior coordination described emphasis evolution fuzzy coordination rules using genetic programming gp paradigm both conventional gp steadystate gp applied evolve fuzzybehavior sensorbased goalseeking the usefulness behavior hierarchy partial design gp evident performance results simulated autonomous navigation
we present distribution model binary vectors called influence combination model show model used basis unsupervised learning algorithms feature selection the model closely related harmonium model defined smolensky rmch in first part paper analyze properties distribution representation scheme we show arbitrary distributions binary vectors approximated combination model we show weight vectors model interpreted high order correlation patterns among input bits we compare combination model mixture model principle component analysis in second part paper present two algorithms learning combination model examples the first algorithm based gradient ascent here give closed form gradient significantly easier compute corresponding gradient general boltzmann machine the second learning algorithm greedy method creates hidden units computes weights one time this method variant projection pursuit density estimation in third part paper give experimental results learning methods synthetic data natural data handwritten digit images
complex group behavior arises social insects colonies integration actions simple redundant individual insects adler gordon oster wilson furthermore colony act information center expedite foraging brown we apply lessons natural systems model collective action memory computational agent society collective action expedite search combinatorial optimization problems dorigo et al collective memory improve learning multiagent systems garland alterman our collective adaptation integrates simplicity collective action pattern detection collective memory significantly improve gathering processing knowledge as test role society information center examine ability society distribute task allocation without omnipotent centralized control
we study annealed theories learning boolean functions using concept class finite cardinality the naive annealed theory used derive universal learning curve bound zero temperature learning similar inverse square root bound vapnikchervonenkis theory tighter nonuniversal learning curve bounds also derived a refined annealed theory leads still tighter bounds cases similar results previously obtained using onestep replica symmetry breaking
this article describes numerical method may used efficiently locate track underwater sonar targets nearfield bearing range estimation case large passive arrays the approach used requirement priori knowledge source uses limited information receiver array shape the role sensor position uncertainty consequence targets always nearfield analysed problems associated manipulation large matrices inherent conventional eigenvalue type algorithms noted a simpler numerical approach presented reduces problem search optimization when using method location target corresponds finding position maximum weighted sum output sensors since search procedure dealt using modern stochastic optimization methods genetic algorithm operational requirement acceptable accuracy achieved real time usually met the array studied consists elements positioned along flexible cable towed behind ship sensors giving effective aperture for long array far field assumption used beamforming algorithms longer appropriate the waves emitted targets considered curved rather plane it shown simulated data significant noise
this paper introduces new type intelligent agent called constructive inductionbased learning agent cila this agent differs adaptive agents ability learn assist user task also incrementally adapt knowledge representation space better fit given learning task the agents ability autonomously make problemoriented modifications originally given representation space due constructive induction ci learning method selective induction si learning methods agents based methods rely good representation space a good representation space misclassification noise intercorrelated attributes irrelevant attributes our proposed cila methods overcoming problems in agent domains poor representations cibased learning agent learn accurate rules useful sibased learning agent this paper gives architecture cibased learning agent gives empirical comparison ci si set six abstract domains involving dnftype disjunctive normal form descriptions
we propose method decreasing computational complexity selforganising maps the method uses partitioning neurons disjoint clusters teaching neurons occurs clusterbasis instead neuronbasis for teaching nneuron network n samples computational complexity decreases on n on log n furthermore introduce measure amount order selforganising map show introduced algorithm behaves well original algorithm
inductive learning relational domains shown intractable general many approaches task suggested nevertheless way restrict hypothesis space searched they roughly divided two groups datadriven restriction encoded algorithm modelbased restrictions made less explicit form declarative bias this paper describes incy inductive learner seeks combine aspects approaches incy initially datadriven using examples background knowledge put forth specialize hypotheses based connectivity data hand it modeldriven hypotheses abstracted rule models used control decisions datadriven phase modelguided induction key words inductive learning relational domains cooperation datadriven modelguided methods implicit declarative bias
the problem learning decision rules sequential tasks addressed focusing problem learning tactical plans simple flight simulator plane must avoid missile the learning method relies notion competition employs genetic algorithms search space decision policies experiments presented address issues arising differences simulation model learning occurs target environment decision rules ultimately tested specifically either model target environment may contain noise these experiments examine effect learning tactical plans without noise testing plans noisy environment effect learning plans noisy simulator testing plans noisefree environment empirical results show best result obtained training model closely matches target environment using training environment noisy target environment better using using training environment less noise target environment
navigation obstacles mine fields important capability autonomous underwater vehicles one way produce robust behavior perform projective planning however realtime performance critical requirement navigation what needed truly autonomous vehicle robust reactive rules perform well wide variety situations also achieve realtime performance in work samuel learning system based genetic algorithms used learn highperformance reactive strategies navigation collision avoidance
in paper introduce investigate mathematically rigorous theory learning curves based ideas statistical mechanics the advantage theory wellestablished vapnikchervonenkis theory bounds considerably tighter many cases also reflective true behavior functional form learning curves this behavior often exhibit dramatic properties phase transitions well power law asymptotics explained vc theory the disadvantages theory application requires knowledge input distribution limited far finite cardinality function classes we illustrate results many concrete examples learning curve bounds derived theory
although considerable interest shown language inference automata induction using recurrent neural networks success models mostly limited regular languages we previously demonstrated neural network pushdown automaton nnpda model capable learning deterministic contextfree languages eg n b n parenthesis languages examples however learning task computationally intensive in paper discuss ways priori knowledge task data could used efficient learning we also observe knowledge often experimental prerequisite learning nontrivial languages eg n b n cb
connectionist learning procedures presented sigmoid noisyor varieties stochastic feedforward network these networks class belief networks used expert systems they represent probability distribution set visible variables using hidden variables express correlations conditional probability distributions exhibited stochastic simulation use tasks classification learning empirical data done via gradientascent method analogous used boltzmann machines due feedforward nature connections negative phase boltzmann machine learning unnecessary experimental results show result learning sigmoid feedforward network faster boltzmann machine these networks advantages boltzmann machines pattern classification decision making applications provide link work connectionist learning work representation expert knowledge
genetic programming gp uses variable size representations programs size becomes important interesting emergent property structures evolved gp the size programs controlling controlled factor gp search size influences efficiency search process related generality solutions this paper analyzes size generality issues standard gp gp using subroutines addresses question whether analysis help control search process we relate size generalization modularity issues programs evolved control agent dynamic nondeterministic environment exemplified pacman game
we present definition cause effect terms decisiontheoretic primitives thereby provide principled foundation causal reasoning our definition departs traditional view causation causal assertions may vary set decisions available we argue approach provides added clarity notion cause also paper examine encoding causal relationships directed acyclic graphs we describe special class influence diagrams canonical form show relationship pearls representation cause effect finally show canonical form facilitates counterfactual reasoning
fuzzy logic evolutionary computation proven convenient tools handling realworld uncertainty designing control systems respectively an approach presented combines attributes paradigms purpose developing intelligent control systems the potential genetic programming paradigm gp learning rules use fuzzy logic controllers flcs evaluated focussing problem discovering controller mobile robot path tracking performance results incomplete rulebases compare favorably complete flc designed usual trialanderror approach a constrained syntactic representation supported structurepreserving genetic operators also introduced
we review estimation interval censoring models including nonparametric estimation distribution function estimation regression models in nonparametric setting describe computational procedures asymptotic properties nonparametric maximum likelihood estimators in regression setting focus proportional hazards proportional odds accelerated failure time semiparametric regression models particular emphasis given calculation fisher information regression parameters we also discuss computation regression parameter estimators via profile likelihood maximization semiparametric likelihood distributional results maximum likelihood estimators estimation asymptotic variances some problems open questions also reviewed
genetic programming distinguished evolutionary algorithms uses tree representations variable size instead linear strings fixed length the flexible representation scheme important allows underlying structure data discovered automatically one primary difficulty however solutions may grow big without improvement generalization ability in paper investigate fundamental relationship performance complexity evolved structures the essence parsimony problem demonstrated empirically analyzing error landscapes programs evolved neural network synthesis we consider genetic programming statistical inference problem apply bayesian modelcomparison framework introduce class fitness functions error complexity terms an adaptive learning method presented automatically balances modelcomplexity factor evolve parsimonious programs without losing diversity population needed achieving desired training accuracy the effectiveness approach empirically shown induction sigmapi neural networks solving realworld medical diagnosis problem well benchmark tasks
dynamic probabilistic networks dpns useful tool modeling complex stochastic processes the simplest inference task dpns monitoring computing posterior distribution state variables time step given observations time recursive constantspace algorithms wellknown monitoring dpns models this paper concerned hindsight computing posterior distribution given past future observations hindsight essential subtask learning dpn models data existing algorithms hindsight dpns use osn space time n total length observation sequence s state space size time step they therefore impractical hindsight complex models long observation sequences this paper presents os log n space osn log n time hindsight algorithm we demonstrates effectiveness algorithm two realworld dpn learning problems we also discuss possibility osspace osn time algorithm
learning methods vary optimism pessimism regard informativeness learned knowledge pessimism implicit hypothesis testing wish draw cautious conclusions experimental evidence however paper demonstrates optimism utility derived rules may preferred bias learning systems we examine continuum naive pessimism naive optimism context decision tree learner prunes rules based stringent ie pessimistic weak ie optimistic tests significance our experimental results indicate cases optimism preferred particularly cases sparse training data high noise this work generalizes earlier findings fisher schlimmer schaffer discuss relevance unsupervised learning small disjuncts issues
we already shown extracting longterm dependencies sequential data difficult deterministic dynamical systems recurrent networks probabilistic models hidden markov models hmms inputoutput hidden markov models iohmms in practice avoid problem researchers used domain specific apriori knowledge give meaning hidden state variables representing past context in paper propose use general type apriori knowledge namely temporal dependencies structured hierarchically this implies longterm dependencies represented variables long time scale this principle applied recurrent network includes delays multiple time scales experiments confirm advantages structures a similar approach proposed hmms iohmms
we present new algorithm finding low complexity neural networks high generalization capability the algorithm searches flat minimum error function a flat minimum large connected region weightspace error remains approximately constant an mdlbased bayesian argument suggests flat minima correspond simple networks low expected overfitting the argument based gibbs algorithm variant novel way splitting generalization error underfitting overfitting error unlike many previous approaches require gaussian assumptions depend good weight prior instead prior inputoutput functions thus taking account net architecture training set although algorithm requires computation second order derivatives backprops order complexity automatically effectively prunes units weights input lines various experiments feedforward recurrent nets described in application stock market prediction flat minimum search outperforms conventional backprop weight decay optimal brain surgeon optimal brain damage we also provide pseudo code algorithm omitted ncversion
this paper describes method improving comprehensibility accuracy generality reactive plans a reactive plan set reactive rules our method involves two phases formulate explanations execution traces generate new reactive rules explanations since explanation phase previously described primary focus paper rule generation phase this latter phase consists taking subset explanations using explanations generate set new reactive rules add original set the particular subset explanations chosen yields rules provide new domain knowledge handling knowledge gaps original rule set the original rule set complimentary manner provides expertise fill gaps domain knowledge provided new rules incomplete
technical report ai may abstract a new method developing good valueordering strategies constraint satisfaction search presented using evolutionary technique called sane individual neurons evolve cooperate form neural network problemspecific knowledge discovered results better valueordering decisions based problemgeneral heuristics a neural network evolved chronological backtrack search decide ordering cars resourcelimited assembly line the network required backtracks random ordering backtracks maximization future options heuristic the sane approach extend well domains heuristic information either difficult discover problemspecific
conversational casebased reasoning cbr shells eg inferences cbr express commercially successful tools supporting development help desk related applications in contrast rulebased expert systems capture knowledge cases rather problematic rules incrementally extended however rather eliminate knowledge engineering bottleneck refocus case engineering task carefully authoring cases according library design guidelines ensure good performance designing complex libraries according guidelines difficult software needed assist users case authoring we describe approach revising case libraries according design guidelines implementation clire empirical results showing conditions approach improve conversational cbr performance
we present computational model movement skill learning the types skills addressed class trajectory following movements involving multiple accelerations decelerations changes direction lasting seconds these skills acquired observation improved practice we also review speedaccuracy tradeoffone robust phenomena human motor behavior we present two speedaccuracy tradeoff experiments models performance fits human behavior quite well
reinforcement learning class problems autonomous agent acting given environment improves behavior progressively maximizing function calculated basis succession scalar responses received environment qlearning classifier systems cs two methods among used solve reinforcement learning problems notwithstanding popularity shared goal past often considered two different models in paper first show classifier system restricted sharp simplification called discounted max simple classifier system d max vscs boils tabular qlearning it follows d max vscs converges optimal policy proved watkins dayan draw profit results experimental theoretical works dedicated improve qlearning facilitate use concrete applications in second part paper show three restrictions need impose cs deriving equivalence qlearning internal states dont care symbols structural changes turn essential recently rediscovered reprogrammed qlearning adepts eventually sketch similarities among ongoing work within research contexts the main contribution paper therefore make explicit strong similarities existing qlearning classifier systems show experience gained research within one domain useful direct future research one
some recent work investigated dichotomy compact coding using dimensionality reduction sparse distributed coding context understanding biological information processing we introduce artificial neural network self organises basis simple hebbian learning negative feedback activation show capable forming compact codings data distributions also identifying filters sensitive sparse distributed codes the network extremely simple biological relevance investigated via response set images typical everyday life however analysis networks identification filter sparse coding reveals coding may globally optimal exists innate limiting factor transcended
for classes concepts defined certain classes analytic functions depending n parameters nonempty open sets samples length n shattered a slighly weaker result also proved piecewiseanalytic functions the special case neural networks discussed
two important goals evaluation ai theory model assess merit design decisions performance implemented computer system analyze impact performance system faces problem domains different characteristics this particularly difficult casebased reasoning systems systems typically complex tasks domains operate we present methodology evaluation casebased reasoning systems systematic empirical experimentation range system configurations environmental conditions coupled rigorous statistical analysis results experiments this methodology enables us understand behavior system terms theory design computational model select best system configuration given domain predict system behave response changing domain problem characteristics a case study multistrategy casebased reinforcement learning system performs autonomous robotic navigation presented example
for casebased reasoner use knowledge flexibly must equipped powerful case adapter a casebased reasoner cope variation form problems given extent cases memory efficiently adapted fit wide range new situations in paper address task adapting abstract knowledge planning fit specific planning situations first show adapting abstract cases requires reconciling incommensurate representations planning situations next describe representation system memory organization adaptation process tailored requirement our approach implemented brainstormer planner takes abstract advice
the maximum likelihood estimator mle proportional hazards model current status data studied it shown mle regression parameter asymptotically normal p nconvergence rate achieves information bound even though mle baseline cumulative hazard function converges n rate estimation asymptotic variance matrix mle regression parameter also considered to prove main results also establish general theorem showing mle finite dimensional parameter class semiparametric models asymptotically efficient even though mle infinite dimensional parameter converges rate slower the results illustrated applying data set tumoriginicity study introduction in many survival analysis problems interested p
po box wellington new zealand tel fax internet techreportscompvuwacnz technical report cstr october abstract people often give advice telling stories stories recommend course action exemplify general conditions recommendation appropriate a computational model advice taking using stories must address two related problems determining storys recommendations appropriateness conditions showing obtain new situation in paper present efficient solution second problem based caching results first our proposal implemented brainstormer planner takes abstract advice
there increasing need efficient estimation mixture distributions especially following explosion use modelling tools many applied fields we propose paper bayesian noninformative approach estimation normal mixtures relies reparameterisation secondary components mixture terms divergence main component as well providing intuitively appealing representation modelling stage reparameterisation important bearing prior distribution performance mcmc algorithms we compare two possible reparameterisations extending mengersen robert show reparameterisation link secondary components together associated poor convergence properties mcmc algorithms
a worldwide web www server implemented common lisp order facilitate exploratory programming global hypermedia domain provide access complex research programs particularly artificial intelligence systems the server initially used provide interfaces document retrieval email servers more advanced applications include interfaces systems inductive rule learning naturallanguage question answering continuing research seeks fully generalize automatic formprocessing techniques developed email servers operate seamlessly web the conclusions argue presentationbased interfaces sophisticated form processing moved clients order reduce load servers provide advanced interaction models users
survival analysis concerned finding models predict survival patients assess efficacy clinical treatment a key part modelbuilding process selection predictor variables it standard use stepwise procedure guided series significance tests select single model make inference conditionally selected model however ignores model uncertainty substantial we review standard bayesian model averaging solution problem extend survival analysis introducing partial bayes factors cox proportional hazards model in two examples taking account model uncertainty enhances predictive performance extent could clinically useful
we propose bootstrapbased method model averaging selection focuses training points left individual bootstrap samples this information used estimate optimal weighting factors combining estimates different bootstrap samples also finding best subsets linear model setting these proposals provide alternatives bayesian approaches model averaging selection requiring less computation fewer subjective choices
technical report december statistics department university california berkeley ca abstract the theory behind success adaptive reweighting combining algorithms arcing adaboost freund schapire others reducing generalization error well understood by formulating prediction classification regression game one player makes selection instances training set convex linear combination predictors finite set existing arcing algorithms shown algorithms finding good game strategies an optimal game strategy finds combined predictor minimizes maximum error training set a bound generalization error combined predictors terms maximum error proven sharper bounds date arcing algorithms described converge optimal strategy schapire etal offered explanation adaboost works terms ability reduce margin comparing adaboost optimal arcing algorithm shows explanation valid answer lies elsewhere in situation vctype bounds misleading some empirical results given explore situation
conversational casebased reasoning ccbr form interactive casebased reasoning users input partial problem description text the ccbr system responds ranked solution display lists solutions stored cases whose problem descriptions best match users ranked question display lists unanswered questions cases users interact displays either refining problem description answering selected questions selecting solution apply ccbr systems support dialogue inferencing infer answers questions implied problem description otherwise questions listed user believes already answered the standard approach dialogue inferencing allows case library designers insert rules define implications problem description unanswered questions however approach imposes substantial knowledge engineering requirements we introduce alternative approach whereby intelligent assistant guides designer defining model case library implication rules derived we detail approach benefits explain supported integration parkadb fast relational database system we evaluate approach context ccbr system named nacodae this paper appeared aaai spring symposium multimodal reasoning ncarai tr aic we introduce integrated reasoning approach modelbased reasoning component performs important inferencing role conversational casebased reasoning ccbr system named nacodae breslow aha figure ccbr form casebased reasoning users enter text queries describing problem system assists eliciting refinements aha breslow cases three components
a readonce formula boolean formula variable occurs such formulas also called formulas boolean trees this paper treats problem exactly identifying unknown readonce formula using specific kinds queries the main results polynomial time algorithm exact identification monotone readonce formulas using membership queries polynomial time algorithm exact identification general readonce formulas using equivalence membership queries protocol based notion minimally adequate teacher our results improve valiants previous results readonce formulas we also show polynomial time algorithm using membership queries equivalence queries exactly identify readonce formulas
recursive autoassociative memory raam structures show promise general representation vehicle uses distributed patterns however training often difficult explains least part relatively small networks studied we show technique transforming collection hierarchical structures set training patterns sequential raam effectively trained using simple elmanstyle recurrent network tr aining produces set distributed patterns corresponding structures
we propose analyze distribution learning algorithm variable memory length markov processes these processes described subclass probabilistic finite automata name probabilistic finite suffix automata the learning algorithm motivated real applications manmachine interaction handwriting speech recognition conventionally used fixed memory markov hidden markov models either severe practical theoretical drawbacks though general hardness results known learning distributions generated sources similar structure prove algorithm indeed efficiently learn distributions generated restricted sources in particular show kldivergence distribution generated target source distribution generated hypothesis made small high confidence polynomial time sample complexity we demonstrate applicability algorithm learning structure natural english text using hy pothesis correction corrupted text
the clausal discovery engine claudien presented claudien discovers regularities data representative inductive logic programming paradigm as represents data regularities means first order clausal theories because search space clausal theories larger attribute value representation claudien also accepts input declarative specification language bias determines set syntactically wellformed regularities whereas papers claudien focuss semantics logical problem specification claudien discovery algorithm paclearning aspects paper wants illustrate power resulting technique in order achieve aim show claudien used learn integrity constraints databases functional dependencies determinations properties sequences mixed quantitative qualitative laws reverse engineering classification rules
in context machine learning examples paper deals problem estimating quality attributes without dependencies greedy search prevents current inductive machine learning algorithms detect significant dependencies attributes recently kira rendell developed relief algorithm estimating quality attributes able detect dependencies attributes we show strong relation reliefs estimates impurity functions usually used heuristic guidance inductive learning algorithms we propose use relieff extended version relief instead myopic impurity functions we reimplemented assistant system top induction decision trees using relieff estimator attributes selection step the algorithm tested several artificial several real world problems results show advantage presented approach inductive learning open wide rang possibilities using relieff
an investigation dynamics genetic programming applied chaotic time series prediction reported an interesting characteristic adaptive search techniques ability perform well many problem domains failing others because genetic programmings flexible tree structure particular problem represented myriad forms these representations variegated effects search performance therefore aspect fundamental engineering significance find representation acted upon genetic programming operators optimizes search performance we discover case chaotic time series prediction representation commonly used domain yield optimal solutions instead find population converges onto one accurately replicating tree trees explored to correct premature convergence make simple modification crossover operator in paper review previous work gp time series prediction pointing anomalous result related overlearning report improvement effected modified crossover operator
current ilp algorithms typically use variants extensions greedy search this prevents detect significant relationships training objects instead myopic impurity functions propose use heuristic based relief guidance ilp algorithms at step ilpr system heuristic used determine beam candidate literals the beam used exhaustive search potentially good conjunction literals from efficiency point view introduce interesting declarative bias enables us keep growth training set introducing new variables within linear bounds linear respect clause length this bias prohibits crossreferencing variables variable dependency tree the resulting system tested various artificial problems the advantages deficiencies approach discussed
instead myopic impurity functions propose use relieff heuristic guidance inductive learning algorithms the basic algoritm relief developed kira rendell kira rendell ab able efficiently solve classification problems involving highly dependent attributes parity problems however sensitive noise unable deal incomplete data multiclass regression problems continuous class we extended relief several directions the extended algorithm relieff able deal noisy incomplete data used multiclass problems regressional variant rrelieff deal regression problems another area application inductive logic programming ilp instead myopic measures relieff used estimate utility literals theory construction
in paper present tdleaf variation td algorithm enables used conjunction minimax search we present experiments chess backgammon demonstrate utility provide comparisons td another less radical variant tddirected in particular chess program knightcap used tdleaf learn evaluation function playing free internet chess server fics ficsonenetnet it improved rating rating games we discuss reasons success relationship results tesauros results backgammon
this paper deals asymptotic properties metropolishastings algorithm distribution interest unknown approximated sequential estimator density we prove simple conditions rate convergence metropolishastings algorithm sequential estimator latter introduced reversible measure metropolishastings kernel this problem natural extension previous work new simulated annealing algorithm sequential estimator energy
we explored two approaches recognizing faces across changes pose first developed representation face images based independent component analysis ica compared principal component analysis pca representation face recognition the ica basis vectors data set spatially local pca basis vectors ica representation greater invariance changes pose second present model development viewpoint invariant responses faces visual experience biological system the temporal continuity natural visual experience incorporated attractor network model hebbian learning following lowpass temporal filter unit activities when combined temporal filter basic hebbian update rule became generalization griniasty et al associates temporally proximal input patterns basins attraction the system acquired rep resentations faces largely independent pose
problems regression smoothing curve fitting addressed via predictive inference flexible class mixture models multidimensional density estimation using dirichlet mixture models provides theoretical basis semiparametric regression methods fitted regression functions may deduced means conditional predictive distributions these bayesian regression functions features similar generalised kernel regression estimates formal analysis addresses problems multivariate smoothing parameter estimation assessment uncertainties regression functions naturally computations based multidimensional versions existing markov chain simulation analysis univariate dirichlet mixture models
on basis early theoretical empirical studies genetic algorithms typically used point crossover operators standard mechanisms implementing recombination however number recent studies primarily empirical nature shown benefits crossover operators involving higher number crossover points from traditional theoretical point view surprising new results relate uniform crossover involves average l crossover points strings length l in paper extend existing theoretical results attempt provide broader explanatory predictive theory role multipoint crossover genetic algorithms in particular extend traditional disruption analysis include two general forms multipoint crossover npoint crossover uniform crossover we also analyze two aspects multipoint crossover operators namely recombination potential exploratory power the results analysis provide much clearer view role multipoint crossover genetic algorithms the implications results implementation issues performance discussed several directions research suggested
in paper discuss methodological issues using class neural networks called mixture density networks mdn discriminant analysis mdn models advantage rigorous probabilistic interpretation proven viable alternative classification procedure discrete domains we address classification interpretive aspects discriminant analysis compare approach traditional method linear discrimin ants implemented standard statistical packages we show mdn approach adopted performs well aspects many observations made restricted particular case hand applicable applications discriminant analysis educational research fl url httpwwwcshelsinkifiresearchcosco
satisfiability sat refers task finding truth assignment makes arbitrary boolean expression true this paper compares simulated annealing algorithm sasat gsat selman et al greedy algorithm solving satisfiability problems gsat solve problem instances extremely difficult traditional satisfiability algorithms results suggest sasat scales better number variables increases solving least many hard sat problems less effort the paper presents ablation study helps explain relative advantage sasat gsat next improvement basic sasat algorithm examined based random walk implemented gsat selman et al finally examine performance sasat test suite satisfiability problems produced dimacs challenge
we present comparison errorbased entropybased methods discretization continuous features our study includes extensive empirical comparison well analysis scenarios error minimization may inappropriate discretization criterion we present discretization method based c decision tree algorithm compare existing entropybased discretization algorithm employs minimum description length principle recently proposed errorbased technique we evaluate discretization methods respect c naivebayesian classifiers datasets uci repository analyze computational complexity method our results indicate entropybased mdl heuristic outperforms error minimization average we analyze shortcomings errorbased approaches comparison entropybased methods
here show similar construction multipleoutput systems modifications let a b c discretetime signlinear system state space ir n p outputs perform change a n fi n invertible a n fi n nilpotent if a b reachable pair a c observable pair minimal sense signlinear system inputoutput behavior dimension least n but n lt n det a observable hence canonical let us find another system necessarily signlinear inputoutput behavior canonical let relative degree ith row markov sequence a minf pg let initial state x there difference case smallest relative degree greater equal n case lt n roughly speaking n outputs signlinear system give us information sign cx sign cax sign ca x first outputs sys tem after use inputs outputs learn x first n components x when lt n may able use controls learn x last n components x time n nilpotency a finally lemma two states x z indistinguishable x z proof in case n equations x z equality the first output terms exactly terms so equalities satisfied first output terms coincide x z input equality everything first n components equivalent first n output terms coinciding x z since jth row qth output initial state x example either sign c j a q x j gt q sign c j a q x a j j u q j j q case may use control u q j identify c j a q x using remark
so applying corollary second equation conclude from get jgy n k obtain jy n k from see righthand side bounded since system a k b jyj ev n now suppose lim sup jytj gt then jyj ev since j kyj ljyj using obtain jyj ev l ffi note righthand side inequality trivial since know jyj ev from ffi n ffi gt n however see still holds so established cases from get jyj ev taking lim sup lefthand side n ffi ie n ffi substituting get jyj ev ffi jyj ev n ffi so take n n l conclusion follows to complete proof need deal general case gt inputs this done induction proof omitted fuller at in large stability relay saturated control systems linear controllers int j control gutman po p hagander a new design constrained controllers linear systems ieee transactions automat contr ac kosut rl design linear systems saturating linear control bounded states ieee trans autom control ac krikelis nj sk barkas design tracking systems subject actuator saturation integrator windup int j control schmitendorf we br barmish null controllability linear systems constrained controls siam j control opt slemrod m feedback stabilization linear control system hilbert space math control signals systems slotine jje w li applied nonlinear control prenticehall englewood cliffs sontag ed an algebraic approach bounded controllability linear systems int j control sontag ed remarks stabilization inputtostate stability proc ieee cdc tampa dec ieee publications pp sontag ed mathematical control theory deterministic finite dimensional systems springer new york sontag ed hj sussmann nonlinear output feedback design linear systems saturating controls proc ieee cdc honolulu dec ieee publications pp sussmann h j y yang on stabilizability multiple integrators means bounded feedback controls proc ieee cdc brighton uk dec ieee publications teel ar global stabilization restricted tracking multiple integrators bounded controls systems control letters yang y hj sussmann ed sontag stabilization linear systems bounded controls proc june nolcos bordeaux m fliess ed ifac publications pp yang y global stabilization linear systems bounded feedback ph d thesis mathematics department rutgers university jyj ev m ffi
gross error detection plays vital role parameter estimation data reconciliation dynamic steady state systems in particular recent advances process optimization allow data reconciliation dynamic systems appropriate problem formulations need considered data errors due either miscalibrated faulty sensors random events nonrepresentative underlying statistical distribution induce heavy biases parameter estimates reconciled data in paper concentrate robust estimators exploratory statistical methods allow us detect gross errors data reconciliation performed these robust methods property insensitive departures ideal statistical distributions therefore insensitive presence outliers once regression done outliers detected readily using exploratory statistical techniques an important feature performance optimization algorithm uniqueness reconciled data ability classify variables according observability redundancy properties here observable variable unmeasured quantity estimated measured variables physical model nonredundant variable measured variable estimated measurements variable classification used aid design instrumentation schemes in
many significant realworld classification tasks involve large number categories arranged hierarchical structure example classifying documents subject categories library congress scheme classifying worldwideweb documents topic hierarchies we investigate potential benefits using given hierarchy base classes learn accurate multicategory classifiers domains first consider possibility exploiting class hierarchy prior knowledge help one learn accurate classifier we explore benefits learning categorydiscriminants hard topdown fashion compare soft approach shares training data among sibling categories in verify hierarchies potential improve prediction accuracy but argue reasons subtle sometimes improvement using hierarchy happens constrain expressiveness hypothesis class appropriate manner however various controlled experiments show cases performance advantage associated using hierarchy really seem due prior knowledge encodes
many algorithms inferring decision tree data involve twophase process first large decision tree grown typically ends overfitting data to reduce overfitting second phase tree pruned using one number available methods the final tree output used classification test data in paper suggest alternative approach pruning phase using given unpruned decision tree present new method making predictions test data prove algorithms performance much worse precise technical sense predictions made best reasonably small pruning given decision tree thus procedure guaranteed competitive terms quality predictions pruning algorithm we prove procedure efficient highly robust our method viewed synthesis two previously studied techniques first apply cesabianchi et als results predicting using expert advice view pruning expert obtain algorithm provably low prediction loss computationally infeasible next generalize apply method developed buntine willems shtarkov tjalkens derive efficient implementation procedure
we present efficient algorithm paclearning general class geometric concepts lt fixed more specifically let t set halfspaces let x x x arbitrary point lt with t associate boolean indicator function i x x halfspace the concept class c study consists concepts formed boolean function i i t this concept class much general geometric concept class known paclearnable our results easily extended efficiently learn boolean combination polynomial number concepts selected concept class c lt given vcdimension c dependence thus constant constant polynomial time algorithm determine concept c consistent given set labeled examples we also present statistical query version algorithm tolerate random classification noise noise rate strictly less finally present generalization standard net result haussler welzl apply give alternative noisetolerant algorithm based geometric subdivisions
in work develop new criteria perform pessimistic decision tree pruning our method theoretically sound based theoretical concepts uniform convergence vapnikchervonenkis dimension we show criteria well motivated theory side performs well practice the accuracy new criteria comparable current method used c
in paper study performance probabilistic networks context protein sequence analysis molecular biology specifically report results initial experiments applying framework problem protein secondary structure prediction one main advantages probabilistic approach describe ability perform detailed experiments experiment different models we easily perform local substitutions mutations measure probabilistically effect global structure windowbased methods support experimentation readily our method efficient training prediction important order able perform many experiments different networks we believe probabilistic methods comparable methods prediction quality in addition predictions generated methods precise quantitative semantics shared classification methods specifically causal statistical independence assumptions made explicit networks thereby allowing biologists study experiment different causal models convenient manner
in paper prove sanitycheck bounds error leaveoneout crossvalidation estimate generalization error bounds showing worstcase error estimate much worse training error estimate the name sanitycheck refers fact although often expect leaveoneout estimate perform considerably better training error estimate seeking assurance performance considerably worse perhaps surprisingly assurance given limited cases prior literature crossvalidation any nontrivial bound error leaveoneout must rely notion algorithmic stability previous bounds relied rather strong notion hypothesis stability whose application primarily limited nearestneighbor local algorithms here introduce new weaker notion error stability apply obtain sanitycheck bounds leaveoneout classes learning algorithms including training error minimization procedures bayesian algorithms we also provide lower bounds demonstrating necessity form error stability proving bounds error leaveoneout estimate fact training error minimization algorithms worst case bounds must still depend vapnikchervonenkis dimension hypothesis class
this paper describes program observes behaviour actors simulated world uses observations guides conducting experiments an experiment sequence actions carried actor order support weaken case generalisation concept a generalisation attempted program observes state world similar previous state a partial matching algorithm used find substitutions enable two states unified the generalisation two states unifier
widespread adoption genetic programming techniques domainindependent problem solving tool depends good underlying software structure a system presented mirrors conceptual makeup gp system consisting loose collection software components strict interface definitions roles system maximises flexibility minimises effort applied new problem domain
coevolution competitive species provides interesting testbed study role adaptive behavior provides unpredictable dynamic environments in paper experimentally investigate arguments coevolution different adaptive protean behaviors competing species predators preys both species implemented simulated mobile robots kheperas infrared proximity sensors predator additional vision module whereas prey maximum speed set twice predator different types variability life neurocontrollers architecture genetic length compared it shown simple forms proteanism affect coevolutionary dynamics preys rather exploit noisy controllers generate random trajectories whereas predators benefit directionalchange controllers improve pursuit behavior
the calculation second derivatives required recent training analysis techniques connectionist networks elimination superfluous weights estimation confidence intervals weights network outputs we review develop exact approximate algorithms calculating second derivatives for networks jwj weights simply writing full matrix second derivatives requires ojwj operations for networks radial basis units sigmoid units exact calculation necessary intermediate terms requires order h backwardforwardpropagation passes h number hidden units network we also review compare three approximations ignoring components second derivative numerical differentiation scoring our algorithms apply arbitrary activation functions networks error functions instance connections skip layers radial basis functions crossentropy error softmax units etc
in paper describe principles problem solving analogy applied domain functional program synthesis for reason treat programs syntactical structures we discuss two different methods handle structures graph metric determining distance two program schemes b structure mapping engine existing system examine analogical processing furthermore show experimental results discuss
there many ways learning system generalize training set data this paper presents several generalization styles using prototypes attempt provide accurate generalization training set data wide variety applications these generalization styles efficient terms time space lend well massively parallel architectures empirical results generalizing several realworld applications given results indicate prototype styles generalization presented potential provide accurate generalization many applications
this paper provides exposition recent research regarding systemtheoretic aspects continuoustime recurrent dynamic neural networks sigmoidal activation functions the class systems introduced discussed result cited regarding universal approximation properties known characterizations controllability observability parameter identifiability reviewed well result minimality facts regarding computational power recurrent nets also mentioned fl supported part us air force grant afosr
most artificial neural networks anns fixed topology learning often suffer number shortcomings result anns use dynamic topologies shown ability overcome many problems adaptive self organizing concurrent systems asocs class learning models inherently dynamic topologies this paper introduces locationindependent transformations lits general strategy implementing learning models use dynamic topologies efficiently parallel hardware a lit creates set locationindependent nodes node computes part network output independent nodes using local information this type transformation allows efficient support adding deleting nodes dynamically learning in particular paper presents location independent asocs lia model lit asocs adaptive algorithm the description lia gives formal definitions lia algorithms because lia implements basic asocs mechanisms definitions provide formal description basic asocs mechanisms general addition lia
reinforcement learning algorithms often work finding functions satisfy bellman equation this yields optimal solution prediction markov chains controlling markov decision process mdp finite number states actions this approach also frequently applied markov chains mdps infinite states we show case bellman equation may multiple solutions many lead erroneous predictions policies baird algorithms conditions presented guarantee single optimal solution bellman equation
a major issue casebasedsystems retrieving appropriate cases memory solve given problem this implies case indexed appropriately stored memory a casebased system dynamic stores cases reuse needs learn indices new knowledge system designers envision knowledge irrespective type indexing structural functional hierarchical organization case memory raises two distinct related issues index learning learning indexing vocabulary learning right level generalization in paper show structurebehaviorfunction sbf models help learning structural indices design cases domain physical devices the sbf model design provides functional causal explanation structure design delivers function we describe sbf model design provides vocabulary structural indexing design cases inductive biases index generalization we discuss modelbased learning integrated similaritybased learning uses prior design cases learning level index generalization
we present representational format observed movements the representation temporal structure relating components single complex movement we also present oxbow unsupervised learning system constructs classes movements empirical results indicate system builds abstract movement concepts appropriate component structure allowing predict latter portions partially observed movement
constructive induction divides problem learning inductive hypothesis two intertwined searches onefor best representation space twofor best hypothesis space in datadriven constructive induction dci learning system searches better representation space analyzing input examples data the presented datadriven constructive induction method combines aqtype learning algorithm two classes representation space improvement operators constructors destructors the implemented system aqdci experimentally applied gnp prediction problem using world bank database the results show decision rules learned aqdci outperformed rules learned original representation space predictive accuracy rule simplicity
we report novel possibility extracting small subset data base contains information necessary solve given classification task using support vector algorithm train three different types handwritten digit classifiers observed types classifiers construct decision surface strongly overlapping small subsets data base this finding opens possibility compressing data bases significantly disposing data important solution given task in addition show theory allows us predict classifier best generalization ability based solely performance training set characteristics learning machines this finding important cases amount available data limited
physical variables orientation line visual field location body space coded activity levels populations neurons reconstruction decoding inverse problem physical variables estimated observed neural activity reconstruction useful first quantifying much information physical variables present population second providing insight brain might use distributed representations solving related computational problems visual object recognition spatial navigation two classes reconstruction methods namely probabilistic bayesian methods basis function methods discussed they include important existing methods special cases population vector coding optimal linear estimation template matching as representative example reconstruction problem different methods applied multielectrode spike train data hippocampal place cells freely moving rats the reconstruction accuracy trajectories rats compared different methods bayesian methods especially accurate continuity constraint enforced best errors within factor two informationtheoretic limit accurate reconstruction comparable intrinsic experimental errors position tracking in addition reconstruction analysis uncovered interesting aspects place cell activity tendency erratic jumps reconstructed trajectory animal stopped running in general theoretical values minimal achievable reconstruction errors quantify accurately physical variable encoded neuronal population sense mean square error regardless method used reading information one related result theoretical accuracy independent width gaussian tuning function two dimensions finally reconstruction methods considered paper implemented unified neural network architecture brain could feasibly use solve related problems
the headdirection hd cells found limbic system freely moving rats represent instantaneous head direction animal horizontal plane regardless location animal the internal direction represented cells uses selfmotion information inertially based updating familiar visual landmarks calibration here model dynamics hd cell ensemble presented the stability localized static activity profile network dynamic shift mechanism explained naturally synaptic weight distribution components even odd symmetry respectively under symmetric weights symmetric reciprocal connections stable activity profile close known directional tuning curves emerge by adding slight asymmetry weights activity profile shift continuously without disturbances shape shift speed accurately controlled strength oddweight component the generic formulation shift mechanism determined uniquely within current theoretical framework the attractor dynamics system ensures modalityindependence internal representation facilitates correction cumulative error putative localview detectors the model offers specific onedimensional example computational mechanism truly worldcentered representation derived observercentered sensory inputs integrating selfmotion information
we present biasvariance decomposition expected misclassification rate commonly used loss function supervised classification learning the biasvariance decomposition quadratic loss functions well known serves important tool analyzing learning algorithms yet decomposition offered commonly used zeroone misclassification loss functions recent work kong dietterich breiman their decomposition suffers major shortcomings though eg potentially negative variance decomposition avoids we show practice naive frequencybased estimation decomposition terms biased show correct bias we illustrate decomposition various algorithms datasets uci repository
the dominant component computational burden solving n n trivial p r b l e w h evolutionary algorithms task measuring fitness individual generation evolving population the advent r p l r e c n f g u r b l e f e l programmable gate arrays fpgas idea evolvable hardware opens possiblity e b n g individual evolving population hardware purpose accelerating timeconsuming fitness evaluation task this paper demonstrates massive parallelism rapidly r e c n f g u r b l e x l n x x c fpga exploited accelerate computationally burdensome fitness evaluation task genetic programming the work done virtual computing corporations lowcost hots expansion board pc type computers a step sorter evolved two fewer steps sorting network described oconnor nelson patent sorting networks number steps minimal sorter devised floyd knuth subsequent patent
a theoretically justifiable fast finite successive linear approximation algorithm proposed obtaining parsimonious solution corrupted linear system ax b p corruption p due noise error measurement the proposed linearprogrammingbased algorithm finds solution x parametrically minimizing number nonzero elements x error k ax b p k numerical tests signalprocessingbased example indicate proposed method comparable method parametrically minimizes norm solution x error k ax b p k methods superior orders magnitude solutions obtained least squares well combinatorially choosing optimal solution specific number nonzero elements
in natural visual experience different views object face tend appear close temporal proximity a set simulations presented demonstrate viewpoint invariant representations faces developed visual experience capturing temporal relationships among input patterns the simulations explored interaction temporal smoothing activity signals hebbian learning foldiak feedforward system recurrent system the recurrent system generalization hopfield network lowpass temporal filter unit activities following training sequences graylevel images faces changed pose multiple views given face fell basin attraction system acquired representations faces approximately viewpoint invariant
neural networks successfully applied wide range supervised unsupervised learning applications neuralnetwork methods commonly used datamining tasks however often produce incomprehensible models require long training times in article describe neuralnetwork learning algorithms able produce comprehensible models require excessive training times specifically discuss two classes approaches data mining neural networks the first type approach often called rule extraction involves extracting symbolic models trained neural networks the second approach directly learn simple easytounderstand networks we argue given current state art neuralnetwork methods deserve place tool boxes datamining specialists
a statistical theory overtraining proposed the analysis treats realizable stochastic neural networks trained kullbackleibler loss asymptotic case it shown asymptotic gain generalization error small perform early stopping even access optimal stopping time considering crossvalidation stopping answer question in ratio examples divided training testing sets order obtain optimum performance in nonasymptotic region crossvalidated early stopping always decreases generalization error our large scale simulations done cm nice agreement analytical findings
mathematical programming approaches three fundamental problems described feature selection clustering robust representation the feature selection problem considered discriminating two sets recognizing irrelevant redundant features suppressing this creates lean model often generalizes better new unseen data computational results real data confirm improved generalization leaner models clustering exemplified unsupervised learning patterns clusters may exist given database useful tool knowledge discovery databases kdd a mathematical programming formulation problem proposed theoretically justifiable computationally implementable finite number steps a resulting kmedian algorithm utilized discover useful survival curves breast cancer patients medical database robust representation concerned minimizing trained model degradation applied new problems a novel approach proposed purposely tolerates small error training process order avoid overfitting data may contain errors examples applications concepts given
concept learning viewed search space concept descriptions the hypothesis language determines search space in standard inductive learning algorithms structure search space determined generalizationspecialization operators algorithms perform locally optimal search using hillclimbing andor beamsearch strategy to overcome limitation concept learning viewed stochastic search space concept descriptions the proposed stochastic search method based simulated annealing known successful means solving combinatorial optimization problems the stochastic search method implemented rule learning system atris based compact efficient representation problem appropriate operators structuring search space furthermore heuristic pruning search space method enables also handling imperfect data the paper introduces stochastic search method describes atris learning algorithm gives results experiments
the increasing availability finelygrained parallel architectures resulted variety evolutionary algorithms eas population spatially distributed local selection algorithms operate parallel small overlapping neighborhoods the effects design choices regarding particular type local selection algorithm well size shape neighborhood particularly well understood generally tested empirically in paper extend techniques used formally analyze selection methods sequential eas apply local neighborhood models resulting much clearer understanding effects neighborhood size shape
qualitative probabilistic reasoning bayesian network often reveals tradeoffs relationships ambiguous due competing qualitative influences we present two techniques combine qualitative numeric probabilistic reasoning resolve tradeoffs inferring qualitative relationship nodes bayesian network the first approach incrementally marginalizes nodes contribute ambiguous qualitative relationships the second approach evaluates approximate bayesian networks bounds probability distributions uses bounds determinate qualitative relationships question this approach also incremental algorithm refines state spaces random variables tighter bounds qualitative relationships resolved both approaches provide systematic methods tradeoff resolution potentially lower computational cost application purely numeric methods
a simple powerful modification standard gaussian distribution studied the variables rectified gaussian constrained nonnegative enabling use nonconvex energy functions two multimodal examples competitive cooperative distributions illustrate representational power rectified gaussian since cooperative distribution represent translations pattern demonstrates potential rectified gaussian modeling pattern manifolds
this paper appear neural computation abstract we introduce novel fast algorithm independent component analysis used blind source separation feature extraction it shown neural network learning rule transformed txedpoint iteration provides algorithm simple depend userdetned parameters fast converge accurate solution allowed data the algorithm tnds one time nongaussian independent components regardless probability distributions the computations performed either batch mode semiadaptive manner the convergence algorithm rigorously proven convergence speed shown cubic some comparisons gradient based algorithms made showing new algorithm usually times faster sometimes giving solution iterations
barlows seminal work minimal entropy codes unsupervised learning reiterated in particular need transmit probability events put practical neuronal framework detecting suspicious events a variant bcm learning rule presented together mathematical results suggesting optimal minimal entropy coding
constructive induction divides problem learning inductive hypothesis two intertwined searches onefor best representation space twofor best hypothesis space in datadriven constructive induction dci learning system searches better representation space analyzing input examples data the presented datadriven constructive induction method combines aqtype learning algorithm two classes representation space improvement operators constructors destructors the implemented system aqdci experimentally applied gnp prediction problem using world bank database the results show decision rules learned aqdci outperformed rules learned original representation space predictive accuracy rule simplicity
heuristic measures estimating quality attributes mostly assume independence attributes domains strong dependencies attributes performance poor relief extension relieff capable correctly estimating quality attributes classification problems strong dependencies attributes by exploiting local information provided different contexts provide global view we present analysis relieff lead us adaptation regression continuous class problems the experiments artificial realworld data sets show regressional relieff correctly estimates quality attributes various conditions used nonmyopic learning regression trees regressional relieff relieff provide unified view estimating attribute quality regression classification
a new research area inductive logic programming presently emerging while inheriting various positive characteristics parent subjects logic programming machine learning hoped new area overcome many limitations forebears the background present developments within area discussed various goals aspirations increasing body researchers identified inductive logic programming needs based sound principles logic statistics on side statistical justification hypotheses discuss possible relationship algorithmic complexity theory probablyapproximatelycorrect pac learning in terms logic provide unifying framework muggleton buntines inverse resolution ir plotkins relative least general generalisation rlgg rederiving rlgg terms ir this leads discussion feasibility extending rlgg framework allow invention new predicates previously discussed within context ir
bayesian methods applicable complex modeling tasks in review principles bayesian inference presented applied neural network models several approximate implementations discussed advantages conventional frequentist model training selection outlined it argued bayesian methods preferable traditional approaches although empirical evidence still sparse
this paper presents efficient algorithm learning bayesian belief networks databases the algorithm takes database input constructs belief network structure output the construction process based computation mutual information attribute pairs given data set large enough algorithm generate belief network close underlying model time enjoys time when data set normal dagfaithful see section probability distribution algorithm guarantees structure perfect map pearl underlying dependency model generated to evaluate algorithm present experimental results three versions wellknown alarm network database attributes records the results show algorithm accurate efficient the proof correctness analysis complexity o n conditional independence ci tests
this paper presents efficient algorithm constructing bayesian belief networks databases the algorithm takes database attributes ordering ie causal attributes attribute appear earlier order input constructs belief network structure output the construction process based computation mutual information attribute pairs given data set large enough dagisomorphic probability distribution algorithm guarantees perfect map underlying dependency tests to evaluate algorithm present experimental results three versions wellknown alarm network database attributes records the correctness proof analysis computational complexity also presented we also discuss features work relate previous works model generated time enjoys time complexity o n conditional independence ci
a novel method regression recently proposed v vapnik et al the technique called support vector machine svm well founded mathematical point view seems provide new insight function approximation we implemented svm tested data base chaotic time series used compare performances different approximation techniques including polynomial rational approximation local polynomial techniques radial basis functions neural networks the svm performs better approaches presented we also study particular time series variability performance respect free parameters svm
the requirement dense interconnect artificial neural network systems led researchers seek highdensity interconnect technologies this paper reports implementation using multichip modules mcms interconnect medium the specific system described selforganizing parallel dynamic learning model requires dense interconnect technology effective implementation requirement fulfilled exploiting mcm technology the ideas presented paper regarding mcm implementation artificial neural networks versatile adapted apply neural network connectionist models
when specializing recursive predicate order exclude set negative examples without excluding set positive examples may possible specialize remove clauses refutation negative example without excluding positive exam ples a previously proposed solution problem apply program transformation order obtain nonrecursive target predicates recursive ones however application method prevents recursive specializations found in work present algorithm spectre ii limited specializing nonrecursive predicates the key idea upon algorithm based enough specialize remove clauses refutations negative examples order obtain correct specializations sometimes necessary specialize clauses appear refutations positive examples in contrast predecessor spectre new algorithm limited specializing clauses defining one predicate may specialize clauses defining multiple predicates furthermore positive negative examples longer required instances predicate it proven algorithm produces correct specialization positive examples logical consequences original program finite number derivations positive negative examples positive negative examples sequence input clauses refutations
program wrt positive negative examples viewed problem pruning sldtree refutations negative examples refutations positive examples excluded it shown actual pruning performed applying unfolding clause removal the algorithm spectre presented based idea the input algorithm besides logic program positive negative examples computation rule determines shape sldtree pruned it shown generality resulting specialization dependent computation rule experimental results presented using three different computation rules the experiments indicate computation rule formulated number applications unfolding kept low possible the algorithm uses divideandconquer method also compared covering algorithm the experiments show higher predictive accuracy achieved focus discriminating positive negative examples rather achieving high coverage positive examples
cognitive mapping qualitative decision modeling technique developed twenty years ago political scientists continues see occasional use social science decisionaiding applications in paper i show cognitive maps viewed context recent formalisms qualitative decision modeling latter provide firm semantic foundation facilitate development powerful inference procedures well extensions expressiveness models sort
casebased reasoning systems traditionally used perform highlevel reasoning problem domains adequately described using discrete symbolic representations however many realworld problem domains autonomous robotic navigation better characterized using continuous representations such problem domains also require continuous performance continuous sensorimotor interaction environment continuous adaptation learning performance task we introduce new method continuous casebased reasoning discuss applied dynamic selection modification acquisition robot behaviors autonomous navigation systems we conclude general discussion casebased reasoning issues addressed work
the eastwest challenge title second international competition machine learning programs organized fall donald michie stephen muggleton david page ashwin srinivasan oxford university the goal competition solve trains problems discover simplest classification rules trainlike structured objects the rule complexity judged prolog program counted number various components rule expressed prolog horn clauses there entries several countries submitted competition the gmu teams entry generated three members aq family learning programs aqdt induce aqhci the paper analyses results obtained programs compares obtained learning programs it also presents ideas research inspired competition one ideas challenge machine learning community develop measure knowledge complexity would adequately capture cognitive complexity knowledge a preliminary measure cognitive complexity called ccomplexity different prologcomplexity pcomplexity used competition briefly discussed the authors thank professors donald michie steve muggleton david page ashwin srinivasan organizing westeast challenge competition machine learning programs provided us stimulating challenge learning programs inspired new ideas improving the authors also thank nabil allkharouf ali hadjarian help suggestions efforts solve problems posed competition this research conducted center machine learning inference george mason university the centers research supported part advanced research projects agency grant no nj administered office naval research grant no fj administered air force office scientific research part office naval research grant no nj part national science foundation grants no iri cda dmi
previous algorithms recovery bayesianbelief network structures data either highly dependent conditional independence ci tests required ordering nodes supplied user we present algorithm integrates two approaches ci tests used generate ordering nodes database used recover underlying bayesian network structure using non ci test based method results evaluation algorithm number databases eg alarm led soybean presented we also discuss algorithm performance issues open problems
we propose new criterion model selection prediction problems the covariance inflation criterion adjusts training error average covariance predictions responses prediction rule applied permuted versions dataset this criterion applied general prediction problems example regression classification general prediction rules example stepwise regression treebased models neural nets as byproduct obtain measure effective number parameters used adaptive procedure we relate covariance inflation criterion model selection procedures illustrate use regression classification problems we also revisit conditional bootstrap approach model selection
this paper introduces magnetic neural gas mng algorithm extends unsupervised competitive learning class information improve positioning radial basis functions the basic idea mng discover heterogeneous clusters ie clusters data different classes migrate additional neurons towards the discovery effected heterogeneity coefficient associated neuron migration guided introducing kind magnetic effect the performance mng tested number data sets including thyroid data set results demonstrate promise
this research supported national science foundation fellowship awarded dario salvucci office naval research grant n awarded john anderson the views conclusions contained document authors interpreted representing official policies either expressed implied national science foundation office naval research united states government
efficient algorithms developed estimating model parameters measured data even presence gross errors in addition point estimates parameters however assessments uncertainty needed linear approximations provide standard errors misleading applied models substantially nonlinear to overcome difficulty profiling methods developed case regressor variables error free in paper extend profiling methods errorinvariablemeasurement evm models we use laplaces method integrate incidental parameters associated measurement errors apply profiling methods obtain approximate confidence contours parameters this approach computationally efficient requiring function evaluations applied large scale problems it useful certain measurement errors eg input variables relatively small small ignored
a novel architecture set learning rules cortical selforganization proposed the model based idea multiple information channels modulate one anothers plasticity features learned bottomup information sources thus influenced learned contextual pathways vice versa a maximum likelihood cost function allows scheme implemented biologically feasible hierarchical neural circuit in simulations model first demonstrate utility temporal context modulating plasticity the model learns representation categorizes peoples faces according identity independent viewpoint taking advantage temporal continuity image sequences in second set simulations add plasticity contextual stream explore variations architecture in case model learns twotiered representation starting coarse viewbased clustering proceeding finer clustering specific stimulus features this model provides tenable account people may perform d object recognition hierarchical bottomup fashion
the boosting algorithm adaboost developed freund schapire exhibited outstanding performance several benchmark problems using c weak algorithm boosted like ensemble learning approaches adaboost constructs composite hypothesis voting many individual hypotheses in practice large amount memory required store hypotheses make ensemble methods hard deploy applications this paper shows selecting subset hypotheses possible obtain nearly levels performance entire set the results also provide insight behavior adaboost
infusion gaba agonist reiter stryker infusion nmda receptor antagonist bear et al primary visual cortex kittens monocular deprivation shifts ocular dominance toward closed eye cortical region near infusion site this reverse ocular dominance shift previously modeled variants covariance synaptic plasticity rule bear et al clothiaux et al miller et al reiter stryker kasamatsu et al showed infusion nmda receptor antagonist adult cat primary visual cortex changes ocular dominance distribution reduces binocularity reduces orientation direction selectivity this paper presents novel account effects pharmacological treatments based exin synaptic plasticity rules marshall include instar afferent excitatory outstar lateral inhibitory rule functionally exin plasticity rules enhance efficiency discrimination contextsensitivity neural networks representation perceptual patterns marshall marshall gupta the exin model decreases lateral inhibition neurons outside infusion site control regions neurons inside infusion region monocular deprivation in model plasticity afferent pathways neurons affected pharmacological treatments assumed blocked opposed previous models bear et al miller et al reiter stryker afferent pathways open eye neurons infusion region weakened the proposed model consistent results suggesting longterm plasticity blocked nmda antagonists postsynaptic hyperpolarization bear et al dudek bear goda stevens kirkwood et al since role plasticity lateral inhibitory pathways producing cortical plasticity received much attention several predictions made based exin lateral inhibitory plasticity rule
we present two algorithms use membership equivalence queries exactly identify concepts given union discretized axisparallel boxes ddimensional discretized euclidean space coordinate n discrete values the first algorithm receives sd counterexamples uses time membership queries polynomial log n constant further equivalence queries made formulated union osd log axisparallel boxes next introduce new complexity measure better captures complexity union boxes simply number boxes dimensions our new measure number segments target polyhedron segment maximum portion one sides polyhedron lies entirely inside entirely outside halfspaces defining polyhedron we present improvement first algorithm uses time queries polynomial log n the hypothesis class used decision trees height sd further show time queries used algorithm polynomial log n constant thus generalizing exact learnability dnf formulas constant number terms in fact single algorithm efficient either constant
approaches combining genetic algorithms neural networks received great deal attention recent years as result much work reported two major areas neural network design training topology optimization this paper focuses key issues associated problem pruning multilayer perceptron using genetic algorithms simulated annealing the study presented considers number aspects associated network training may alter behavior stochastic topology optimizer enhancements discussed improve topology searches simulation results two mentioned stochastic optimization methods applied nonlinear system identification presented compared simple random search
local belief propagation rules sort proposed pearl guaranteed converge optimal beliefs singly connected networks recently number researchers empirically demonstrated good performance algorithms networks loops theoretical understanding performance yet achieved here lay foundation understanding belief propagation networks loops for networks single loop derive analytical relationship steady state beliefs loopy network true posterior probability using relationship show category networks map estimate obtained belief update belief revision proven optimal although beliefs incorrect we show nodes use local information messages receive order correct steady state beliefs furthermore prove networks single loop map estimate obtained belief revision convergence guaranteed give globally optimal sequence states the result independent length cycle size state space for networks multiple loops introduce concept balanced network show simulation results comparing belief revision update networks we show turbo code structure balanced present simulations toy turbo code problem indicating decoding obtained belief revision convergence significantly likely correct this report describes research done center biological computational learning department brain cognitive sciences massachusetts institute technology support center provided part grant national science foundation contract asc yw also supported nei r ey e h adelson
previous work showed combination genetic algorithm using order permutation chromosome combined hand coded greedy optimizers readily produce optimal schedule four node test problem langdon following ga used find low cost schedules south wales region uk high voltage power network this paper describes evolution best known schedule base south wales problem using genetic programming starting hand coded heuris tics used ga
search mechanisms artificial intelligence combine two elements representation determines search space search mechanism actually explores space unfortunately many searches may explore redundant andor invalid solutions genetic programming refers class evolutionary algorithms based genetic algorithms utilizing parameterized representation form trees these algorithms perform searches based simulation nature they face problems redundantinvalid subspaces these problems recently addressed systematic manner this paper presents methodology devised public domain genetic programming tool lilgp this methodology uses data typing semantic information constrain representation space valid possibly unique solutions explored the user enters problemspecific constraints transformed normal set this set checked feasibility subsequently used limit space explored the constraints determine valid possibly unique space moreover also used exclude subspaces user considers uninteresting using problemspecific knowledge a simple example followed thoroughly illustrate constraint language transformations normal set experiments boolean multiplexer illustrate practical applications method limit redundant space exploration utilizing problemspecific knowledge fl supported grant nasajsc nag
knowledge acquisition difficult errorprone timeconsuming task the task automatically improving existing knowledge base using learning methods addressed class systems performing theory refinement this paper presents system forte firstorder revision theories examples refines firstorder hornclause theories integrating variety different revision techniques coherent whole forte uses techniques within hillclimbing framework guided global heuristic it identifies possible errors theory calls library operators develop possible revisions the best revision implemented process repeats revisions possible operators drawn variety sources including propositional theory refinement firstorder induction inverse resolution forte demonstrated several domains including logic programming qualitative modelling
the problem sequence categorization generalize corpus labeled sequences procedures accurately labeling future unlabeled sequences the choice representation sequences major impact task absence background knowledge good representation often known straightforward representations often far optimal we propose feature generation method called fgen creates boolean features check presence absence heuristically selected collections subsequences we show empirically representation computed fgen improves accuracy two commonly used learning systems c ripper new features added existing representations sequence data we show superiority fgen across range tasks selected three domains dna sequences unix command sequences english text
genetic algorithms one example use random element within algorithm combinatorial optimization we consider application genetic algorithm particular problem assembly line balancing problem a general description genetic algorithms given specialized use testbed problems discussed we carry extensive computational testing find appropriate values various parameters associated genetic algorithm these experiments underscore importance correct choice scaling parameter mutation rate ensure good performance genetic algorithm we also describe parallel implementation genetic algorithm give comparisons parallel serial implementations both versions algorithm shown effective producing good solutions problems type appropriately chosen parameters
this paper presents application casebased reasoning methods kosimo data base international conflicts a casebased reasoning tool viecbr deveolped used classification various outcome variables like political military territorial outcome solution modalities conflict intensity in addition case retrieval algorithms presented interactive usermodifiable tool intelli gently searching conflict data base precedent cases
in order learn behaviour casebased reasoners learning systems formalise simple casebased learner pac learning algorithm using casebased representation hcb we first consider naive casebased learning algorithm cb h learns collecting available cases casebase calculates similarity counting number features two problem descriptions agree we present results concerning consistency learning algorithm give partial results regarding sample complexity we able characterise cb h weak general learning algorithm we consider sample complexity casebased learning reduced specific classes target concept application inductive bias prior knowledge class target concepts following recent work demonstrating casebased learning improved choosing similarity measure appropriate concept learnt define second casebased learning algorithm cb learns using best possible similarity measure might inferred chosen target concept while cb executable learning strategy since chosen similarity measure defined terms priori knowledge actual target concept allows us assess limit maximum possible contribution approach casebased learning also addition illustrating role inductive bias definition cb simplifies general problem establishing functions might represented form hcb reasoning casebased representation special case therefore little straightforward general case cb h allowing substantial results regarding representable functions sample complexity presented cb in assessing results forced conclude casebased learning best approach learning chosen concept space space monomial functions we discuss however study demonstrated context casebased learning operation concepts well known machine learning inductive bias tradeoff computational complexity sample complexity
in past years evolutionary computation landscape rapidly changing result increased levels interaction various research groups injection new ideas challenge old tenets the effect simultaneously exciting invigorating annoying bewildering oldtimers well newcomers field emerging activity beginnings structure common themes agreement important open issues we attempt summarize emergent properties paper
we quantify experimentally analytically performance memorybased reasoning mbr algorithms to start gaining insight capabilities mbr algorithms compare mbr algorithm using value difference metric popular bayesian classifier these two approaches similar make certain independence assumptions data however whereas mbr uses specific cases perform classification bayesian methods summarize data probabilistically we demonstrate particular mbr system called pebls works comparatively well wide range domains using real artificial data with respect artificial data consider distributions concept classes separated functional discriminants well timeseries data generated markov models varying complexity finally show formally pebls learn limit natural concept classes bayesian classifier learn attain perfect accuracy whenever
the knearestneighbor decision rule assigns object unknown class plurality class among k labeled training objects closest closeness usually deflned terms metric distance euclidean space input measurement variables axes the metric chosen deflne distance strongly efiect performance an optimal choice depends problem hand characterized respective class distributions input measurement space within given problem location unknown object space in paper new types knearestneighbor procedures described estimate local relevance input variable linear combinations individual point classifled this information used separately customize metric used deflne distance object flnding nearest neighbors these procedures hybrid regular knearestneighbor methods treestructured recursive partitioning techniques popular statistics machine learning
seismic data interpretation problems typically solved using computationally intensive local search methods often result inferior solutions here traditional hybrid genetic algorithm compared different staged hybrid genetic algorithms geophysical imaging static corrections problem the traditional hybrid genetic algorithm used applied local search every offspring produced genetic search the staged hybrid genetic algorithms designed temporally separate local genetic search components distinct phases minimize interference two search methods the results show staged hybrid genetic algorithms produce higher quality solutions using significantly less computational time problem
we describe immune system model based universe binary strings the model directed understanding pattern recognition processes learning take place individual species levels immune system the genetic algorithm ga central component model in paper study behavior ga two pattern recognition problems relevant natural immune systems finally compare model explicit fitness sharing techniques genetic algorithms show model implements form implicit fitness sharing
we present method accurate representation highdimensional unknown functions random samples drawn input space the method builds representations function recursively splitting input space smaller subspaces subspaces linear approximation computed the representations function levels ie depths tree retained learning process good generalisation available well accurate representations subareas therefore fast accurate learning combined method
several authors made link hidden markov models time series energybased models luttrell williams saul jordan saul jordan discuss linear boltzmann chain model statestate transition energies a ii going state state symbol emission energies b ij probability entire state fi l j l g l whilst hmm written linear boltzmann chain setting expa ii ii expb ij b ij exp linear boltzmann chains represented hmms saul jordan however difference two models minimal to precise final hidden state l linear boltzmann chain constrained particular end state distribution sequences identical hidden markov model
we present coevolutionary approach learning sequential decision rules appears number advantages noncoevolutionary approaches the coevolutionary approach encourages formation stable niches representing simpler subbehaviors the evolutionary direction subbehavior controlled independently providing alternative evolving complex behavior using intermediate training steps results presented showing significant learning rate speedup noncoevolutionary approach simulated robot domain in addition results suggest coevolutionary approach may lead emer gent problem decompositions
appropriate bias widely viewed key efficient learning generalization i present new algorithm incremental deltabardelta idbd algorithm learning appropriate biases based previous learning experience the idbd algorithm developed case simple linear learning systemthe lms delta rule separate learningrate parameter input the idbd algorithm adjusts learningrate parameters important form bias system because bias approach adapted based previous learning experience appropriate testbeds drifting nonstationary learning tasks for particular tasks type i show idbd algorithm performs better ordinary lms fact finds optimal learning rates the idbd algorithm extends improves prior work jacobs fully incremental single free parameter this paper also extends previous work presenting derivation idbd algorithm gradient descent space learningrate parameters finally i offer novel interpretation idbd algorithm incremental form holdoneout cross validation
neural network pruning methods level individual network parameters eg connection weights improve generalization an open problem pruning methods known today obd obs autoprune epsiprune selection number parameters removed pruning step pruning strength this paper presents pruning method lprune automatically adapts pruning strength evolution weights loss generalization training the method requires algorithm parameter adjustment user the results extensive experimentation indicate lprune often superior autoprune superior obd diagnosis tasks unless severe pruning early training process required results statistical significance tests comparing autoprune new method lprune well backpropagation early stopping given different problems
icsim connectionist net simulator developed icsi written sather it objectoriented meet requirements flexibility reuse homogeneous structured connectionist nets allow user encapsulate efficient customized implementations perhaps running dedicated hardware nets composed combining offtheshelf library classes necessary specializing behaviour general user interface classes allow uniform customized graphic presentation nets modeled
in experiencebased casebased reasoning new problems solved retrieving adapting solutions similar problems encountered past an important issue experiencebased reasoning identify different types knowledge reasoning useful different classes caseadaptation tasks in paper examine class nonroutine caseadaptation tasks involve patterned insertions new elements old solutions we describe modelbased method solving task context design physical devices the method uses knowledge generic teleological mechanisms gtms cascading old designs adapted meet new functional specifications accessing instantiating appropriate gtm the kritik system evaluates computational feasibility sufficiency method design adaptation
the utility problem learning systems occurs knowledge learned attempt improve systems performance degrades performance instead we present methodology analysis utility problems uses computational models problem solving systems isolate root causes utility problem detect threshold conditions problem arise design strategies eliminate we present models casebased reasoning controlrule learning systems compare performance respect swamping utility problem our analysis suggests casebased reasoning systems resistant utility problem controlrule learning systems
we present model similaritybased retrieval attempts capture three psychological phenomena people extremely good judging similarity analogy given items compare superficial remindings much frequent structural remindings people sometimes experience use purely structural analogical remindings our model called macfac many called chosen consists two stages the first stage mac uses computationally cheap nonstructural matcher filter candidates pool memory items that redundantly encode structured representations content vectors whose dot product yields estimate well corresponding structural representations match the second stage fac uses sme compute true structural match probe output first stage macfac fully implemented show capable modeling patterns access found psychological data
we present algorithm online learning linear functions optimal within constant factor respect bounds sum squared errors worst case sequence trials the bounds logarithmic number variables furthermore algorithm shown optimally robust respect noise data within constant factor key words machine learning computational learning theory online learning linear functions worstcase loss bounds adaptive filter theory subject classifications t
a fundamental issue casebased reasoning similarity assessment determining similarities differences new retrieved cases many methods developed comparing input case descriptions cases already memory however success methods depends input case description sufficiently complete reflect important features new situation assured in casebased explanation anomalous events story understanding anomaly arises current situation incompletely understood consequently similarity assessment based matches known current features old cases likely fail gaps current cases description our solution problem gaps new cases description approach call constructive similarity assessment constructive similarity assessment treats similarity assessment simple comparison fixed new old cases process deciding types features investigated new situation features borne knowledge added description current case constructive similarity assessment merely compare new cases old using prior cases guide dynamically carves augmented descriptions new cases memory
much recent research modeling memory processes focused identifying useful indices retrieval strategies support particular memory tasks another important question concerning memory processes however retrieval criteria learned this paper examines issues involved modeling learning memory search strategies it discusses general requirements appropriate strategy learning presents model memory search strategy learning applied problem retrieving relevant information adapting cases casebased reasoning it discusses implementation model based lessons learned implementation points towards issues directions refining model
the problem approximating probability distribution occurs frequently many areas applied mathematics including statistics communication theory machine learning theoretical analysis complex systems neural networks saul jordan recently proposed powerful method efficiently approximating probability distributions known structured variational approximations in structured variational approximations exact algorithms probability computation tractable substructures combined variational methods handle interactions substructures make system whole intractable in note i present mathematical result simplify derivation struc tured variational approximations exponential family distributions
this paper presents asocs adaptive selforganizing concurrent system model massively parallel processing incrementally defined rule systems areas adaptive logic robotics logical inference dynamic control an asocs adaptive network composed many simple computing elements operating asynchronously parallel this paper focuses adaptive algorithm aa details architecture learning algorithm it advantages previous asocs models simplicity implementability cost an asocs operate either data processing mode learning mode during data processing mode asocs acts parallel hardware circuit in learning mode rules expressed boolean conjunctions incrementally presented asocs all asocs learning algorithms incorporate new rule distributed fashion short bounded time
this paper describes novel search algorithm called dynamic hill climbing borrows ideas genetic algorithms hill climbing techniques unlike genetic hill climbing algorithms dynamic hill climbing ability dynamically change coordinate frame course optimization furthermore algorithm moves coarsegrained search finegrained search function space changing mutation rate uses diversitybased distance metric ensure searches new regions space dynamic hill climbing empirically compared traditional genetic algorithm using de jongs wellknown five function test suite shown vastly surpass performance genetic algorithm often finding better solutions using many function evaluations
autonomous vehicles likely require sophisticated software controllers maintain vehicle performance presence vehicle faults the test evaluation complex software controllers expected challenging task the goal e ffort apply machine learning techniques field arti ficial intelligence general problem evaluating intelligent controller autonomous vehicle the approach involves subjecting controller adaptively chosen set fault scenarios within vehicle simulator searching combinations faults produce noteworthy performance vehicle controller the search employs genetic algorithm we illustrate approach evaluating performance subsumptionbased controller autonomous vehicle the preliminary evidence suggests approach e ffective alternative manual testing sophisticated software controllers
speedup learning seeks improve efficiency searchbased problem solvers in paper propose new theoretical model speedup learning captures systems improve problem solving performance solving usergiven set problems we also use model motivate notion batch problem solving argue congenial learning sequential problem solving our theoretical results applicable serially decomposable domains we empirically validate results domain eight puzzle
nonparametric density estimation problem approximating values probability density function given samples associated distribution nonparametric estimation finds applications discriminant analysis cluster analysis flow calculations based smoothed particle hydrodynamics usual estimators make use kernel functions require order n arithmetic operations evaluate density n sample points we describe sequence special weight functions requires almost linear number operations n computation
in paper consider learning firstorder horn programs entailment in particular show subclass firstorder acyclic horn programs constant arity exactly learnable equivalence entailment membership queries provided allows polynomialtime subsumption procedure satisfies closure conditions one consequence firstorder acyclic determinate horn programs constant arity exactly learnable equiv alence entailment membership queries
a strategy using genetic algorithms gas solve npcomplete problems presented the key aspect approach taken exploit observation although npcomplete problems equally difficult general computational sense much better ga representations others leading much successful use gas npcomplete problems others since npcomplete problem mapped one polynomial time strategy described consists identifying canonical npcomplete problem gas work well solving npcomplete problems indirectly mapping onto canonical problem initial empirical results presented support claim boolean satisfiability problem sat gaeffective canonical problem npcomplete problems poor ga representations solved efficiently mapping first onto sat problems
fully cooperative multiagent systemsthose agents share joint utility modelis special interest ai a key problem ensuring actions individual agents coordinated especially settings agents autonomous decision makers we investigate approaches learning coordinated strategies stochastic domains agents actions directly observable others much recent work game theory adopted bayesian learning perspective general problem equilibrium selection tends assume actions observed we discuss special problems arise actions observable including effects rates convergence effect action failure probabilities asymmetries we also use likelihood estimates means generalizing fictitious play learning models setting finally propose use maximum likelihood means removing strategies consideration aim convergence conventional equilibrium point learning deliberation cease
humans appear often solve problems new domain transferring expertise familiar domain however making crossdomain analogies hard often requires abstractions common source target domains recent work casebased design suggests generic mechanisms one type abstractions used designers however one important yet unexplored issue generic mechanisms come we hypothesize acquired incrementally problemsolving experiences familiar domains generalization patterns regularity three important issues generalization experiences generalize experience far generalize methods use in paper show mental models familiar domain provide content together problemsolving context learning occurs also provide constraints learning generic mechanisms design experiences in particular show modelbased learning method integrated similaritybased learning addresses issues generalization experiences
the optimization single bit string means iterated mutation selection best genetic algorithm discussed respect three simple fitness functions the counting ones problem standard binary encoded integer gray coded integer optimization problem a mutation rate schedule optimal respect success probability mutation presented objective functions turns standard binary code hamper search process even case unimodal objective functions while normally mutation rate l l denotes bit string length recommendable results indicate variation mutation rate useful cases fitness function multimodal pseudoboolean function multimodality may caused objective function well encoding mechanism
genetic algorithms used learn navigation collision avoidance behaviors robots the learning performed simulation resulting behaviors used control the approach learning behaviors robots described reflects particular methodology learning via simulation model the motivation making mistakes real systems may costly dangerous in addition time constraints might limit number experiences learning real world many cases simulation model made run faster real time since learning may require experimenting behaviors might occasionally produce unacceptable results applied real world might require much time real environment assume hypothetical behaviors evaluated simulation model offline system as illustrated figure current best behavior placed real online system learning continues offline system the learning algorithm designed learn useful behaviors simulations limited fidelity the expectation behaviors learned simulations useful realworld environments previous studies illustrated knowledge learned simulation robust might applicable real world simulation general ie noise varied conditions etc real world environment where possible important identify differences simulation world note effect upon learning process the research reported continues examine hypothesis the next section briefly explains learning algorithm gives pointers extensive documentation found after actual robot described then describe simulation robot the task actual robot
conventional intelligent tutoring systems its acknowledge uncertainty students knowledge yet outcome teaching intervention exact state students knowledge uncertain in recent years researchers made startling progress management uncertainty knowledgebased systems building developments describe its architecture explicitly models uncertainty this facilitate accurate student modeling provide itss learn
satisfiability sat refers task finding truth assignment makes arbitrary boolean expression true this paper compares neural network algorithm nnsat gsat greedy algorithm solving satisfiability problems gsat solve problem instances difficult traditional satisfiability algorithms results suggest nnsat scales better number variables increase solving least many hard sat problems
in last years several researchers within artificial life mobile robotics community used artificial neural networks explicitly viewing neural networks artificial life perspective number consequences make research call artificial life neural networks alnns rather different traditional connectionist research the aim paper make differences alnns classical neural networks explicit
the recognition d objects sequences d views modeled family selforganizing neural architectures called viewnet use view information encoded with networks viewnet incorporates preprocessor generates compressed d invariant representation image supervised incremental learning system fuzzy artmap classifies preprocessed representations d view categories whose outputs combined d invariant object categories working memory makes d object prediction accumulating evidence time d object category nodes multiple d views experienced viewnet benchmarked mit lincoln laboratory database x d views aircraft including small frontal views without additive noise a recognition rate achieved one d view correct three d views the properties d view d object category nodes compared cells monkey inferotemporal cortex
recent interest come deriving various neural network architectures modelling timedependent signals a number algorithms published multilayer perceptrons synapses described finite impulse response fir infinite impulse response iir filters latter case also known locally recurrent globally feedforward networks the derivations algorithms used different approaches calculating gradients note present short unifying account different algorithms compare fir case derivation performance new algorithms subsequently presented simulation results performed benchmark algorithms in note results compared mackeyglass chaotic time series number methods including standard multilayer perceptron local approximation method
this paper presents methodology estimate optimal number learning samples number hidden units needed obtain desired accuracy function approximation feedforward network the representation error generalization error components total approximation error analyzed approximation accuracy feedforward network investigated function number hidden units number learning samples based asymptotical behavior approximation error asymptotical model error function amef introduced parameters determined experimentally an alternative model error function include theoretical results general bounds approximation also analyzed in combination knowledge computational complexity learning rule optimal learning set size number hidden units found resulting minimum computation time given desired precision approximation this approach applied optimize learning camerarobot mapping visually guided robot arm complex logarithm function approximation
we propose methodology bayesian model determination decomposable graphical gaussian models to achieve aim consider hyper inverse wishart prior distribution concentration matrix given graph to ensure compatibility across models prior distributions obtained marginalisation prior conditional complete graph we explore alternative structures hyperparameters latter consequences model model determination carried implementing reversible jump mcmc sampler in particular dimensionchanging move propose involves adding dropping edge graph we characterise set moves preserve decomposability graph giving fast algorithm maintaining junction tree representation graph sweep as state variable propose use incomplete variancecovariance matrix containing elements corresponding element inverse nonzero this allows computations performed locally clique level clear advantage analysis large complex datasets finally statistical computational performance procedure illustrated means artificial real multidimensional datasets
an essential component opportunistic behavior opportunity recognition recognition conditions facilitate pursuit suspended goal opportunity recognition special case situation assessment process sizing novel situation the ability recognize opportunities reinstating suspended problem contexts one way goals manifest design crucial creative design in order deal real world opportunity recognition attribute limited inferential power relevant suspended goals we propose goals suspended working memory monitor internal hidden representations currently recognized objects a suspended goal satisfied current internal representation suspended goal match we propose computational model working memory compare relevant theories opportunistic planning this working memory model implemented part improviser system
technical report umiacstr cstr institute advanced computer studies university maryland college park md abstract one important aspects machine learning paradigm scales according problem size complexity using task known optimal training error prespecified maximum number training updates investigate convergence backpropagation algorithm respect complexity required function approximation b size network relation size required optimal solution c degree noise training data in general solution found worse function approximated complex b oversized networks result lower training generalization error certain cases c use committee ensemble techniques beneficial level noise training data increased for experiments performed obtain optimal solution case we support observation larger networks produce better training generalization error using face recognition example network many parameters training points generalizes better smaller networks
for many reasons neural networks become popular ai machine learning models two important aspects machine learning models well model generalizes unseen data well model scales problem complexity using controlled task known optimal training error investigate convergence backpropagation bp algorithm we find optimal solution typically found furthermore observe networks larger might expected result lower training generalization error this result supported another real world example we investigate training behavior analyzing weights trained networks excess degrees freedom seen little harm aid convergence contrasting interpolation characteristics multilayer perceptron neural networks mlps polynomial models overfitting behavior different mlp often biased towards smoother solutions finally analyze relevant theory outlining reasons significant practical differences these results bring question common beliefs neural network training regarding convergence optimal network size suggest alternate guidelines practical use lower fear excess degrees freedom help direct future work eg methods creation parsimonious solutions importance mlpbp bias possibly worse performance improved training algorithms
this paper presents novel induction algorithm rulearner induces classification rules using galois lattice explicit map search space rules the rulearner system shown compare favorably commonly used symbolic learning methods use heuristics rather explicit map guide search rule space furthermore learning system shown robust presence noisy data the rulearner system also capable learning decision lists unordered rule sets allowing comparisons different learning paradigms within algorithmic framework
keywords casebased reasoning case retrieval case representation this paper deals retrieval useful cases casebased reasoning it focuses questions useful could mean search useful cases organized we present new search algorithm fish shrink able search quickly case base even aspects deflne usefulness spontaneously combined query time we compare fish shrink algorithms show make implicit closed world assumption we flnally refer realization presented idea context prototype fabelproject the scenery follows previously collected cases stored large scaled case base an expert describes problem gives aspects requested case similar the similarity measure thus given spontaneously shall used explore case base within short time shall present required number cases make sure none cases similar the question prepare previously collected cases deflne retrieval algorithm able deal sponta neously userdeflned similarity measures
the parallel genetic algorithm pga uses two major modifications compared genetic algorithm firstly selection mating distributed individuals live d world selection mate done individual independently neighborhood secondly individual may improve fitness lifetime eg local hillclimbing the pga totally asynchronous running maximal efficiency mimd parallel computers the search strategy pga based small number active intelligent individuals whereas ga uses large population passive individuals we investigate pga deceptive problems traveling salesman problem we outline pga succesful abstractly pga parallel search information exchange individuals if represent optimization problem fitness landscape certain configuration space see pga tries jump two local minima third still better local minima using crossover operator this jump probabilistically successful fitness landscape certain correlation we show correlation traveling salesman problem configuration space analysis the pga explores implicitly correlation
the dominant theme casebased research recent ml conferences classifying cases represented feature vectors however useful tasks targeted representations often preferable we review recent literature casebased learning focusing alternative performance tasks expressive case representations we also highlight topics need additional research
current approaches computational lexicology language technology knowledgebased competenceoriented try abstract away specific formalisms domains applications this results severe complexity acquisition reusability bottlenecks as alternative propose particular performanceoriented approach natural language processing based automatic memorybased learning linguistic lexical tasks the consequences approach computational lexicology discussed application approach number lexical acquisition disambiguation tasks phonology morphology syntax described
let h function explicitly defined approximable sequence h n n functional estimators in context propose new sequential algorithm optimise asymptotically h using stepwise estimators h n we prove mild conditions almost sure convergence law algorithm
in paper study new informationtheoretically justified approach missing data estimation multivariate categorical data the approach discussed modelbased imputation procedure relative model class ie functional form probability distribution complete data matrix case set multinomial models independence assumptions based given model class assumption informationtheoretic criterion derived select different complete data matrices intuitively general criterion called stochastic complexity represents shortest code length needed coding complete data matrix relative model class chosen using informationtheoretic criteria missing data problem reduced search problem ie finding data completion minimal stochastic complexity in experimental part paper present empirical results approach using two real data sets compare results achived commonly used techniques case deletion imputating sample averages
we present paper new evolutionary procedure solving general optimization problems combines efficiently mechanisms genetic algorithms tabu search in order explore solution space properly interaction phases interspersed periods optimization algorithm an adaptation search principle national hockey league nhl problem discussed the hybrid method developed paper well suited open shop scheduling problems ossp the results obtained appear quite satisfactory
behavioural observations often described sequence symbols drawn finite alphabet however inductive inference strings automated technique produce models data nontrivial task this paper considers modelling behavioural data using probabilistic finite state automata pfsas there number informationtheoretic techniques evaluating possible hypotheses the measure used paper minimum message length mml wallace although attempts made construct pfsa models incremental addition substrings using heuristic rules mml give lowest information cost resultant models shown globally optimal fogels evolutionary programming produce globally optimal pfsa models evolving data structures arbitrary complexity without requirement encode pfsa binary strings genetic algorithms however evaluation pfsas evolution process mml pfsa alone possible since symbols consumed partially correct solution it suggested addition cant consume symbol symbol alphabet obviates difficulty the addition null symbol alphabet also permits evolution explanatory models need explain data useful property avoid overfitting noisy data results given test set optimal pfsa model known set eye glance data derived instrument panel simulator
technical report cuedfinfengtr we use reversible jump markov chain monte carlo mcmc methods green address problem model order uncertainty autoregressive ar time series within bayesian framework efficient model jumping achieved proposing model space moves full conditional density ar parameters obtained analytically this compared alternative method moves cheaper compute proposals made new parameters move results presented synthetic audio time series
learning viewed problem planning series modifications memory we adopt view learning propose applicability casebased planning methodology task planning learn we argue relatively simple finegrained primitive inferential operators needed support flexible planning we show possible obtain benefits casebased reasoning within planning learn framework
v scbr simple instancebased learning algorithm adjusts weighted similarity measure well collecting cases this paper presents pac analysis v scbr motivated pac learning framework demonstrates two main ideas relevant study instancebased learners firstly hypothesis spaces learner different target concepts compared predict difficulty target concepts learner secondly helpful consider constituent parts instancebased learner explore separately many examples needed infer good similarity measure many examples needed case base applying approaches show v scbr learns quickly variables representation irrelevant target concept slowly relevant variables the paper relates overall behaviour behaviour constituent parts v scbr
partial determinations interesting form dependency attributes relation they generalize functional dependencies allowing exceptions we modify known mdl formula evaluating partial determinations allow use admissible heuristic exhaustive search furthermore describe efficient preprocessingbased approach handling numerical attributes an empirical investigation tries evaluate viability presented ideas
the induction optimal finite state machine explanation symbol strings known least npcomplete however satisfactory approximately optimal explanations may found use evolutionary programming it shown information theoretic measure finite state machine explanations used fitness function required evaluation candidate explanations search nearoptimal explanation it obvious measure class explanation favoured others search by empirical studies possible gain insight dimensions measure optimising in general probabilistic finite state machines explanations assessed minimum message length estimator minimum number transitions favoured explanations the information measure also favour explanations uneven distributions frequencies transitions node suggesting repeated sequences symbol strings preferred explanation approximate bounds acceptance explanations length string required induction successful also derived considerations simplest possible random explanations information measure
how evolutionary process interact decentralized distributed system order produce globally coordinated behavior using genetic algorithm ga evolve cellular automata cas show evolution spontaneous synchronization one type emergent coordination takes advantage underlying mediums potential form embedded particles the particles typically phase defects synchronous regions designed evolutionary process resolve frustrations global phase we describe detail one typical solution discovered ga delineating discovered synchronization algorithm terms embedded particles interactions we also use particlelevel description analyze evolutionary sequence solution discovered our results implications understanding emergent collective behavior natural systems automatic programming decentralized spatially extended multiprocessor systems
we prove general bootstrap theorem possibly infinitedimensional zestimators builds recent infinitedimensional ztheorem due van der vaart our result extends finitedimensional results type bootstrap due arcones gine lele newton raftery we sketch three examples models infinitedimensional parameter spaces fi applicatons general theorem
the prediction survival time recurrence time important learning problem medical domains the recurrence surface approximation rsa method natural effective method predicting recurrence times using censored input data this paper introduces survival curve rsa scrsa extension rsa approach produces accurate predicted rates recurrence maintaining accuracy individual predicted recurrence times the method applied problem breast cancer recurrence using two different datasets
we analyze query committee algorithm method filtering informative queries random stream inputs we show twomember committee algorithm achieves information gain positive lower bound prediction error decreases exponentially number queries we show particular exponential decrease holds query learning perceptrons keywords selective sampling query learning bayesian learning experimental design fl yoav freund room b att bell laboratories mountain ave murray hill nj telephone
a new method performing nonlinear form principal component analysis proposed by use integral operator kernel functions one efficiently compute principal components highdimensional feature spaces related input space nonlinear map instance space possible pixel products fi images we give derivation method present first experimental results polynomial feature extraction pattern recognition
modeling techniques developed recently ai uncertain reasoning communities permit significantly flexible specifications probabilistic knowledge specifically graphical decisionmodeling formalismsbelief networks influence diagrams variantsprovide compact representation probabilistic relationships support inference algorithms automatically exploit dependence structure models these advances brought resurgence interest computational decision systems based normative theories belief preference however graphical decisionmodeling languages still quite limited purposes knowledge representation describe relationships among particular event instances capture general knowledge probabilistic relationships across classes events the inability capture general knowledge serious impediment ai tasks relevant factors decision problem enumerated advance a graphical decision model encodes particular set probabilistic dependencies predefined set decision alternatives specific mathematical form utility function given properly specified model exist relatively efficient algorithms calculating posterior probabilities optimal decision policies a range similar cases may handled parametric variations original model however structure dependencies set available alternatives form utility function changes situation situation fixed network representation longer adequate an ideal computational decision system would possess general broad knowledge domain would ability reason particular circumstances given decision problem within domain one obvious approachwhich call call knowledgebased model construction kbmcis generate decision model dynamically runtime based problem description information received thus far model construction consists selection instantiation assembly causal associational relationships broad knowledge base general relationships among domain concepts for example suppose wish develop system recommend appropriate actions maintaining computer network the natural graphical decision model would include chance
determining conditions given learning algorithm appropriate open problem machine learning methods selecting learning algorithm given domain met limited success this paper proposes new approach predicting given examples class locating example space choosing best learners region example space make predictions the regions example space defined prediction patterns learners used the learners chosen prediction selected according past performance region this dynamic approach learning algorithm selection compared methods selecting multiple learning algorithms the approach extended weight rather select algorithms according past performance given region both approaches evaluated set determining conditions given learning algorithm appropriate open problem machine learning methods selecting learning algorithm given domain eg aha breiman portion domain brodley brodley met limited success this paper proposes new approach dynamically selects learning algorithm example locating example space choosing best learners prediction part example space the regions example space formed observed prediction patterns learners used the learners chosen prediction selected according past performance region defined crossvalidation history this paper introduces ds method dynamic selection learning algorithms we call dynamic learning algorithms used classify novel example depends example preliminary experimentation motivated dw extension ds dynamically weights learners predictions according regional accuracy further experimentation compares ds dw collection metalearning strategies crossvalidation breiman various forms stacking wolpert in phase experiementation metalearners six constituent learners heterogeneous search representation methods eg rule learner cn clark decision tree learner c quinlan oblique decision tree learner oc murthy instancebased learner pebls cost knearest neighbor learner ten domains compared several metalearning strategies
tw important issues machine learning explored role memory plays acquiring new concepts extent learner take active part acquiring concepts this chapter describes program called marvin uses concepts learned previously learn new concepts the program forms hypotheses concept learned tests hypotheses asking trainer questions learning begins trainer shows marvin example concept learned the program determines objects example belong concepts stored memory a description new concept formed using information obtained memory generalize description training example the generalized description tested program constructs new examples shows trainer asking belong target concept
adaptation ecological systems environments commonly viewed explicit fitness function defined priori experimenter measured posteriori estimations based population size andor reproductive rates these methods capture role environmental complexity shaping selective pressures control adaptive process ecological simulations enabled computational tools latent energy environments lee model allow us characterize closely effects environmental complexity evolution adaptive behaviors lee described paper its motivation arises need vary complexity controlled predictable ways without assuming relationship changes adaptive behaviors engender this goal achieved careful characterization environments different forms energy welldefined a genetic algorithm using endogenous fitness local selection used model evolutionary process individuals population modeled neural networks simple sensorymotor systems variations behaviors related interactions varying environments we outline results three experiments analyze different sources environmental complexity effects collective behaviors evolving populations
in paper investigate efficiency subsumption basic provability relation ilp as d c npcomplete even restrict linked horn clauses fix c contain small constant number literals investigate several restrictions d we first adapt notion determinate clauses used ilp show subsumption decidable polynomial time d determinate respect c secondly adapt notion klocal horn clauses show subsumption efficiently computable reasonably small k we show results combined give efficient reasoning procedure determinate klocal horn clauses ilpproblem recently suggested polynomial predictable cohen simple counting argument we finally outline reduction algorithm essential part every lgg ilplearning algorithm im proved ideas
bbn technical report abstract genetic programming powerful method automatically generating computer programs via process natural selection koza however limitation known closure ie variables constants arguments functions values returned functions must data type to correct deficiency introduce variation genetic programming called strongly typed genetic programming stgp in stgp variables constants arguments returned values data type provision data type value specified beforehand this allows initialization process genetic operators generate syntactically correct parse trees key concepts stgp generic functions true strongly typed functions rather templates classes functions generic data types analogous to illustrate stgp present four examples involving vectormatrix manipulation list manipulation multidimensional leastsquares regression problem multidimensional kalman filter list manipulation function nth list manipulation function mapcar
category algorithms architectures recurrent networks no part paper submitted elsewhere preference poster abstract existing proofs demonstrating computational limitations recurrent cascade correlation rcc network fahlman explicitly limit results units sigmoidal hardthreshold transfer functions giles et al kremer the proof given shows given finite discrete deterministic transfer function used units rcc network finitestate automata fsa network model matter many units used the proof applies equally well continuous transfer functions finite number fixedpoints sigmoid function
subsumption decidable incomplete approximation logic implication important inductive logic programming theorem proving we show context based elimination possible matches certain superset determinate clauses tested subsumption polynomial time we discuss relation subsumption clique problem showing particular using additional prior knowledge substitution space small fraction search space identified possibly containing globally consistent solutions leads effective pruning rule we present empirical results demonstrating combination approaches provides extreme reduction computational effort
we introduce new algorithm designed learn sparse perceptrons input representations include highorder features our algorithm based hypothesisboosting method able paclearn relatively natural class target concepts moreover algorithm appears work well practice set three problem domains algorithm produces classifiers utilize small numbers features yet exhibit good generalization performance perhaps importantly algorithm generates concept descriptions easy humans understand
current inductive machine learning algorithms typically use greedy search limited lookahead this prevents detect significant conditional dependencies attributes describe training objects instead myopic impurity functions lookahead propose use relieff extension relief developed kira rendell heuristic guidance inductive learning algorithms we reimplemented assistant system top induction decision trees using relieff estimator attributes selection step the algorithm tested several artificial several real world problems results compared well known machine learning algorithms excellent results artificial data sets two real world problems show advantage presented approach inductive learning
this paper presents new approach hierarchical reinforcement learning based maxq decomposition value function the maxq decomposition procedural semanticsas subroutine hierarchyand declarative semanticsas representation value function hierarchical policy maxq unifies extends previous work hierarchical reinforcement learning singh kaelbling dayan hinton conditions maxq decomposition represent optimal value function derived the paper defines hierarchical q learning algorithm proves convergence shows experimentally learn much faster ordinary flat q learning finally paper discusses interesting issues arise hierarchical reinforcement learning including hierarchical credit assignment problem nonhierarchical execution maxq hierarchy
causality relates changes structure object effects changes changes properties behavior object this paper analyzes concept causality genetic programming gp suggests used adapting control parameters speeding gp search we first analyze effects crossover show weak causality gp representation operators hierarchical gp approaches based discovery evolution functions amplify phenomenon however selection gradually retains strongly causal changes causality correlated search space exploitation discussed context explorationexploitation tradeoff the results described argue bottomup gp evolutionary thesis finally new developments based idea gp architecture evolution koza discussed causality perspective
methods voting classification algorithms bagging adaboost shown successful improving accuracy certain classifiers artificial realworld datasets we review algorithms describe large empirical study comparing several variants conjunction decision tree inducer three variants naivebayes inducer the purpose study improve understanding algorithms use perturbation reweighting combination techniques affect classification error we provide bias variance decomposition error show different methods variants influence two terms this allowed us determine bagging reduced variance unstable methods boosting methods adaboost arcx reduced bias variance unstable methods increased variance naivebayes stable we observed arcx behaves differently adaboost reweighting used instead resampling indicating fundamental difference voting variants introduced paper include pruning versus pruning use probabilistic estimates weight perturbations wagging backfitting data we found bagging improves probabilistic estimates conjunction nopruning used well data backfit we measure tree sizes show interesting positive correlation increase average tree size adaboost trials success reducing error we compare meansquared error voting methods nonvoting methods show voting methods lead large significant reductions meansquared errors practical problems arise implementing boosting algorithms explored including numerical instabilities underflows we use scatterplots graphically show adaboost reweights instances emphasizing hard areas also outliers noise
the longterm goal field creation understanding intelligence productive research ai practical theoretical benefits notion intelligence precise enough allow cumulative development robust systems general results the concept rational agency long considered leading candidate fulfill role this paper outlines gradual evolution formal conception rationality brings closer informal conception intelligence simultaneously reduces gap theory practice some directions future research indicated
a solution problem representing compositional structure using distributed representations described the method uses circular convolution associate items represented vectors arbitrary variable bindings short sequences various lengths frames reduced representations compressed fixed width vector these representations items right used constructing compositional structures the noisy reconstructions given convolution memories cleaned using separate associative memory good reconstructive properties
based angluins l fl algorithm the algorithm maintains model consistent past examples when new counterexample arrives tries extend model minimal fashion we conducted set experiments random automata represent different strategies generated algorithm tried learn based prefixclosed samples behavior the algorithm managed learn compact models agree samples the size sample small effect size model the experimental results suggest random prefixclosed samples algorithm behaves well however following angluins result difficulty learning almost uniform complete samples angluin obvious algorithm solve complexity issue inferring dfa general prefixclosed sample we currently looking classes prefixclosed samples usl behaves well carmel markovitch d carmel s markovitch the m algorithm incorporating opponent models adversary search technical report cis report technion march carmel markovitch d carmel s markovitch unsupervised learning finite automata a practical approach technical report cis report technion march shoham tennenholtz y shoham m tennenholtz colearning evolution social activity technical report stancstr stanford univrsity department computer science
aa incremental learning algorithm adaptive selforganizing concurrent systems asocs asocs selforganizing dynamically growing networks computing nodes aa learns discrimination implements knowledge distributed fashion nodes this paper reviews aa perspective convergence generalization a formal proof aa converges arbitrary boolean instance set given a discussion generalization aspects aa including problem handling inconsistency follows results simulations realworld data presented they show aa gives promising generalization
the term bias widely usedand different meaningsin fields machine learning statistics this paper clarifies uses term shows measure visualize statistical bias variance learning algorithms statistical bias variance applied diagnose problems machine learning bias paper shows four examples finally paper discusses methods reducing bias variance methods based voting reduce variance paper compares breimans bagging method tree randomization method voting decision trees both methods uniformly improve performance data sets irvine repository tree randomization yields perfect performance letter recognition task a weighted nearest neighbor algorithm based infinite bootstrap also introduced in general decision tree algorithms moderatetohigh variance important implication work variancerather appropriate inappropriate machine learning biasis important cause poor performance decision tree algorithms
we analyze use builtin policies macroactions form domain knowledge improve speed scaling reinforcement learning algorithms such macroactions often used robotics macrooperators also wellknown aid statespace search ai systems the macroactions consider closedloop policies termination conditions the macroactions chosen level primitive actions macroactions commit learning agent act particular purposeful way sustained period time overall macroactions may either accelerate retard learning depending appropriateness macroactions particular task we analyze effect simple example breaking acceleration effect two parts effect macroaction changing exploratory behavior independent learning effect macroaction learning independent effect behavior in example effects significant latter appears larger finally provide complex gridworld illustration appropriately chosen macroactions accelerate overall learning
we present new approach reinforcement learning policies considered learning process constrained hierarchies partially specified machines this allows use prior knowledge reduce search space provides framework knowledge transferred across problems component solutions recombined solve larger complicated problems our approach seen providing link reinforcement learning behaviorbased teleoreactive approaches control we present provably convergent algorithms problemsolving learning hierarchical machines demonstrate effectiveness problem several thousand states
when casebased planner retrieving previous case preparation solving new similar problem often aware implicit features new problem situation determine particular case may successfully applied this means cases may retrieved error case may fail improve planners performance retrieval may incrementally improved detecting explaining failures occur in paper provide definition case failure planner dersnlp derivation replay snlp solves new problems replaying previous plan derivations we provide ebl explanationbased learning techniques detecting constructing reasons failure we also describe organize case library incorporate failure information produced finally present empirical study demonstrates effectiveness approach improving performance dersnlp
this paper presents new methods training large neural networks phoneme probability estimation an architecture combining timedelay windows recurrent connections used capture important dynamic information speech signal because number connections fully connected recurrent network grows superlinear number hidden units schemes sparse connection connection pruning explored it found sparsely connected networks outperform fully connected counterparts equal number connections the implementation combined architecture training scheme described detail the networks evaluated hybrid hmmann system phoneme recognition timit database word recognition waxholm database the achieved phone errorrate standard phoneme set core testset timit database range lowest reported all training simulation software used made freely available author detailed information software training process given appendix
the error rate decisiontree classification learners often much reduced bagging learning multiple models bootstrap samples database combining uniform voting in paper empirically test two alternative explanations based bayesian learning theory bagging works approximation optimal procedure bayesian model averaging appropriate implicit prior bagging works effectively shifts prior appropriate region model space all experimental evidence contradicts first hypothesis confirms second bagging breiman simple effective way reduce error rate many classification learning algorithms for example empirical study described reduces error decisiontree learner databases average in bagging procedure given training set size bootstrap replicate constructed taking samples replacement training set thus new training set size produced original examples may appear on average original examples appear bootstrap sample the learning algorithm applied training set this procedure repeated times resulting models aggregated uniform voting bagging one several multiple model approaches recently received much attention see example chan stolfo wolpert other procedures type include boosting freund schapire stacking wolpert
we propose algorithm called query committee committee students trained data set the next query chosen according principle maximal disagreement the algorithm studied two toy models highlow game perceptron learning another perceptron as number queries goes infinity committee algorithm yields asymptotically finite information gain this leads generalization error decreases exponentially number examples this marked contrast learning randomly chosen inputs information gain approaches zero generalization error decreases relatively slow inverse power law we suggest asymptotically finite information gain may important characteristic good query algorithms
tech report department statistics open university walton hall mk aa uk tech report department computer science monash university clayton vic australia abstract this paper examines minimum encoding approaches inference minimum message length mml minimum description length mdl this paper written objective providing introduction area statisticians we describe coding techniques data examine techniques applied perform inference model selection
field suggested neurons line edge selectivities found primary visual cortex cats monkeys form sparse distributed representation natural scenes barlow reasoned responses emerge unsupervised learning algorithm attempts find factorial code independent visual features we show nonlinear infomax applied ensemble natural scenes produces sets visual filters localised oriented some filters gaborlike resemble produced sparsenessmaximisation network olshausen field in addition outputs filters independent possible since infomax network able perform independent components analysis ica we compare resulting ica filters associated basis functions decorrelating filters produced principal components analysis pca zerophase whitening filters zca the ica filters sparsely distributed kurtotic outputs natural scenes they also resemble receptive fields simple cells visual cortex suggests neurons form informationtheoretic coordinate system images
loan applications banks often long requiring applicant provide large amounts data is necessary can save applicant frustration bank expense using subset relevant variables to answer question i attempted model current loan approval process particular bank i used several model selection techniques logistic regression including stepwise regression occams window markov chain monte carlo model composition raftery madigan hoeting bayesian random searching the resulting models largely agree upon subset onethird original variables fl this paper completed partial fulfillment phd data analysis requirement
learning planning representing knowledge multiple levels temporal abstraction key challenges ai in paper develop approach problems based mathematical framework reinforcement learning markov decision processes mdps we extend usual notion action include optionswhole courses behavior may temporally extended stochastic contingent events examples options include picking object going lunch traveling distant city well primitive actions muscle twitches joint torques options may given priori learned experience they may used interchangeably actions variety planning learning methods the theory semimarkov decision processes smdps applied model consequences options basis planning learning methods using in paper develop connections building prior work bradtke duff parr prep others our main novel results concern interface mdp smdp levels analysis we show set options altered changing termination conditions improve smdp methods additional cost we also introduce intraoption temporaldifference methods able learn fragments options execution finally propose notion subgoal used improve options overall argue options models provide hitherto missing aspects powerful clear expressive framework representing organizing knowledge
articles neural network learning algorithms published examined amount experimental evaluation contain employ even single realistic real learning problem only articles present results one problem using real world data furthermore one third articles present quantitative comparison previously known algorithm these results suggest strive better assessment practices neural network learning algorithm research for longterm benefit field publication standards raised respect easily accessible collections benchmark problems built
technical report number cs computer science engineering ucsd abstract the developmental mechanisms transforming genotypic phenotypic forms typically omitted formulations genetic algorithms gas two representational spaces identical we argue careful analysis developmental mechanisms useful understanding success several standard ga techniques clarify relationships recently proposed enhancements we provide framework distinguishes two developmental mechanisms learning maturation also showing several common effects ga search this framework used analyze maturation local search change dynamics ga we observe contexts maturation local search incorporated fitness evaluation illustrate reasons considering seperately further identify contexts maturation local search distinguished fitness evaluation
finding optimal least good monitoring strategies important consideration designing agent we applied genetic programming task mixed results since agent control language kept purposefully general set monitoring strategies constitutes small part overall space possible behaviors because often difficult genetic algorithm evolve even though performance superior these results raise questions easy genetic programming scale areas applied become complex
marketing decision making tasks require acquisition efficient decision rules noisy questionnaire data unlike popular learningfromexample methods tasks must interpret characteristics data without clear features data predetermined evaluation criteria the problem domain experts get simple easytounderstand accurate knowledge noisy data this paper describes novel method acquire efficient decision rules questionnaire data using simulated breeding inductive learning techniques the basic ideas method simulated breeding used get effective features questionnaire data inductive learning used acquire simple decision rules data the simulated breeding one genetic algorithm based techniques subjectively interactively evaluate qualities offspring generated genetic operations the proposed method qualitatively quantitatively validated case study consumer product questionnaire data acquired rules simpler results direct application inductive learning domain expert admits easy understand level accuracy compared methods
this paper experimentally compares three approaches program induction inductive logic programming ilp genetic programming gp genetic logic programming glp variant gp inducing prolog programs each methods used induce four simple recursive listmanipulation functions the results indicate ilp likely induce correct program small sets random examples gp generally less accurate glp performs worst rarely able induce correct program interpretations results terms differences search methods inductive biases presented keywords genetic programming inductive logic programming empiri cal comparison this paper also submitted th int workshop inductive logic programming
two major problems casebased reasoning efficient justified retrieval source cases adaptation retrieved solutions conditions target for analogical theorem proving induction describe solutionrelevant abstraction restrict retrieval source cases mapping source problem target problem determine reformulations adapt source solution
most commonly casebased reasoning applied domains attribute value representations cases sufficient represent features relevant support classification diagnosis design tasks distance functions like hammingdistance transformation similarity functions applied retrieve past cases used generate solution actual problem often domain knowledge available adapt past solutions new problems evaluate solutions however domains like architectural design law structural case representations corresponding structural similarity functions needed often acquisition adaptation knowledge seems impossible rather requires effort manageable fielded applications despite humans use cases main source generate adapted solutions how achieve computationally this paper presents general approach structural similarity assessment adaptation the approach allows explore structural case representations limited domain knowledge support design tasks it exemplarily instantiated three modules design assistant fabelidea generates adapted design solutions basis prior cad layouts
the main difficulty implementing natural gradient learning rule compute inverse fisher information matrix input dimension large we found new scheme represent fisher information matrix based scheme designed algorithm compute inverse fisher information matrix when input dimension n much larger number hidden neurons complexity algorithm order on complexity conventional algorithms purpose order on the simulation confirmed efficience robustness natural gradient learning rule
the ability casebased reasoning cbr systems apply cases novel situations depends case adaptation knowledge however endowing cbr systems adequate adaptation knowledge proven difficult task this paper describes hybrid method performing case adaptation using combination rulebased casebased reasoning it shows approach provides framework acquiring flexible adaptation knowledge experiences autonomous adaptation suggests potential basis acquisition adaptation knowledge interactive user guidance it also presents initial experimental results examining benefits approach comparing relative contributions case learning adaptation learning reasoning performance
this paper discusses traditional reinforcement learning methods algorithms applied models result poor performance dynamic situated multiagent domains characterized multiple goals noisy perception action inconsistent reinforcement we propose methodology designing representation forcement functions take advantage implicit domain knowledge order accelerate learning domains demonstrate experimentally two different mobile robot domains
learning problem solving intimately related problem solving determines knowledge requirements reasoner learning must fulfill learning enables improved problemsolving performance different models problem solving however recognize different knowledge needs result set different learning tasks some recent models analyze problem solving terms generic tasks methods subtasks these models require learning problemsolving concepts new tasks new task decompositions we view reflection core process learning problemsolving concepts in paper identify learning issues raised taskstructure framework problem solving we view problem solver abstract device represent works terms structurebehaviorfunction model specifies knowledge reasoning problem solver results accomplishment tasks we describe model enables reflection modelbased reflection enables reasoner adapt task structure produce solutions better quality the autognostic system illustrates reflection process
realistic complex planning situations require mixedinitiative planning framework human automated planners interact mutually construct desired plan ideally joint cooperation potential achieving better plans either human machine create alone human planners often take casebased approach planning relying past experience planning retrieving adapting past planning cases planning analogical reasoning generative casebased planning combined prodigyanalogy provides suitable framework study mixedinitiative integration however human user engaged planning loop creates variety new research questions the challenges found creating mixedinitiative planning system fall three categories planning paradigms differ human machine planning visualization plan planning process complex necessary task human users range across spectrum experience respect planning domain underlying planning technology this paper presents approach three problems designing interface incorporate human process planning analogical reasoning prodigyanalogy the interface allows user follow generative casebased planning supports visualization plan planning rationale addresses variance experience user allowing user control presentation information
evolutionary programming evolution strategies rather similar representatives class probabilistic optimization algorithms gleaned model organic evolution discussed compared respect similarities differences basic components well performance experimental runs theoretical results global convergence step size control strictly convex quadratic function extension convergence rate theory evolution strategies presented discussed respect implications evolutionary programming
a control law constructed linear time varying system solving two player zero sum differential game moving horizon game used construct h controller finite horizon conditions given controller results stable system satisfies infinite horizon h norm bound a risk sensitive formulation used provide state estimator observation feedback case
in paper investigate genetic algorithms two parents involved recombination operation in particular introduce gene scanning reproduction mechanism generalizes classical crossovers npoint crossover uniform crossover applicable arbitrary number two parents we performed extensive tests optimizing numerical functions tsp graph coloring observe effect different numbers parents the experiments show parent recombination outperformed using parents classical dejong functions for problems results conclusive cases parents optimal others parents better
a novel method proposed combining multiple probabilistic classifiers different feature sets in order achieve improved classification performance generalized finite mixture model proposed linear combination scheme implemented based radial basis function networks in linear combination scheme soft competition different feature sets adopted automatic feature rank mechanism different feature sets always simultaneously used optimal way determine linear combination weights for training linear combination scheme learning algorithm developed based expectationmaximization em algorithm the proposed method applied typical real world problem viz speaker identification different feature sets often need consideration simultaneously robustness simulation results show proposed method yields good performance speaker identification
different learning models employ different styles generalization novel inputs this paper proposes need multiple styles generalization support broad application base the priority asocs model priority adaptive selforganizing concurrent system overviewed presented potential platform support multiple generalization styles pasocs adaptive network composed many simple computing elements operating asynchronously parallel the pasocs operate either data processing mode learning mode during data processing mode system acts parallel hardware circuit during learning mode pasocs incorporates rules attached priorities represent application learned learning accomplished distributed fashion time logarithmic number rules the new model significant learning time space complexity improvements previous models generalization learning system best always guess the proper style generalization application dependent thus one style generalization may sufficient allow learning system support broad spectrum applications current connectionist models use one specific style generalization implicit learning algorithm we suggest type generalization used selforganizing parameter learning system discovered learning takes place this requires model allows flexible generalization styles b mechanisms guide system best style generalization problem learned this paper overviews learning model seeks efficiently support requirement the model called priority asocs pasocs member class models called asocs adaptive selforganizing concurrent systems section paper gives example different generalization techniques approach problem section presents overview pasocs section illustrates flexible generalization supported section concludes paper
we introduce new approach model selection performs better standard complexitypenalization holdout error estimation techniques many cases the basic idea exploit intrinsic metric structure hypothesis space determined natural distribution unlabeled training patterns use metric reference detect whether empirical error estimates derived small labeled training sample trusted region around empirically optimal hypothesis using simple metric intuitions develop new geometric strategies detecting overfitting performing robust yet responsive model selection spaces candidate functions these new metricbased strategies dramatically outperform previous approaches experimental studies classical polynomial curve fitting moreover technique simple efficient applied function learning tasks the requirement access auxiliary collection unlabeled training data
in paper use genetic algorithm evolve set classification rules realvalued attributes we show realvalued attribute ranges encoded realvalued genes present new uniform method representing dont cares rules we view supervised classification optimization problem evolve rule sets maximize number correct classifications input instances we use variant pitt approach geneticbased machine learning system novel conflict resolution mechanism competing rules within rule set experimental results demonstrate effectiveness proposed approach benchmark wine classifier system
genetic algorithms proven powerful tool within area machine learning however classes problems seem scarcely applicable eg solution given problem consists several parts influence in case classic genetic operators crossover mutation work well thus preventing good performance this paper describes approach overcome problem using highlevel genetic operators integrating task specific domain independent knowledge guide use operators the advantages approach shown learning rule base adapt parameters image processing operator path within solution system
in sparse data environments greater classification accuracy achieved learning several concept descriptions data combining classifications stochastic search general tool used generate many good concept descriptions rule sets class data bayesian probability theory offers optimal strategy combining classifications individual concept descriptions use approximation theory this strategy useful additional data difficult obtain every increase classification accuracy important the primary result paper multiple concept descriptions particularly helpful flat hypothesis spaces many equally good ways grow rule similar gain another result experimental evidence learning multiple rule sets yields accurate classifications learning multiple rules domains to demonstrate behaviors learn multiple concept descriptions adapting hydra noisetolerant relational learning algorithm
in paper present novel multiagent learning paradigm called teampartitioned opaquetransition reinforcement learning tpotrl tpotrl introduces concept using actiondependent features generalize state space in work use learned actiondependent feature space tpotrl effective technique allow team agents learn cooperate towards achievement specific goal it adaptation traditional rl methods applicable complex nonmarkovian multiagent domains large state spaces limited training opportunities multiagent scenarios opaquetransition team members always full communication one another adversaries may affect environment hence learner rely knowledge future state transitions acting world tpotrl enables teams agents learn effective policies training examples even face large state space large amounts hidden state the main responsible features dividing learning task among team members using coarse actiondependent feature space allowing agents gather reinforcement directly observation environment tpotrl fully implemented tested robotic soccer domain complex multiagent framework this paper presents algorithmic details tpotrl well empirical results demonstrating effectiveness developed multiagent learning approach learned features
this paper discusses method training multilayer perceptron networks called dmp dynamic multilayer perceptron the method based upon divide conquer approach builds networks form binary trees dynamically allocating nodes layers needed the focus paper effects using multiple node types within dmp framework simulation results show dmp performs favorably comparison learning algorithms using multiple node types beneficial network performance
specification refinement part formal program derivation method software directly constructed provably correct specification because program derivation intensive manual exercise used critical software systems automated approach would allow viable many types software systems the goal research determine genetic programming gp used automate specification refinement process the initial steps toward goal show wellknown proof logic program derivation encoded gpbased system infer sentences logic proof particular sentence the results promising indicate gp useful aiding pro gram derivation
this paper appears chapter kenneth e kinnear jr peter j angeline editors advances genetic programming mit press abstract genetic programming gp automatic method generating computer programs stored data structures manipulated evolve better programs an extension restricting search space strongly typed genetic programming stgp basic premise removal closure typing arguments return values functions also typing terminal set a restriction stgp two levels typing we extend stgp allowing type hierarchy allows two levels typing
we integrated distributed search genetic programming based systems collective memory form collective adaptation search method such system significantly improves search problem complexity increased however still considerable scope improvement in collective adaptation search agents gather knowledge environment deposit central information repository process agents able manipulate focused knowledge exploiting exploration search agents we examine utility increasing capabilities centralized pro cess agents
our casebased reasoning cbr integration constraint satisfaction problem csp formalism undergone several transformations journey initial research idea productintent design both unexpected research results well interesting insights realworld applicability integrated methodology emerged integration explored alternative viewpoints in paper alternative viewpoints results enabled viewpoints described
in order rank performance machine learning algorithms many researchers conduct experiments benchmark data sets since learning algorithms domainspecific parameters popular custom adapt parameters obtain minimal error rate test set the rate used rank algorithm causes optimistic bias we quantify bias showing particular algorithm parameters probably ranked higher equally good algorithm fewer parameters we demonstrate result showing number parameters trials required order pretend outperform c foil respectively various benchmark problems we describe unbiased ranking experiments conducted
we report series experiments decision trees consistent training data constructed these experiments run gain understanding properties set consistent decision trees factors affect accuracy individual trees in particular investigated relationship size decision tree consistent training data accuracy tree test data the experiments performed massively parallel maspar computer the results experiments several artificial two real world problems indicate many problems investigated smaller consistent decision trees average less accurate average accuracy slightly larger trees
an ensemble consists set independently trained classifiers neural networks decision trees whose predictions combined classifying novel instances previous research shown ensemble whole often accurate single classifiers ensemble bagging breiman boosting freund schapire two relatively new popular methods producing ensembles in paper evaluate methods using neural networks decision trees classification algorithms our results clearly show two important facts the first even though bagging almost always produces better classifier individual component classifiers relatively impervious overfitting generalize better baseline neuralnetwork ensemble method the second boosting powerful technique usually produce better ensembles bagging however susceptible noise quickly overfit data set
pruning decision tree considered researchers important part tree building noisy domains while many approaches pruning alternative approach averaging decision trees received much attention we perform empirical comparison pruning approach averaging decision trees for comparison use computationally efficient method averaging namely averaging extended fanned set tree since wide range approaches pruning compare tree averaging traditional pruning approach along optimal pruning approach
a widely held idea regarding information processing brain cellassembly hypothesis suggested hebb according hypothesis basic unit information processing brain assembly cells act briefly closed system response specific stimulus this work presents novel method characterizing supposed activity using hidden markov model this model able reveal underlying cortical network activity behavioral processes in study process hand simultaneous activity several cells recorded frontal cortex behaving monkeys using model able identify behavioral mode animal directly identify corresponding collective network activity furthermore segmentation data discrete states also provides direct evidence state dependency shorttime correlation functions pair cells thus crosscorrelation depends network state activity local connectivity alone
we consider problem model selection accounting model uncertainty highdimensional contingency tables motivated expert system applications the approach used currently stepwise strategy guided tests based approximate asymptotic p values leading selection single model inference conditional selected model the sampling properties strategy complex failure take account model uncertainty leads underestimation uncertainty quantities interest in principle panacea provided standard bayesian formalism averages posterior distributions quantity interest models weighted posterior model probabilities furthermore approach optimal sense maximising predictive ability however used practice computing posterior model probabilities hard number models large often greater we argue standard bayesian formalism unsatisfactory propose alternative bayesian approach contend takes full account true model uncertainty averaging much smaller set models an efficient search algorithm developed finding models we consider two classes graphical models arise expert systems recursive causal models decomposable fl david madigan assistant professor statistics adrian e raftery professor statistics sociology department statistics gn university washington seattle wa madigans research partially supported graduate school research fund university washington nsf rafterys research supported onr contract nj the authors grateful gregory cooper leo goodman shelby haberman david hinkley graham upton jon wellner nanny wermuth jeremy york walter zucchini two anonymous referees helpful comments discussions michael r butler providing data scrotal swellings example
z yorks research supported nsf graduate fellowship the authors grateful julian besag david bradshaw jeff bradshaw james carlsen david draper ivar heuch robert kass augustine kong steffen lauritzen adrian raftery james zidek helpful comments discussions
we present automated emotion recognition system capable identifying six basic emotions happy surprise sad angry fear disgust novel face images an ensemble simple feedforward neural networks used rate images the outputs networks combined generate score emotion the networks trained database face images human subjects consistently rated portraying single emotion such system achieves generalization novel face images individuals networks trained drawn database the neural network model exhibits categorical perception emotion pairs a linear sequence morph images created two expressions individuals face sequence analyzed model sharp transitions output response vector occur single step sequence emotion pairs others we plan us models response limit direct testing determining human subjects exhibit categorical perception morph image sequences
we discuss advantages using overdetermined mixtures improve upon blind source separation algorithms designed extract sound sources acoustic mixtures a study nature room impulse responses helps us choose adaptive filter architecture we use ideal inverses acquired room impulse responses compare effectiveness differentsized separating filter configurations various filter lengths using multichannel blind leastmeansquare algorithm mblms show adding additional sensors improve upon separation signals mixed real world filters
rissanens minimum description length mdl principle adapted handle continuous attributes inductive logic programming setting application developed coding mdl pruning mechanism devised the behavior mdl pruning tested synthetic domain artificially added noise different levels two real life problems modelling surface roughness grinding workpiece modelling mutagenicity nitroaromatic compounds results indicate mdl pruning successful parameterfree noise fighting tool reallife domains since acts safeguard building complex models retaining accuracy model
we address difficult problem separating multiple speakers multiple microphones real room we combine work torkkola amari cichocki yang give natural gradient information maximisation rules recurrent iir networks blindly adjusting delays separating deconvolving mixed signals while work well simulated data rules fail real rooms usually involve nonminimum phase transfer functions notinvertible using stable iir filters an approach sidesteps problem perform infomax feedforward architecture frequency domain lambert we demonstrate realroom separation two natural signals using approach
for blind source separation fisher information matrix used riemannian metric tensor parameter space steepest descent algorithm maximize likelihood function riemannian parameter space becomes serial updating rule equivariant property this algorithm simplified using asymptotic form fisher information matrix around equilibrium
we discovered new scheme represent fisher information matrix stochastic multilayer perceptron based scheme designed algorithm compute inverse fisher information matrix when input dimension n much larger number hidden neurons complexity algorithm order on complexity conventional algorithms purpose order on the inverse fisher information matrix used natural gradient descent algorithm train singlelayer multilayer perceptrons it confirmed simulation natural gradient
in paper define task place learning describe one approach problem our framework represents distinct places evidence grids probabilistic description occupancy place recognition relies nearest neighbor classification augmented registration process correct translational differences two grids the learning mechanism lazy involves simple storage inferred evidence grids experimental studies physical simulated robots suggest approach improves place recognition experience handle significant sensor noise benefits improved quality stored cases scales well environments many distinct places additional studies suggest using historical information robots path environment actually reduce recognition accuracy previous researchers studied evidence grids place learning combined two powerful concepts used systematic experimentation evaluate methods abilities
let us call nondeterministic incremental algorithm one able construct solution combinatorial problem selecting incrementally ordered sequence choices defines solution choice made nondeterministically in case state space represented tree solution path root tree leaf this paper describes simulated evolution population nondeterministic incremental algorithms offers new approach exploration state space compared techniques like genetic algorithms ga evolutionary strategies es hill climbing in particular efficiency method implemented evolving nondeterminism end model presented sorting network problem reference problem challenged computer science then shall show end model remedies drawbacks optimization techniques even outperforms problem indeed input sorting networks good best known built scratch even yearold result input problem improved one comparator
visor neural network system object recognition scene analysis learns visual schemas examples processing visor based cooperation competition parallel bottomup topdown activation schema representations similar principles appear underlie much human visual processing visor therefore used model various perceptual phenomena this paper focuses analyzing three phenomena simulation visor priming mental imagery perceptual reversal circular reaction the results illustrate similarity subtle differences mechanisms mediating priming mental imagery show two opposing accounts perceptual reversal neural satiation cognitive factors may contribute phenomenon demonstrate intentional actions gradually learned reflex actions successful simulation effects suggests similar mechanisms may govern human visual perception learning visual schemas
a novel approach object recognition scene analysis based neural network representation visual schemas described given input scene visor system focuses attention successively component schema representations cooperate compete match inputs the schema hierarchy learned examples unsupervised adaptation reinforcement learning visor learns objects important others identifying scene importance spatial relations varies depending scene as inputs differ increasingly schemas visors recognition process remarkably robust automatically generates measure confidence analysis
truly autonomous vehicles require projec tive planning reactive components order perform robustly projective components needed longterm planning replanning explicit reasoning future states required reactive components allow system always action available realtime exhibit robust behavior lack ability expli citly reason future states long time period this work addresses problem creating reactive components autonomous vehicles creating reactive behaviors stimulusresponse rules generally difficult requiring acquisition much knowledge domain experts problem referred knowledge acquisition bottleneck samuel system learns reactive behaviors autonomous agents samuel learns behaviors simulation automating process creating stimulusresponse rules therefore reducing bottleneck the learning algorithm designed learn useful behaviors simulations limited fidelity current work investigating well behaviors learned simulation environments work real world environments in paper describe samuel describe behaviors learned simulated autonomous aircraft autonomous underwater vehicles robots these behaviors include dog fighting missile evasion track ing navigation obstacle avoidance
feedforward nets sigmoidal activation functions often designed minimizing cost criterion it pointed technique may outperformed classical perceptron learning rule least problems in paper show pathologies arise error criterion threshold lms type ie zero values beyond desired target values more precisely show data linearly separable one considers nets hidden neurons error function local minima global simulations networks hidden units consistent results often data classified minimizing threshold lms criterion may fail classified using instead simple lms cost in addition proof gives following stronger result stated hypotheses continuous gradient adjustment procedure initial weight configuration separating set weights obtained finite time this precise analogue perceptron learning theorem the results compared classical pattern recognition problem threshold lms linear activations spurious local minima exist even nonseparable data shown even using threshold criterion bad local minima may occur data separable sigmoids used
this paper combines existing models longitudinal spatial data hierarchical bayesian framework particular emphasis role time spacevarying covariate effects data analysis implemented via markov chain monte carlo methods the methodology illustrated tentative reanalysis ohio lung cancer data two approaches adjust unmeasured spatial covariates particularly tobacco consumption described the first includes random effects model account unobserved heterogeneity second adds simple urbanization measure surrogate smoking behaviour the ohio dataset particular interest suggestion nuclear facility southwest state may caused increased levels lung cancer however contend data inadequate proper investigation issue fl email leostatunimuenchende
although many algorithms learning examples developed many comparisons reported generally accepted benchmark classifier learning the existence standard benchmark would greatly assist comparisons sixteen dimensions proposed describe classification tasks based thirteen realworld synthetic datasets chosen set covering method uci repository machine learning databases form benchmark
hollands schema theorem widely taken foundation explanations power genetic algorithms gas yet dissent expressed implications here dissenting arguments reviewed elaborated upon explaining schema theorem implications well ga performing interpretations schema theorem implicitly assumed correlation exists parent offspring fitnesses assumption made explicit results based prices covariance selection theorem schemata play part performance theorems derived representations operators general however schemata reemerge recombination operators used using geiringers recombination distribution representation recombination operators missing schema theorem derived makes explicit intuition ga perform well finally method adaptive landscape analysis examined counterexamples offered commonly used correlation statistic instead alternative statistic transmission function fitness domain proposed optimal statistic estimating ga performance limited samples
this report supported part navy medical research development command office naval research department navy work unit onrreimb the views expressed article authors reflect official policy position department navy department defense us government approved public release distribution unlimited
an approach analytic learning described searches accurate entailments horn clause domain theory a hillclimbing search guided information based evaluation function performed applying set operators derive frontiers domain theories the analytic learning system one component multistrategy relational learning system we compare accuracy concepts learned analytic strategy concepts learned analytic strategy operationalizes domain theory
any system learns filter documents suffer poor performance initial training phase one way addressing problem exploit filters learned users collaborative fashion we investigate direct transfer learned filters settinga limiting case collaborative learning system we evaluate stability several different learning methods direct transfer conclude symbolic learning methods use negatively correlated features data perform poorly transfer even perform well conventional evaluation settings this effect robust holds several learning methods diverse set users used training classifier even learned classifiers adapted new users distribution our experiments give rise several concrete proposals improving generalization performance collaborative setting including beneficial variation feature selection method widely used text categorization
we present coevolutionary architecture solving decomposable problems apply evolution artificial neural networks although work preliminary nature number advantages noncoevolutionary approaches the coevolutionary approach utilizes divideandconquer technique species representing simpler subtasks evolved separate instances genetic algorithm executing parallel collaborations among species formed representing complete solutions species created dynamically needed results presented coevolutionary architecture produces higher quality solutions fewer evolutionary trials compared alternative non coevolutionary approach problem evolving cascade networks parity computation
we introduce algorithm lllama combines simple pattern recognizers general method estimating entropy sequence each pattern recognizer exploits partial match subsequences build model sequence since primary features interest biological sequence domains subsequences small variations exact composition lllama particularly suited domains we describe two methods lllamalength lllamaalone use entropy estimate perform maximum posteriori classification we apply methods several problems threedimensional structure classification short dna sequences the results include fl email loewenstpaulrutgersedu phone fax email bermanadeninerutgersedu z email hirshcsrutgersedu
rise domingos press rule induction algorithm proceeds gradually generalizing rules starting one rule per example this several advantages compared common strategy gradually specializing initially null rules shown lead significant accuracy gains algorithms like crules cn large number application domains however rises running time like rule induction algorithms quadratic number examples making unsuitable processing large databases this paper studies use partitioning speed rise compares wellknown method windowing the use partitioning specifictogeneral induction setting creates synergies would possible generaltospecific system partitioning often reduces running time improves accuracy time in noisy conditions performance windowing deteriorates rapidly partitioning remains stable
to investigate issue modularity emerges nature present artificial life model allow us reproduce computer organisms ie robots genotype nervous system sensory motor organs environment organisms live behave reproduce in simulations neural networks evolutionarily trained control mobile robot designed keep arena clear picking trash objects releasing outside arena during evolutionary process modular neural networks control robots behavior emerge result genetic duplications preliminary simulation results show duplicationbased modular architecture outperforms nonmodular architecture represents starting architecture simulations moreover interaction mutation duplication rate emerges results our future goal use model order explore relationship evolutionary emergence modularity phenomenon gene duplication
we describe new theory differential learning broad family pattern classifiers including many wellknown neural network paradigms learn stochastic concepts efficiently we describe relationship classifiers ability generalize well unseen test examples efficiency strategy learns we list series proofs differential learning efficient information computational resource requirements whereas traditional probabilistic learning strategies the proofs illustrated simple example lends closedform analysis we conclude optical character recognition task three different types differentially generated classifiers generalize significantly better probabilistically generated counterparts
with machine learning methods given knowledge representation space inadequate learning process fail this also true methods using neural networks form representation space to overcome limitation automatic construction method neural network proposed this paper describes bphci method hypothesisdriven constructive induction neural network trained backpropagation algorithm the method searches better representation space analyzing hypotheses generated step iterative learning process the method applied ten problems include particular exclusiveor monk paritybit inverse paritybit problems all problems successfully solved initial set parameters extension representation space necessary extension problem
this paper investigates alternative estimators accuracy concepts learned examples in particular crossvalidation bootstrap estimators studied using synthetic training data foil learning algorithm our experimental results contradict previous papers statistics advocate bootstrap method superior crossvalidation nevertheless results also suggest conclusions based crossvalidation previous machine learning papers unreliable specifically observations true error concept learned foil independently drawn sets examples concept varies widely ii estimate true error provided crossvalidation high variability approximately unbiased iii bootstrap estimator lower variability crossvalidation systematically biased
the problem driving autonomous vehicle normal traffic engages many areas ai research substantial economic significance we describe work progress new approach problem uses decisiontheoretic architecture using dynamic probabilistic networks the architecture provides sound solution problems sensor noise sensor failure uncertainty behavior vehicles effects ones actions we report advances theory inference decision making dynamic partially observable domains our approach implemented simulation system autonomous vehicle successfully negotiates variety difficult situations
two recently implemented machine learning algorithms ripper sleeping experts phrases evaluated number large text categorization problems these algorithms construct classifiers allow context word w affect even whether presence absence w contribute classification however ripper sleeping experts differ radically many respects differences include different notions constitutes context different ways combining contexts construct classifier different methods search combination contexts different criteria contexts included combination in spite differences ripper sleeping experts perform extremely well across wide variety categorization problems generally outperforming previously applied learning methods we view result confirmation usefulness classifiers represent contextual information
we address problem finding parameter settings result optimal performance given learning algorithm using particular dataset training data we describe wrapper method considering determination best parameters discrete function optimization problem the method uses bestfirst search crossvalidation wrap around basic induction algorithm search explores space parameter values running basic algorithm many times training holdout sets produced crossvalidation get estimate expected error parameter setting thus final selected parameter settings tuned specific induction algorithm dataset studied we report experiments method datasets selected uci statlog collections using c basic induction algorithm at confidence level method improves performance c nine domains degrades performance one statistically indistinguishable c rest on sample datasets used comparison method yields average relative decrease error rate we expect see similar performance improvements using method machine learning al gorithms
the feature vector editor offers userextensible environment exploratory data analysis several empirical studies applied environment sherfacs international conflict management dataset current analysis techniques include boolean analysis temporal analysis automatic rule learning implemented portably ansi common lisp common lisp interface manager clim system features advanced interface makes intuitive people manipulate data discover significant relationships the system encapsulates data within objects defines generic protocols mediate interactions data users analysis algorithms generic data protocols make possible rapid integration new datasets new analytical algorithms heterogeneous data formats more sophisticated research reformulates sherfacs conflict codings machineparsable narratives suitable processing semantic representations relatus natural language system experiments sherfacs cases demonstrated feasibility building knowledge bases synthetic texts exceeding pages
we introduce two boosting algorithms aim increase generalization accuracy given classifier incorporating level component stacked generalizer both algorithms construct complementary level classifier generate coarse hypotheses training data we show two algorithms boost generalization accuracy representative collection data sets the two algorithms distinguished one modifies class targets selected training instances order train complementary classifier we show two algorithms achieve approximately equal generalization accuracy create complementary classifiers display different degrees accuracy diversity our study provides evidence may useful investigate families boosting algorithms incorporate varying levels accuracy diversity achieve appropriate mix given task domain
object localization applications many areas engineering science the goal spatially locate arbitrarilyshaped object in many applications desirable minimize number measurements collected purpose ensuring sufficient localization accuracy in surgery example collecting large number localization measurements may either extend time required perform surgical procedure increase radiation dosage patient exposed localization accuracy function spatial distribution discrete measurements object measurement noise present in simon et al metrics presented evaluate information available set discrete object measurements in study new approaches discrete point data selection problem described these include hillclimbing genetic algorithms gas populationbased incremental learning pbil extensions standard ga pbil methods employ multiple parallel populations explored the results extensive empirical testing provided the results suggest combination pbil hillclimbing result best overall performance a computerassisted surgical system incorporates methods presented paper currently evaluated cadaver trials evolutionbased methods selecting point data shumeet baluja supported national science foundation graduate student fellowship graduate student fellowship national aeronautics space administration administered lyndon b johnson space center houston tx david simon partially supported national science foundation national challenge grant award iri object localization applications
the research reported paper describes fossil ilp system uses search heuristic based statistical correlation several interesting properties heuristic discussed shown naturally extended simple powerful stopping criterion independent number training examples instead fossils stopping criterion depends search heuristic estimates utility literals uniform scale after comparison foil mfoil krk domain mesh data outline ideas fossil adopted topdown pruning present preliminary results
psychological evidence shows probability theory proper descriptive model intuitive human judgment instead heuristics proposed descriptive model this paper argues probability theory limi tations even normative model a new normative model judgment uncertainty designed assumption systems knowledge resources insufficient respect questions system needs answer the proposed heuristics human reasoning also observed new model justified according assumption
this paper describes approach using gp image analysis based idea image enhancement feature detection image segmentation reframed image filtering problems gp used discover efficient optimal filters solve problems however order make search feasible effective terminal sets function sets fitness functions meet requirements in paper requirements described terminals functions fitness functions satisfy proposed some preliminary experiments also reported gp mentioned characteristics applied segmentation brain magnetic resonance images extremely difficult problem simple solution known compared artificial neural nets
reading area human cognition studied decades psychologists education researchers artificial intelligence researchers yet still exist theory accurately describes complete process we believe past attempts fell short due incomplete understanding overall task reading namely complete set mental tasks reasoner must perform read mechanisms carry tasks we present functional theory reading process argue represents coverage task the theory combines experimental results psychology artificial intelligence education linguistics along insights gained research this greater understanding mental tasks necessary reading enable new natural language understanding systems flexible capable earlier ones furthermore argue creativity necessary component reading process must considered theory system attempting describe we present functional theory creative reading novel knowledge organization scheme supports creativity mechanisms the reading theory currently implemented isaac integrated story analysis and creativity system computer system reads science fiction stories fl this paper part georgia institute technology college computing technical report series
we examine performance memoryless vector quantizer changes function training set size specifically study well training set distortion predicts test distortion training set randomly drawn subset blocks test training images using vapnikchervonenkis dimension derive formal bounds difference test training distortion vector quantizer codebooks we describe extensive empirical simulations test bounds variety bit rates vector dimensions give practical suggestions determining training set size necessary achieve good generalization codebook we conclude using training sets comprised small fraction available data one produce results close results obtainable available data used
this paper deals global finitegain inputoutput stabilization linear systems saturated controls for neutrally stable systems shown linear feedback law suggested passivity approach indeed provides stability respect every l p norm explicit bounds closedloop gains obtained related norms respective systems without saturation these results extend class systems state matrix eigenvalues imaginary axis nonsimple size gt jordan blocks contradicting may expected fact systems globally asymptotically stabilizable statespace sense shown particular double integrator
this paper deals problem global stabilization linear discrete time systems means bounded feedback laws the main result proved analog one proved continuous time case authors shows stabilization possible system stabilizable arbitrary controls transition matrix spectral radius less equal one the proof provides principle algorithm construction feedback laws implemented either cascades parallel connections single hidden layer neural networks simple saturation functions
the npcomplete problem determining whether two disjoint point sets ndimensional real space r n separated two planes cast bilinear program minimizing scalar product two linear functions polyhedral set the bilinear program vertex solution processed iterative linear programming algorithm terminates finite number steps point satisfying necessary optimality condition global minimum encouraging computational experience number test problems reported
the problem discriminating two finite point sets ndimensional feature space separating plane utilizes features possible formulated mathematical program parametric objective function linear constraints the step function appears objective function approximated sigmoid concave exponential nonnegative real line treated exactly considering equivalent linear program equilibrium constraints lpec computational tests three approaches publicly available realworld databases carried compared adaptation optimal brain damage obd method reducing neural network complexity one feature selection algorithm via concave minimization fsv reduced crossvalidation error cancer prognosis database reducing problem features feature selection important problem machine learning in basic form problem consists eliminating many features given problem possible still carrying preassigned task acceptable accuracy having minimal number features often leads better generalization simpler models easily interpreted in present work task discriminate two given sets ndimensional feature space using given features possible we shall formulate problem mathematical program parametric objective function attempt achieve task generating separating plane feature space small dimension possible minimizing average distance misclassified points plane one computational experiments carried feature selection procedure showed effectiveness minimizing number features selected also quickly recognizing removing spurious random features introduced thus wisconsin prognosis breast cancer wpbc database feature space dimensions random features added one algorithms fsv immediately removed random features well original features resulting separating plane dimensional reduced feature space by using tenfold crossvalidation separation error dimensional space reduced corresponding error original problem space see section details we note mathematical programming approaches feature selection problem recently proposed even though approach based lpec formulation lpec method solution different ones used the polyhedral concave minimization approach principally involved theoretical considerations one specific algorithm crossvalidatory results given other effective computational applications mathematical programming neural networks given
this work describes approach inferring deterministic contextfree dcf grammars connectionist paradigm using recurrent neural network pushdown automaton nnpda the nnpda consists recurrent neural network connected external stack memory common error function we show nnpda able learn dynamics underlying pushdown automaton examples grammatical nongrammatical strings not network learn state transitions automaton also learns actions required control stack in order use continuous optimization methods develop analog stack reverts discrete stack quantization activations network learned transition rules stack actions we show enhancement networks learning capabilities providing hints in addition initial comparative study simulations first second third order recurrent networks shown increased degree freedom higher order networks improve generalization necessarily learning speed
this paper presents hga genetic algorithm written vhdl intended hardware implementation due pipelining parallelization function call overhead hardware ga yields significant speedup software ga especially useful ga used realtime applications eg disk scheduling image registration since generalpurpose ga requires fitness function easily changed hardware implementation must exploit reprogrammability certain types fieldprogrammable gate arrays fpgas programmed via bit pattern stored static ram thus easily reconfigured after presenting background vhdl paper takes reader hgas code we describe applications hga feasible given stateoftheart fpga technology summarize possible extensions design finally review work hardwarebased gas
one basic probabilistic tools used time series modeling hidden markov model hmm in hmm information past time series conveyed single discrete variablethe hidden state we present generalization hmms state factored multiple state variables therefore represented distributed manner both inference learning model depend critically computing posterior probabilities hidden state variables given observations we present exact algorithm inference model relate forwardbackward algorithm hmms algorithms general belief networks due combinatorial nature hidden state representation exact algorithm intractable as intractable systems approximate inference carried using gibbs sampling mean field theory we also present structured approximation state variables decoupled based derive tractable learning algorithm empirical comparisons suggest approximations efficient accurate alternatives exact methods finally use structured approximation model bachs chorales show outperforms hmms capturing complex temporal patterns dataset
we develop refined mean field approximation inference learning probabilistic neural networks our mean field theory unlike assume units behave independent degrees freedom instead exploits principled way existence large substructures computationally tractable to illustrate advantages framework show incorporate weak higher order interactions firstorder hidden markov model treating corrections first order structure within mean field theory
this chapter takes different standpoint address problem learning we reason terms probability make extensive use chain rule known bayes rule a fast definition basics probability provided appendix a quick reference most chapter review methods bayesian learning applied modelling purposes some original analyses comments also provided section there latent rivalry bayesian orthodox statistics it means intention enter kind controversy we perfectly willing accept orthodox well unorthodox methods long scientifically sound provide good results applied learning tasks the disclaimer applies two frameworks presented they object heated controversy past years neural networks community we take side present frameworks strong points weaknesses in context work bayesian frameworks especially interesting provide continuous update rules used regularised cost minimisation yield automatic selection regularisation level unlike methods presented chapter necessary try several regularisation levels perform many optimisations the bayesian framework one training achieved onepass optimisation procedure
abstract in document i first review theory behind bitsback coding aka free energy coding frey hinton describe interface clanguage software used bitsback coding this method new approach problem optimal compression source code produces multiple codewords given symbol it may seem sensible codeword use case shortest one however proposed bitsback approach random codeword selection yields effective codeword length less shortest codeword length if random choices boltzmann distributed effective length optimal given source code the software i describe guide easy use source code pages long i illustrate bitsback coding software simple quantized gaussian mixture problem
a modified recurrent neural network rnn used learn selfrouting interconnection network srin set routing examples the rnn modified several distinct initial states this equivalent single rnn learning multiple different synchronous sequential machines we define sequential machine structure augmented show srin essentially augmented synchronous sequential machine assm as example learn small sixswitch srin after training extract networks internal representation assm corresponding srin fl this paper adapted goudreau chapter a shortened version paper published goudreau giles
this paper outlines methodology analyzing representational support knowledgebased decisionmodeling broad domain a relevant set inference patterns knowledge types identified by comparing analysis results existing representations insights gained design approach integrating categorical uncertain knowledge context sensitive manner
we propose computational framework understanding modeling human consciousness this framework integrates many existing theoretical perspectives yet sufficiently concrete allow simulation experiments we attempt explain qualia subjective experience instead ask differences exist within cognitive information processing system person conscious mentallyrepresented information versus information unconscious the central idea explore contents consciousness correspond temporally persistent states network computational modules three simulations described illustrating behavior persistent states models corresponds roughly behavior conscious states people experience performing similar tasks our simulations show periodic settling persistent ie conscious states improves performance cleaning inaccuracies noise forcing decisions helping keep system track toward solution
we discuss two types algorithms selecting relevant examples developed context computation learning theory the examples selected stream examples generated independently random the first two algorithms socalled boosting algorithms schapire schapire freund freund querybycommittee algorithm seung seung et al we describe algorithms proven properties point commonalities suggest possible future implications
this paper traces development main ideas led present state knowledge inductive logic programming the story begins research psychology subject human concept learning results research influenced early efforts artificial intelligence combined formal methods inductive inference evolve present discipline inductive logic programming inductive logic programming often considered young discipline however roots research dating back nearly years this paper traces development ideas beginning psychology effect concept learning research artificial intelligence independent requirement psychological basis formal methods inductive inference developed these separate streams eventually gave rise inductive logic programming this account entirely unbiased more attention given work researchers influenced interest machine learning being retrospective paper i attempt describe recent developments ilp this account includes research prior year term inductive logic programming first used muggleton this reason subtitle a prehistoric tale the major headings paper taken names periods evolution life earth
recurrent neural networks readily process recognize generate temporal sequences by encoding grammatical strings temporal sequences recurrent neural networks trained behave like deterministic sequential finitestate automata algorithms developed extracting grammatical rules trained networks using simple method inserting prior knowledge rules recurrent neural networks show recurrent neural networks able perform rule revision rule revision performed comparing inserted rules rules finitestate automata extracted trained networks the results training recurrent neural network recognize known nontrivial randomly generated regular grammar show networks preserve correct rules able correct training inserted rules initially incorrect by incorrect mean rules ones randomly generated grammar
in section survey recombination operators apply two parents create offspring some multiparent recombination operators defined fixed number parents eg arity three operators number parents random number might greater two yet operators arity parameter set arbitrary integer number we pay special attention latter type operators summarize results effect operator arity ea performance
backpropagation learning algorithms typically collapse networks structure single vector weight parameters optimized we suggest performance may improved utilizing structural information instead discarding introduce framework tempering weight accordingly in tempering model activation error signals treated approximately independent random variables the characteristic scale weight changes matched residuals allowing structural properties nodes fanin fanout affect local learning rate backpropagated error the model also permits calculation upper bound global learning rate batch updates turn leads different update rules bias vs nonbias weights
this paper addresses class learning problems require construction descriptions combine mofn rules traditional disjunctive normal form dnf rules the presented method learns descriptions call conditional mofn rules using hypothesisdriven constructive induction approach in approach representation space modified according patterns discovered iteratively generated hypotheses the need mofn rules detected observing exclusiveor equivalence patterns hypotheses these patterns indicate symmetry relations among pairs attributes symmetrical attributes combined maximal symmetry classes for symmetry class method constructs counting attribute adds new dimension representation space the search hypothesis iteratively modified representation spaces done standard aq inductive rule learning algorithm it shown proposed method capable solving problems would difficult tackle traditional symbolic learning methods
inferences conversational casebased reasoning ccbr approach embedded cbr content navigator line products susceptible bias case scoring algorithm in particular shorter cases tend given higher scores assuming factors held constant this report summarizes investigation mediating bias we introduce approach eliminating bias evaluate affects retrieval performance six case libraries we also suggest explanations results note limitations study
we investigate effectiveness stochastic hillclimbing baseline evaluating performance genetic algorithms gas combinatorial function optimizers in particular address four problems gas applied literature maximum cut problem kozas multiplexer problem mdap multiprocessor document allocation problem jobshop problem we demonstrate simple stochastic hillclimbing methods able achieve results comparable superior obtained gas designed address four problems we illustrate case jobshop problem insights obtained formulation stochastic hillclimbing algorithm lead improvements encoding used ga fl department computer science university california berkeley supported nasa graduate fellowship this paper written author visiting researcher ecole normale superieurerue dulm groupe de bioinformatique france email juelscsberkeleyedu department mathematics university california berkeley supported ndseg graduate fellowship email wattenbemathberkeleyedu
this paper describes several means sharing related concepts improve learning domain the sharing comes form substructures possibly entire structures previous concepts may aid learning concepts these substructures highlight useful information domain using two domains evaluate effectiveness concept sharing respect accuracy concept size search complexity noise resistance
genetic algorithms stochastic search optimization techniques used wide range applications this paper addresses application genetic algorithms graph partitioning problem standard genetic algorithms large populations suffer lack efficiency quite high execution time a massively parallel genetic algorithm proposed implementation supernode transputers results various benchmarks given a comparative analysis approach hillclimbing algorithms simulated annealing also presented the experimental measures show algorithm gives better results concerning quality solution time needed reach
support vector learning machines svm finding application pattern recognition regression estimation operator inversion illposed problems against general backdrop methods improving generalization performance improving speed test phase svms increasing interest in paper combine two techniques pattern recognition problem the method improving generalization performance virtual support vector method incorporating known invariances problem this method achieves drop error rate nist test digit images the method improving speed reduced set method approximating support vector decision surface we apply method achieve factor fifty speedup test phase virtual support vector machine the combined approach yields machine times faster original machine better generalization performance achieving error the virtual support vector method applicable svm problem known invariances the reduced set method applicable support vector machine
a significant limitation neural networks representations learn usually incomprehensible humans we present novel algorithm trepan extracting comprehensible symbolic representations trained neural networks our algorithm uses queries induce decision tree approximates concept represented given network our experiments demonstrate trepan able produce decision trees maintain high level fidelity respective networks comprehensible accurate unlike previous work area algorithm general applicability scales well large net works problems highdimensional input spaces
by analyzing relationships among chance weight evidence degree belief shown assertion chances special cases belief functions assertion dempsters rule used combine belief functions based distinct bodies evidence together lead inconsistency dempstershafer theory to solve problem fundamental postulates theory must rejected a new approach uncertainty management introduced shares many intuitive ideas ds theory avoiding problem
in spite popularity explanationbased learning ebl theoretical basis wellunderstood using generalization probably approximately correct pac learning problem solving domains paper formalizes two forms explanationbased learning macrooperators proves sufficient conditions success these two forms ebl called macro caching serial parsing respectively exhibit two distinct sources power bias sparseness solution space decomposability problemspace the analysis shows exponential speedup achieved either biases suitable domain somewhat surprisingly also shows computing preconditions macrooperators necessary obtain speedups the theoretical results confirmed experiments domain eight puzzle our work suggests best way address utility problem ebl implement bias exploits problemspace structure set domains one interested learning
developed recently support vector learning machines achieve high generalization ability minimizing bound expected test error however far existed way adding knowledge invariances classification problem hand we present method incorporating prior knowledge transformation invariances applying transformations support vectors training ex amples critical determining classification boundary
this paper reports recent results using genetic algorithms learn decision rules complex robot behaviors the method involves evaluating hypothetical rule sets simulator applying simulated evolution evolve effective rules the main contributions paper task learned complex behavior involving multiple mobile robots learned rules verified experiments operational mobile robots the case study involves shepherding task one mobile robot attempts guide another robot specified area
in learning systems examples represented fixedlength feature vectors components either real numbers nominal values we propose extension featurevector representation allows value feature set strings instance represent small white black dog nominal features size species setvalued feature color one might use feature vector sizesmall speciescanisfamiliaris colorfwhiteblackg since make assumptions number possible set elements extension traditional featurevector representation closely connected blums infinite attribute representation we argue many decision tree rule learning algorithms easily extended setvalued features we also show example many realworld learning problems efficiently naturally represented setvalued features particular text categorization problems problems arise propositionalizing firstorder representations lend setvalued features
the mobile robot domain challenges policyiteration reinforcement learning algorithms difficult problems structural credit assignment uncertainty structural credit assignment particularly acute domains realtime trial length limiting factor number learning steps physical hardware perform noisy sensors effectors complex dynamic environments complicate learning problem leading situations speed learning policy flexibility may important policy optimality input generalization addresses problems typically time consuming robot domains we present two algorithms yblearning yb perform simple fast generalization input space based bitsimilarity the algorithms trade longterm optimality immediate performance flexibility the algorithms tested simulation nongeneralized learning across different numbers discounting steps yb shown perform better earlier stages learning particularly presence noise in trials performed sonarbased mobile robot subject uncertainty real world yb surpassed simulation results wide margin strongly supporting role quick dirty generalization strategies noisy realtime mobile robot domains
in time series problems noise divided two categories dynamic noise drives process observational noise added measurement process influence future values system in framework empirical volatilities squared relative returns prices exhibit significant amount observational noise to model predict time evolution adequately estimate state space models explicitly include observational noise we obtain relaxation times shocks logarithm volatility ranging three weeks foreign exchange three five months stock indices in cases twodimensional hidden state required yield residuals consistent white noise we compare results ordinary autoregressive models without hidden state find autoregressive models underestimate relaxation times two orders magnitude due ignoring distinction observational dynamic noise this new interpretation dynamics volatility terms relaxators state space model carries stochastic volatility models garch models useful several problems finance including risk management pricing derivative securities
in paper present tdleaf variation td algorithm enables used conjunction minimax search we present experiments chess program knightcap used tdleaf learn evaluation function playing free ineternet chess server fics ficsonenetnet it improved rating rating games days play we discuss reasons success also relationship results tesauros results backgammon
the problem minimizing number misclassified points plane attempting separate two point sets intersecting convex hulls ndimensional real space formulated linear program equilibrium constraints lpec this general lpec converted exact penalty problem quadratic objective linear constraints a frankwolfetype algorithm proposed penalty problem terminates stationary point global solution novel aspects approach include a linear complementarity formulation step function counts misclassifications ii exact penalty formulation without boundedness nondegeneracy constraint qualification assumptions iii an exact solution extraction sequence minimizers penalty function finite value penalty parameter general lpec explicitly exact solution lpec uncoupled constraints iv a parametric quadratic programming formulation lpec associated misclassification minimization problem
technical report idsia abstract it long known neural networks learn faster input hidden unit activities centered zero recently extended approach also encompass centering error signals schraudolph sejnowski here generalize notion factors involved weight update leading us propose centering slope hidden unit activation functions well slope centering removes linear component backpropagated error improves credit assignment networks shortcut connections benchmark results show speed learning significantly without adversely affecting trained networks generalization ability
this paper presents asocs adaptive selforganizing concurrent system model massively parallel processing incrementally defined rule systems areas adaptive logic robotics logical inference dynamic control an asocs adaptive network composed many simple computing elements operating asynchronously parallel an asocs operate either data processing mode learning mode during data processing mode asocs acts parallel hardware circuit during learning mode asocs incorporates rule expressed boolean conjunction distributed fashion time logarithmic number rules this paper proposes learning algorithm architecture priority asocs this new asocs model uses rules priorities the new model significant learning time space complexity improvements previous models nonvon neumann architectures neural networks attack wordatatime bottleneck traditional computing systems neural networks learn inputoutput mappings using highly distributed processing memory their numerous simple processing elements modifiable weighted links permit high degree parallelism a typical neural network fixed topology it learns modifying weighted links nodes a new class connectionist architectures proposed called asocs adaptive selforganizing concurrent systems asocs models support efficient computation selforganized learning parallel execution learning done incremental presentation rules andor examples asocs models learn modifying topology data types include boolean multistate variables recent models support analog variables the model incorporates rules adaptive logic network parallel self organizing fashion in processing mode asocs supports fully parallel execution actual inputs according learned rules the adaptive logic network acts parallel hardware circuit execution mapping n input boolean vectors output boolean vectors combinatoric fashion the overall philosophy asocs follows high level goals current neural network models however mechanisms learning execution vary significantly the asocs logic network topologically dynamic network growing efficiently fit specific application current asocs models based digital nodes asocs also supports use symbolic heuristic learning mechanisms thus combining parallelism distributed nature connectionist computing potential power ai symbolic learning a proof concept asocs chip developed
we exhibit theoretically founded algorithm t agnostic paclearning decision trees levels whose computation time almost linear size training set we evaluate performance learning algorithm t common realworld datasets show datasets t provides simple decision trees little loss predictive power compared c in fact datasets continuous attributes error rate tends lower c to best knowledge first time paclearning algorithm shown applicable realworld classification problems since one prove t agnostic paclearning algorithm t guaranteed produce close optimal level decision trees sufficiently large training sets distribution data in regard t differs strongly learning algorithms considered applied machine learning guarantee given performance new datasets we also demonstrate algorithm t used diagnostic tool investigation expressive limits level decision trees finally t combination new bounds vcdimension decision trees bounded depth derive provides us first time tools necessary comparing learning curves decision trees realworld datasets theoretical estimates pac learning theory
andrew d back department electrical computer engineering university queensland st lucia australia he brain information processing group frontier research program riken the institute physical chemical research hirosawa wakoshi saitama japan abstract the performance neural network simulations often reported terms mean standard deviation number simulations performed different starting conditions however many cases distribution individual results approximate gaussian distribution may symmetric may multimodal we present distribution results practical problems show assuming gaussian distributions significantly affect interpretation results especially comparison studies for controlled task consider find distribution performance skewed towards better performance smoother target functions skewed towards worse performance
the structure environment affects behaviors organisms evolved how structure described behavioral consequences explained predicted we aim establish initial answers questions simulating evolution simple organisms simple environments different structures our artificial creatures called minimats neither sensors memory behave solely picking amongst actions moving eating reproducing sitting according inherited probability distribution our simulated environments contain food multiple minimats structured terms spatial temporal food density patchiness food appears changes environmental parameters affect evolved behaviors minimats different ways three parameters importance describing minimat world one useful behavioral strategies evolves looping movement allows minimatsdespite lack internal stateto match behavior temporal spatial structure environment ultimately find minimats construct environments individual behaviors making study impact global environment structure individual behavior much complex
the primary aim paper show graphical models used mathematical language integrating statistical subjectmatter information in particular paper develops principled nonparametric framework causal inference diagrams queried determine assumptions available sufficient identifying causal effects nonexperimental data if diagrams queried produce mathematical expressions causal effects terms observed distributions otherwise diagrams queried suggest additional observations auxiliary experiments desired inferences obtained key words causal inference graph models structural equations treatment effect
we analyse biases eleven measures estimating quality multivalued attributes the values information gain jmeasure giniindex relevance tend linearly increase number values attribute the values gainratio distance measure relief weight evidence decrease informative attributes increase irrelevant attributes the bias statistic tests based chisquare distribution similar functions able discriminate among attributes different quality we also introduce new function based mdl principle whose value slightly decreases increasing number attributes values
in past nearest neighbor algorithms learning examples worked best domains features numeric values in domains examples treated points distance metrics use standard definitions in symbolic domains sophisticated treatment feature space required we introduce nearest neighbor algorithm learning domains symbolic features our algorithm calculates distance tables allow produce realvalued distances instances attaches weights instances modify structure feature space we show technique produces excellent classification accuracy three problems studied machine learning researchers predicting protein secondary structure identifying dna promoter sequences pronouncing english text direct experimental comparisons learning algorithms show nearest neighbor algorithm comparable superior three domains in addition algorithm advantages training speed simplicity perspicuity we conclude experimental evidence favors use continued development nearest neighbor algorithms domains ones studied
many supervised machine learning algorithms require discrete feature space in paper review previous work continuous feature discretization identify defining characteristics methods conduct empirical evaluation several methods we compare binning unsupervised discretization method entropybased puritybased methods supervised algorithms we found performance naivebayes algorithm significantly improved features discretized using entropybased method in fact tested datasets discretized version naivebayes slightly outperformed c average we also show cases performance c induction algorithm significantly improved features discretized advance experiments performance never significantly degraded interesting phenomenon considering fact c capable locally discretiz ing features
we review recent work done group applying genetic algorithms gas design cellular automata cas perform computations requiring global coordination a ga used evolve cas two computational tasks density classification synchronization in cases ga discovered rules gave rise sophisticated emergent computational strategies these strategies analyzed using computational mechanics framework particles carry information interactions particles effects information processing this framework also used explain process strategies designed ga the work described first step employing gas engineer useful emergent computation decentralized multiprocessor systems it also first step understanding evolutionary process produce complex systems sophisticated collective computational abilities
metastability common phenomenon many evolutionary processes natural artificial alternate periods stasis brief periods rapid change behavior in paper analytical model dynamics mutationonly genetic algorithm ga introduced identifies new general mechanism causing metastability evolutionary dynamics the gas population dynamics described terms flows space fitness distributions the trajectories fitness distribution space derived closed form limit infinite populations we show finite populations induce metastability even regions fitness exhibit local optimum in particular model predicts occurrence fitness epochs periods stasis population fitness distributionsat finite population size identifies locations fitness epochs flows hyperbolic fixed points this enables exact predictions metastable fitness distributions fitness epochs well giving insight nature periods stasis innovations all results obtained closedform expressions terms gas parameters an analysis jacobian matrices neighborhood epochs metastable fitness distribution allows calculation stable unstable manifold dimensions reveals state spaces topological structure more general quantitative features dynamicsfitness fluctuation amplitudes epoch stability speed innovationsare also determined jacobian eigenvalues the analysis shows quantitative predictions range dynamical behaviors specific finite population dynamics derived solution infinite population dynamics the theoretical predictions shown agree well statistics ga simulations we also discuss connections results population genetics molecular evolution theory
in paper explore use adaptive search technique genetic algorithms construct system gabil continually learns refines concept classification rules interaction environment the performance system measured set concept learning problems compared performance two existing systems idr c preliminary results support despite minimal system bias gabil effective concept learner quite competitive idr c target concept increases complexity
we review accuracy estimation methods compare two common methods crossvalidation bootstrap recent experimental results artificial data theoretical results restricted settings shown selecting good classifier set classifiers model selection tenfold crossvalidation may better expensive leaveoneout crossvalidation we report largescale experimentover half million runs c naivebayes algorithmto estimate effects different parameters algorithms realworld datasets for crossvalidation vary number folds whether folds stratified bootstrap vary number bootstrap samples our results indicate realword datasets similar best method use model selection tenfold stratified cross validation even computation power allows using folds
naivebayes induction algorithms previously shown surprisingly accurate many classification tasks even conditional independence assumption based violated however studies done small databases we show larger databases accuracy naivebayes scale well decision trees we propose new algorithm nbtree induces hybrid decisiontree classifiers naivebayes classifiers decisiontree nodes contain univariate splits regular decisiontrees leaves contain naivebayesian classifiers the approach retains interpretability naivebayes decision trees resulting classifiers frequently outperform constituents especially larger databases tested
we present mlc library c classes tools supervised machine learning while mlc provides general learning algorithms used end users main objective provide researchers experts wide variety tools accelerate algorithm development increase software reliability provide comparison tools display information visually more collection existing algorithms mlc attempt extract commonalities algorithms decompose unified view simple coherent extensible in paper discuss problems mlc aims solve design mlc current functionality
bayesian models involving dirichlet process mixtures heart modern nonparametric bayesian movement much rapid development models last decade direct result advances simulationbased computational methods some early work area circa focused use nonparametric ideas models applications otherwise standard hierarchical models this chapter provides historical review perspective developments prime focus use integration nonparametric ideas hierarchical models we illustrate ease strict parametric assumptions common standard bayesian hierarchical models relaxed incorporate uncertainties functional forms using dirichlet process components partly enabled approach computation using mcmc methods the resulting methology illustrated two examples taken unpublished report topic
in paper present averagecase analysis bayesian classifier simple induction algorithm fares remarkably well many learning tasks our analysis assumes monotone conjunctive target concept independent noisefree boolean attributes we calculate probability algorithm induce arbitrary pair concept descriptions use compute probability correct classification instance space the analysis takes account number training instances number attributes distribution attributes level class noise we also explore behavioral implications analysis presenting predicted learning curves artificial domains give experimental results domains check reasoning one goal research machine learning discover principles relate algorithms domain characteristics behavior to end many researchers carried systematic experimentation natural artificial domains search empirical regularities eg kibler langley others focused theoretical analyses often within paradigm probably approximately correct learning eg haussler however experimental studies based informal analyses learning task whereas formal analyses address worst case thus bear little relation empirical results ber attributes class attribute frequencies obtain predictions behavior induction algorithms used experiments check analyses however research focus algorithms typically used experimental practical sides machine learning important averagecase analyses extended methods recently growing interest probabilistic approaches inductive learning for example fisher described cobweb incremental algorithm conceptual clustering draws heavily bayesian ideas literature reports number systems build work eg allen langley iba gennari thompson langley cheeseman et al outlined autoclass nonincremental system uses bayesian methods cluster instances groups researchers focused induction bayesian inference networks eg cooper kerskovits these recent bayesian learning algorithms complex easily amenable analysis share common ancestor simpler tractable this supervised algorithm refer simply bayesian classifier comes originally work pattern recognition duda hart the method stores probabilistic summary class summary contains conditional probability attribute value given class well probability base rate class this data structure approximates representational power perceptron describes single decision boundary instance space when algorithm encounters new instance updates probabilities stored specified class neither order training instances occurrence classification errors effect process when given test instance classifier uses evaluation function describe detail later rank alter
regularization eg form weight decay important training optimization neural network architectures in work provide tool based asymptotic sampling theory iterative estimation weight decay parameters the basic idea gradient descent estimated generalization error respect regularization parameters the scheme implemented designer net framework network training pruning ie based diagonal hessian approximation the scheme require essential computational overhead addition needed training pruning the viability approach demonstrated experiment concerning prediction chaotic mackeyglass series we find optimized weight decays relatively large densely connected networks initial pruning phase decrease pruning proceeds
vations perceptrons perceptron learning algorithm cycles among hyperplanes hyperplanes may compared select one gives best split examples always possible perceptron build hyper plane separates least one example rest we describe extentron grows multilayer networks capable distinguishing non linearlyseparable data using simple perceptron rule linear threshold units the resulting algorithm simple fast scales well large prob lems retains convergence properties perceptron completely specified using two parameters results presented comparing extentron neural network paradigms symbolic learning systems
technical report idsia abstract it long known neural networks learn faster input hidden unit activities centered zero recently extended approach also encompass centering error signals here generalize notion factors involved networks gradient leading us propose centering slope hidden unit activation functions well slope centering removes linear component backpropagated error improves credit assignment networks shortcut connections benchmark results show speed learning significantly without adversely affecting trained networks generalization ability
we introduce new faulttolerant model algorithmic learning using equivalence oracle incomplete membership oracle answers random subset learners membership queries may missing we demonstrate high probability still possible learn monotone dnf formulas polynomial time provided fraction missing answers bounded constant less one even half membership queries expected yield information algorithm exactly identify mterm nvariable monotone dnf formulas expected omn queries the task shown require exponential time using equivalence queries alone we extend algorithm handle onesided errors discuss several possible error models it hoped work may lead better understanding power membership queries effects faulty teachers query models concept learning
one method making analogies access instantiate abstract domain principles one method acquiring knowledge abstract principles discover experience we view generalization experiences absence prior knowledge target principle task hypothesis formation subtask discovery also view use hypothesized principles analogical design task hypothesis testing another subtask discovery in paper focus discovery physical principles generalization design experiences domain physical devices some important issues generalization experiences generalize experience far generalize methods use we represent reasoners comprehension specific designs form structurebehaviorfunction sbf models an sbf model provides functional causal explanation working device we represent domain principles deviceindependent behaviorfunction bf models we show function device determines generalize sbf model ii sbf model suggests far generalize iii typology functions indicates method use
the power casebased method comes ability retrieve right case new problem specified this implies learning right indices case storing potential reuse crucial success method a hierarchical organization case memory raises two distinct related issues index learning learning indexing vocabulary learning right level generalization in paper show use structurebehaviorfunction sbf models constrains index learning context experiencebased design physical devices the sbf model design provides functional causal explanation structure design delivers function we describe sbf model design together specification task design case might reused provides vocabulary indexing design case memory we also discuss prior design experiences stored casememory help determine level index generalization the kritik system implements evaluates modelbased method learning indices design cases
yang y hj sussmann ed sontag stabilization linear systems bounded controls proc nonlinear control systems design symp bordeaux june m fliess ed ifac publications pp journal version appear ieee trans autom control
the bayesian approach comparing models involves calculating posterior probability plausible model for highdimensional contingency tables set plausible models large we focus attention reversible jump markov chain monte carlo green develop strategies calculating posterior probabilities hierarchical graphical decomposable loglinear models even tables moderate size sets models may large the choice suitable prior distributions model parameters also discussed detail two examples presented for first example fi fi table model probabilities calculated using reversible jump approach compared model probabilities calculated exactly using alternative approximation the second example contingency table exact methods infeasible due large number possible models
in addition learning new knowledge system must able learn knowledge likely applicable an index piece information identified given situation triggers relevant piece knowledge schema systems memory we discuss issue indices may learned automatically context story understanding task present program learn new indices existing explanatory schemas we discuss two methods using system identify relevant schema even input directly match existing index learn new index allow retrieve schema efficiently future
in paper construct suboptimal h controllers satisfy new robust performance condition using receding horizon technique a method described synthesis h controllers online making use exact plant model finite interval extending future inequalities based two riccati differential equation solution finite horizon h problem derived resulting freedom exploited construct h controllers closed loop induced norm less prespecified value plants within set described terms future variation plant dual results possible adaptive interpretation also constructed
concept learning one studied areas machine learning a lot work domain deals decision trees in paper concerned different kind technique based galois lattices concept lattices we present new semilattice based system iglue uses entropy function topdown approach select concepts lattice construction then iglue generates new relevant numerical features transforming initial boolean features concepts iglue uses new features redescribe examples finally iglue applies mahanalobis distance similarity measure examples keywords multistrategy learning instancebased learning galois lattice feature transformation
this paper introduces hybrid learning methodology integrates genetic algorithms gas decision tree learning id order evolve optimal subsets discriminatory features robust pattern classification a ga used search space possible subsets large set candidate discrimination features for given feature subset id invoked produce decision tree the classification performance decision tree unseen data used measure fitness given feature set turn used ga evolve better feature sets this gaid process iterates feature subset found satisfactory classification performance experimental results presented illustrate feasibility approach difficult problems involving recognizing visual concepts satellite facial image data the results also show improved classification performance reduced description complexity compared standard methods feature selection
this paper presents neural network architecture manage structured data refine knowledge bases expressed first order logic language the presented framework well suited classification problems concept de scriptions depend upon numerical features data in fact main goal neural architecture refining numerical part knowledge base without changing structure in particular discuss method translate set classification rules neural computation units here focus attention translation method algorithms refine network weights struc tured data the classification theory refined manually handcrafted automatically acquired symbolic relational learning system able deal numerical features as matter fact primary goal bring neural network architecture capability dealing structured data unrestricted size allowing dynamically bind classification rules different items occur ring input data an extensive experimentation challenging artificial case study shows network converges quite fastly generalizes much better propositional learners equivalent task definition
the evolving population neural nets contains information terms genes also collection behaviors population members such information thought kind culture population two ways exploiting culture explored paper culling overlarge litters generate large number offspring different crossovers quickly evaluate comparing performance population throw away appear poor teaching use backpropagation train offspring toward performance population both techniques result faster effective neuroevolution effectively combined demonstrated inverted pendulum problem additional methods cultural exploitation possible studied future work these results suggest cultural exploitation powerful idea allows leveraging several aspects genetic algorithm
this paper describes structuremapping engine sme program studying analogical processing sme built explore gentners structuremapping theory analogy provides tool kit constructing matching algorithms consistent theory its flexibility enhances cognitive simulation studies simplifying experimentation furthermore sme efficient making useful component machine learning systems well we review structuremapping theory describe design engine we analyze complexity algorithm demonstrate steps polynomial typically bounded o n next demonstrate examples operation taken cognitive simulation studies work machine learning finally compare sme analogy programs discuss several areas future work this paper appeared artificial intelligence pp for information please contact forbusilsnwuedu
we investigate aspects cognition involved invention precisely invention telephone alexander graham bell we propose use structurebehaviorfunction sbf language representation invention knowledge claim sbf shown support wide range reasoning physical devices constitutes plausible account inventor might represent knowledge invention we propose use actr architecture implementation model actr shown precisely model wide range human cognition we draw upon architecture execution productions matching declarative knowledge spreading activation thus present model combines wellestablished cognitive validity actr powerful specialized modelbased reasoning methods facilitated sbf
in online character recognition observe two kinds intraclass variations small geometric deformations completely different writing styles we propose new approach deal problems defining extension tangent distance well known offline character recognition the system implemented knearest neighbor classifier called diabolo classifier respectively both classifiers invariant transformations like rotation scale slope deal variations stroke order writing direction results presented digit database writers
exact boltzmann learning done certain restricted networks technique decimation we enlarged set decimatable boltzmann machines introducing new general decimation rule we compared solutions probability density estimation problem decimatable boltzmann machines results obtained gibbs sampling unrestricted nondecimatable
the majority results computational learning theory concerned concept learning ie special case function learning classes functions range f g much less known theory learning functions larger range in ir in particular relatively results exist general structure common models function learning nontrivial function classes positive learning results exhibited models we introduce paper notion binary branching adversary tree function learning allows us give somewhat surprising equivalent characterization optimal learning cost learning class realvalued functions terms maxmin definition involve learning model another general structural result paper relates cost learning union function classes learning costs individual function classes furthermore exhibit efficient learning algorithm learning convex piecewise linear functions ir ir previously class linear functions ir ir class functions multidimensional domain known learnable within rigorous framework formal model online learning finally give sufficient condition arbitrary class f functions ir ir allows us learn class functions written pointwise maximum k functions f this allows us exhibit number nontrivial classes functions ir ir exist efficient learning algorithms
although applicable wide array problems demonstrated good performance number difficult realworld tasks neural networks usually applied problems comprehensibility acquired concepts important the concept representations formed neural networks hard understand typically involve distributed nonlinear relationships encoded large number realvalued parameters to address limitation developing algorithms extracting symbolic concept representations trained neural networks we first discuss important able understand concept representations formed neural networks we briefly describe approach discuss number issues pertaining comprehensibility arisen work finally discuss choices made research date open research issues yet addressed
one view computational learning theory learner acquiring knowledge teacher we introduce formal model learning capturing idea teachers may gaps knowledge in particular consider learning teacher labels examples positive instance concept learned negative instance concept learned instance unknown classification way knowledge concept class positive negative examples sufficient determine labelling examples labelled the goal learner compensate ignorance teacher attempting infer labels examples labelled rather learn approximation ternary labelling presented teacher thus goal learner still acquire knowledge teacher learner must also identify gaps this notion learning consistently ignorant teacher we present general results describing known learning algorithms used obtain algorithms learn consistently ignorant teacher we investigate learnability variety concept classes model including monomials monotone dnf formulas horn sentences decision trees dfas axisparallel boxes euclidean space among others both learnability nonlearnability results presented
in bagging brea one uses bootstrap replicates training set efr et try improve learning algorithms performance the computational requirements estimating resultant generalization error test set means crossvalidation often prohibitive leaveoneout crossvalidation one needs train underlying algorithm order times size training set number replicates this paper presents several techniques exploiting biasvariance decomposition gbd wol estimate generalization error bagged learning algorithm without invoking yet training underlying learning algorithm the best estimators exploits stacking wol in set experiments reported found accurate alternative crossvalidationbased estimator bagged algorithms error crossvalidationbased estimator underlying algorithms error this improvement particularly pronounced small test sets this suggests novel justification using bagging im proved estimation generalization error
this paper presents algorithm discovery building blocks genetic programming gp called adaptive representation learning arl the central idea arl adaptation problem representation extending set terminals functions set evolvable subroutines the set subroutines extracts common knowledge emerging evolutionary process acquires necessary structure solving problem arl supports subroutine creation deletion subroutine creation discovery performed automatically based differential parentoffspring fitness block activation subroutine deletion relies utility measure similar schema fitness window past generations the technique described tested problem controlling agent dynamic nondeterministic environment the automatic discovery subroutines help scale gp technique complex problems
in paper describe new technique exactly identifying certain classes readonce boolean formulas the method based sampling inputoutput behavior target formula probability distribution determined fixed point formulas amplification function defined probability output formula input bit independently probability p by performing various statistical tests easily sampled variants fixedpoint distribution able efficiently infer structural information logarithmicdepth formula high probability we apply results prove existence short universal identification sequences large classes formulas we also describe extensions algorithms handle high rates noise learn formulas unbounded depth valiants model respect specific distributions fl most research carried three authors mit laboratory computer science support provided nsf grant ccr aro grant daalk darpa contract nj grant siemens corporation an extended abstract paper appeared proceedings st annual symposium foundations computer science supported part ge foundation junior faculty grant z supported afosr grant afosr
we consider problem learning kterm dnf formulas using equivalence queries incomplete membership queries defined angluin slonim we demonstrate model applied nonmonotone classes namely describe polynomialtime algorithm exactly identifies kterm dnf formula kterm dnf hypothesis using incomplete membership queries equivalence queries class dnf formulas
most artificial neural networks anns fixed topology learning typically suffer number shortcomings result variations anns use dynamic topologies shown ability overcome many problems this paper introduces locationindependent transformations lits general strategy parallel implementation feedforward networks use dynamic topologies a lit creates set locationindependent nodes node computes part network output independent nodes using local information this type transformation allows efficient support adding deleting nodes dynamically learning this paper deals specifically lits localist annslocalist sense ultimately one node responsible output in particular paper presents lits two anns singlelayer competitive learning network b counterpropagation network combines elements supervised learning competitive learning the complexity learning execution algorithms anns linear number inputs logarithmic number nodes original network
we present new method obtaining local error bars nonlinear regression ie estimates confidence predicted values depend input we approach problem applying maximumlikelihood framework assumed distribution errors we demonstrate method first computergenerated data locally varying normally distributed target noise we apply laser data santa fe time series competition underlying system noise known quantization error error bars give local estimates model misspecification in cases method also provides weightedregression effect improves generalization performance
introspective reasoning systems reasoning processes form basis learning refine reasoning processes the robbie system uses introspective reasoning monitor retrieval process casebased planner detect retrieval inappropriate cases when retrieval problems detected source problems explained explanations used determine new indices use future case retrieval the goal robbies learning increase ability focus retrieval relevant cases aim simultaneously decreasing number candidates consider increasing likelihood system able successfully adapt retrieved cases fit current situation we evaluate benefits approach light empirical results examining effects index learning
inductive learning algorithms try obtain knowledge system set examples one difficult problems machine learning consists getting structure knowledge we propose algorithm able manage fuzzy information able learn structure rules represent system the algorithm gives reasonable small set fuzzy rules represent original set examples
since consider theory refinement tr possible key concept methodologically clear view knowledgebase maintenance try give structured overview actual stateoftheart tr this overview arranged along description tr search problem we explain basic approach show variety existing systems try give hints direction future research go
the problem protecting computer systems viewed generally problem learning distinguish self we describe method change detection based generation t cells immune system mathematical analysis reveals computational costs system preliminary experiments illustrate method might applied problem computer viruses
markov chain monte carlo mcmc methods introduced gelfand smith provide simulation based strategy statistical inference the application fields related methods well theoretical convergence properties intensively studied recent literature however many improvements still expected provide workable theoretically wellgrounded solutions problem monitoring convergence actual outputs mcmc algorithms ie convergence assessment problem in paper introduce discuss methodology based central limit theorem markov chains assess convergence mcmc algorithms instead searching approximate stationarity primarily intend control precision estimates invariant probability measure integrals functions respect measure confidence regions based normal approximation the first proposed control method tests normality hypothesis normalized averages functions markov chain independent parallel chains this normality control provides good guarantees whole state space explored even multimodal situations it lead automated stopping rules a second tool connected normality control based graphical monitoring stabilization variance n iterations near limiting variance appearing clt both methods require knowledge sampler driving chain in paper mainly focus finite state markov chains since setting allows us derive consistent estimates limiting variance variance n iterations heuristic procedures based berryesseen bounds investigated an extension continuous case also proposed numerical simulations illustrating performance methods given several examples finite chain multimodal invariant probability finite state random walk theoretical rate convergence stationarity known continuous state chain multimodal invariant probability issued gibbs sampler
this paper explores application temporal difference td learning sutton forecasting behavior dynamical systems realvalued outputs opposed gamelike situations the performance td learning comparison standard supervised learning depends amount noise present data in paper use deterministic chaotic time series lownoise laser for task direct fivestep ahead predictions experiments show standard supervised learning better td learning the td algorithm viewed linking adjacent predictions a similar effect obtained sharing internal representation network we thus compare two architectures paradigms first architecture separate hidden units consists individual networks five direct multistep prediction tasks second shared hidden units single larger hidden layer finds representation five predictions next five steps generated for data set find significant difference two architectures fl httpwwwcscoloradoeduandreashomehtml this paper available ftpftpcscoloradoedupubtimeseriesmypaperskazlasweigend nipspsz
we describe wakesleep algorithm allows multilayer unsupervised neural network build hierarchy representations sensory input the network bottomup recognition connections used convert sensory input underlying representations unlike artificial neural networks also topdown generative connections used reconstruct sensory input representations in wake phase learning algorithm network driven bottomup recognition connections topdown generative connections trained better reconstructing sensory input representation chosen recognition process in sleep phase network driven topdown generative connections produce fantasized representation fantasized sensory input the recognition connections trained better recovering fantasized representation fantasized sensory input in phases synaptic learning rule simple local the combined effect two phases create representations sensory input efficient following sense on average takes bits describe sensory input vector directly first describe representation sensory input chosen recognition process describe difference sensory input reconstruction chosen representation
technical report crgtr department computer science university toronto kings college road toronto canada ms a abstract bayesian inference begins prior distribution model parameters meant capture prior beliefs relationship modeled for multilayer perceptron networks parameters connection weights prior lacks direct meaning matters prior functions computed network implied prior weights in paper i show priors weights defined way corresponding priors functions reach reasonable limits number hidden units network goes infinity when using priors thus need limit size network order avoid overfitting the infinite network limit also provides insight properties different priors a gaussian prior hiddentooutput weights results gaussian process prior functions smooth brownian fractional brownian depending hidden unit activation function prior inputtohidden weights quite different effects obtained using priors based nongaussian stable distributions in networks one hidden layer combination gaussian nongaussian priors appears interesting
we present new algorithms reinforcement learning prove polynomial bounds resources required achieve nearoptimal return general markov decision processes after observing number actions required approach optimal return lower bounded mixing time t optimal policy undiscounted case horizon time t discounted case give algorithms requiring number actions total computation time polynomial t number states undiscounted discounted cases an interesting aspect algorithms explicit handling explorationexploitation tradeoff these first results reinforcement learning literature giving algorithms provably converge nearoptimal performance polynomial time general markov decision processes
a basic premise casebased reasoning cbr involves reasoning cases representations real episodes rather rules facts structures stated connection real episodes in fact cbr systems reason directly cases rather reason abstractions simplifications cases in paper argue pure casebased reasoning ie reasoning representations concrete reasonably complete we claim working representations satisfy criteria we illustrate argument examples three previous systems chef swale hypo well cookie cbr system developed first author
to appear g tesauro d s touretzky t k leen eds advances neural information processing systems mit press cambridge ma a straightforward approach curse dimensionality reinforcement learning dynamic programming replace lookup table generalizing function approximator neural net although successful domain backgammon guarantee convergence in paper show combination dynamic programming function approximation robust even benign cases may produce entirely wrong policy we introduce growsupport new algorithm safe divergence yet still reap benefits successful generalization
an exact model simple genetic algorithm developed permutation based representations permutation based representations used scheduling problems combinatorial problems traveling salesman problem a remapping function developed remap model permutations search space the mixing matrices various permutation based operators also developed
test functions commonly used evaluate effectiveness different search algorithms however results evaluation dependent test problems algorithms subject comparison unfortunately developing test suite evaluating competing search algorithms difficult without clearly defined evaluation goals in paper discuss basic principles used develop test suites examine role test suites used evaluate evolutionary search algorithms current test suites include functions easily solved simple search methods greedy hillclimbers some test functions also undesirable characteristics exaggerated dimensionality search space increased new methods examined constructing functions different degrees nonlinearity interactions cost evaluation scale respect dimensionality search space
source separation arises surprising number signal processing applications speech recognition eeg analysis in square linear blind source separation problem without time delays one must find unmixing matrix detangle result mixing n unknown independent sources unknown n fi n mixing matrix the recently introduced ica blind source separation algorithm baram roth bell sejnowski powerful surprisingly simple technique solving problem ica remarkable performing well despite making absolutely use temporal structure input this paper presents new algorithm contextual ica derives maximum likelihood density estimation formulation problem cica incorporate arbitrarily complex adaptive historysensitive source models thereby make use temporal structure input this allows separate number situations standard ica including sources low kurtosis colored gaussian sources sources gaussian histograms since ica special case cica mle derivation provides corollary rigorous derivation classic ica
we inv estigate applicability adaptive neural network problems timedependent input demonstrating deterministic parser natural language inputs significant syntactic complexity developed using recurrent connectionist architectures the traditional stacking mechanism known necessary proper treatment contextfree languages symbolic systems absent design subsumed recurrency network
in paper address problem constructing reliable neuralnet implementations given assumption particular implementation totally correct the approach taken paper organize inevitable errors minimize impact context multiversion system ie system functionality reproduced multiple versions together constitute neuralnet system the unique characteristics neural computing exploited order engineer reliable systems form diverse multiversion systems used together decision strategy majority vote theoretical notions methodological diversity contributing improvement system performance implemented tested an important aspect engineering optimal system overproduce components choose optimal subset three general techniques choosing final system components implemented evaluated several different approaches effective engineering complex multiversion systems designs realized evaluated determine overall reliability well reliability overall system comparison lesser reliability component substructures
littlewood miller present statistical framework dealing coincident failures multiversion software systems they develop theoretical model holds promise high system reliability use multiple diverse sets alternative versions in paper adapt framework investigate feasibility exploiting diversity observable multiple populations neural networks developed using diverse methodologies we evaluate generalisation improvements achieved range methodologically diverse network generation processes we attempt order constituent methodological features respect potential use engineering useful diversity we also define explore use relative measures diversity version sets guide potential exploiting interset diversity
during development lifecycle knowledgebased systems requirements system knowledge system change one types knowledge affected changing requirements controlknowledge prescribes ordering problemsolving steps machinelearning aid developers knowledgebased systems adapting systems changing requirements a number machinelearning techniques learning controlknowledge applied problemsolvers prodigyebl lex in knowledge engineering focus shifted construction knowledgelevel models problemsolving instead directly constructing knowledgebased system problemsolver in paper describe work progress apply machine learning techniques kads model expertise
results abbadingo one dfa learning abstract this paper first describes structure results abbadingo one dfa learning competition the competition designed encourage work algorithms scale wellboth larger dfas sparser training data we describe discuss winning algorithm rodney price orders state merges according amount evidence favor a second winning algorithm hugues juille described separate paper
m abeles h bergman e vaadia school medicine center neural computation hebrew university pob jerusalem israel e seidemann i meilijson school mathematical sciences raymond beverly sackler faculty exact sciences school medicine tel aviv university tel aviv israel i gat n tishby institute computer science center neural computation hebrew university jerusalem israel
in work present new bottomup algorithm decision tree pruning efficient requiring single pass given tree prove strong performance guarantee generalization error resulting pruned tree we work typical setting given tree t may derived given training sample s thus may badly overfit s in setting give bounds amount additional generalization error pruning suffers compared optimal pruning t more generally results show pruning t small error whose size small compared jsj algorithm find pruning whose error much larger this style result called index resolvability result barron cover context density estimation a novel feature algorithm locality decision prune subtree based entirely properties subtree sample reaching to analyze algorithm develop tools local uniform convergence generalization standard notion may prove useful settings
we investigate application support vector machines svms computer vision svm learning technique developed v vapnik team att bell labs seen new method training polynomial neural network radial basis functions classifiers the decision surfaces found solving linearly constrained quadratic programming problem this optimization problem challenging quadratic form completely dense memory requirements grow square number data points we present decomposition algorithm guarantees global optimality used train svms large data sets the main idea behind decomposition iterative solution subproblems evaluation optimality conditions used generate improved iterative values also establish stopping criteria algorithm we present experimental results implementation svm demonstrate feasibility approach face detection problem involves data set data points
one open problems listed rivest schapire whether copies l fl algorithm combined one better performance this paper describes algorithm called d fl combination the idea represent states learned model using observable symbols well hidden symbols constructed learning these hidden symbols created reflect distinct behaviors model states the distinct behaviors represented local distinguishing experiments ldes confused global distinguishing sequences ldes created learners prediction mismatches actual observation unknown machine to synchronize model environment ldes also concatenated form homing sequence it shown d fl learn probability model approximation unknown machine number actions polynomial size environment
all models natural systems represent abstraction simplification system thus models suffer validation problem should believe results model bearing reality this particularly acute problem alife models evolution ecosystems the time scale evolution complexity ecosystems make controlled experiments difficult if alife ever contribute significantly biology must find methods build confidence models one alternative experimental tests model validate previously verified theory i applied series ecological evolutionary validation tests model species diversification examination predatorprey dynamics trophic cascades competitive exclusion adaptation speciesarea curve model shown course grained spatial structure inadequate capture realistic dynamics ecosystem only spatial structure extended local patch dynamics model begin behave realistically wide range parameters validation ecological dynamics model provides indirect support evolutionary behavior species within ecosystem every model abstraction simplification the goal model capture essence system real world behavior model matches behavior real system thus model may ask valid representation real system answering question problem validation traditionally try disprove validity model collecting data real system comparing predictions model in artificial life rarely luxury artificial life models tend highly abstract general field striving discover general properties life this makes experimental validation extremely difficult the time scale evolution tends restrict experiments observation fossil record benton example manipulation organisms extremely short lifecycles simplified environments krukonis example similarly complexity size ecosystems makes ecological experiments cumbersome difficult control an alternative form validation pursued indirectly reference ecological evolutionary theory instead asking model matches experimental data ask model matches understanding dynamics ecology evolution then extent theories ecology evolution validated experimental observations disprove validity model fails match theories what follows example technique applied model designed examine factors impact origin maintenance species diversity while purpose model explore new theoretical ground biology ecological evolutionary dynamics model validated theories predation competition adaptation island biogeography hraber milne looked genotype diversity presence absence selection varying mutation rates echo model holland mirroring bedau et als results found genotypic diversity greatest
one issues evolutionary algorithms eas relative importance two search operators mutation crossover genetic algorithms gas genetic programming gp stress role crossover evolutionary programming ep evolution strategies ess stress role mutation the existence many different forms crossover complicates issue despite theoretical analysis appears difficult decide priori form crossover use even crossover used one possible solution difficulty ea selfadaptive ie ea dynamically modify forms crossover use often use solves problem this paper describes adaptive mechanism controlling use crossover ea explores behavior mechanism number different situations an improvement adaptive mechanism presented surprisingly improvement also used enhance performance nonadaptive ea
graphical techniques modeling dependencies random variables explored variety different areas including statistics statistical physics artificial intelligence speech recognition image processing genetics formalisms manipulating models developed relatively independently research communities in paper explore hidden markov models hmms related structures within general framework probabilistic independence networks pins the paper contains selfcontained review basic principles pins it shown wellknown forwardbackward fb viterbi algorithms hmms special cases general inference algorithms arbitrary pins furthermore existence inference estimation algorithms general graphical models provides set analysis tools hmm practitioners wish explore richer class hmm structures examples relatively complex models handle sensor fusion coarticulation speech recognition introduced treated within graphical model framework illustrate advantages general approach this report describes research done department information computer science university california irvine jet propulsion laboratory california institute technology microsoft research center biological computational learning artificial intelligence laboratory massachusetts institute technology the authors contacted pjsaigjplnasagov heckermamicrosoftcom jordanpsychemitedu support cbcl provided part grant nsf asc support laboratorys artificial intelligence research provided part advanced research projects agency dept defense mij gratefully acknowledges discussions steffen lauritzen application ipf algorithm upins
perceptron learning randomly labeled patterns analyzed using gibbs distribution set realizable labelings patterns the entropy distribution extension vapnikchervonenkis vc entropy reducing exactly limit infinite temperature the close relationship vc gardner entropies seen within replica formalism there recent progress towards understanding relationship statistical physics vapnikchervonenkis vc approaches learning theory the two approaches unified statistical mechanics based vc entropy this paper treats case learning randomly labeled patterns capacity problem extends results previous work finite temperature as explained companion paper extension important treating generalization problem occurs context learning patterns labeled target rule our general framework illustrated simple perceptron sgnw x maps n dimensional realvalued input x valued output given sample x x x inputs weight vector w determines labeling l l l sample via l sgnw x the weight vector w defines normal hyperplane separates positive negative examples the training error labeling l respect reference labeling l defined x l l fraction different labels two labelings we consider case reference labeling chosen random address issue
in pathimitation task one agent traces path second agents sensory field the second agent reproduce path exactly ie move sequence locations visited first agent this nontrivial behaviour whose acquisition might expected involve specialpurpose ie strongly biased learning machinery however present paper shows case the behaviour acquired using fairly primitive learning regime provided agents environment made pass specific sequence dynamic states
this research aims demonstrate solution artificial ant problem likely nongeneral relying specific characteristics santa fe trail it presents consistent method promotes producing general solutions using concepts training testing machine learning research method useful producing general behaviours simulation environments
dynamic bayesian networks dbns useful tool representing complex stochastic processes recent developments inference learning dbns allow use realworld applications in paper apply dbns problem speech recognition the factored state representation enabled dbns allows us explicitly represent longterm articulatory acoustic context addition phoneticstate information maintained hidden markov models hmms furthermore enables us model shortterm correlations among multiple observation streams within single timeframes given dbn structure capable representing long shortterm correlations applied em algorithm learn models parameters the use structured dbn models decreased error rate largevocabulary isolatedword recognition task compared discrete hmm also improved significantly published results task this first successful application dbns largescale speech recognition problem investigation learned models indicates hidden state variables strongly correlated acoustic properties speech signal
we describe evaluate multinetwork connectionist systems composed expert networks by preprocessing training data competitive learning network system automatically organizes process decomposition expert subtasks using several different types challenging problem assess approach degree automatically generated experts really specialists predictable subset overall task comparison decompositions equivalent singlenetworks in addition assess utility approach alongside competition nonexpert multiversion systems previously developed measures diversity systems also applied provide quantitative assessment degree specialization obtained expertnet ensemble we show types problem abstract welldefined datadefined automatic decomposition produce effective set specialist networks together support high level performance curiously study provide support differential effectiveness within two classes problem continuous homogeneous functions discrete discontinuous functions
let us present briefly learning problem address chapter following the ultimate goal modelling mapping f x multidimensional input x output the output multidimensional mostly address situations one dimensional real value furthermore take account fact scarcely ever observe actual true mapping f x this due perturbations eg observational noise we rather joint probability p x we expect probability peaked values x corresponding mapping we focus automatic learning example a set d data sampled joint distribution p x p yjx p x collected with help set try identify model data parameterised set learning optimisation the fit model system given point x measured using criterion representing distance model prediction b system e f w x this local risk the performance model measured expected this quantity represents ability yield good performance possible situations ie x pairs thus called generalisation error the optimal set parameters w f w x b
practical optimization problems jobshop scheduling often involve optimization criteria change time repairbased frameworks identified flexible computational paradigms difficult combinatorial optimization problems since control problem repairbased optimization severe reinforcement learning rl techniques potentially helpful however fundamental assumptions made traditional rl algorithms valid repairbased optimization casebased reasoning cbr compensates limitations traditional rl approaches in paper present casebased reasoning rl approach implemented c a b i n s system repairbased optimization we chose jobshop scheduling testbed approach our experimental results show c a b i n s able effectively solve problems changing optimization criteria known system exist implicitly extensional manner case base
we introduced earlier use regularisation learning procedure it understood regularisation often necessity increase quality results even unregularised solution acceptable likely regularisation produce improvement performance there exist method giving directly best value regularisation parameter even linear case the topic chapter thus propose methods estimate best value the best one leads smallest generalisation error methods presented compared propose estimators generalisation error this estimation used approximate best regularisation level in sections present validationbased techniques they estimate generalisation error basis extra data in sections deal algebraic estimates error use extra data rely number assumptions the contribution chapter present techniques analyse ground we also present short derivations clarifying links different estimators generalisation error well comparison during course chapter error quadratic difference for validationbased methods possible consider kind error without modification method on hand algebraic estimates specific quadratic cost adapting another cost function would require derive new expressions estimators
evolutionary approaches advocated automate robot design some research work shown success evolving controllers robots genetic approaches as observe however controller also robot body affect behavior robot robot system in paper develop hybrid gpga approach evolve controllers robot bodies achieve behaviorspecified tasks in order assess performance developed approach used evolve simulated agent controller body obstacle avoidance simulated environment experimental results show promise work in addition importance coevolving controllers robot bodies analyzed discussed paper
when delays set a ffi fj k j kg p jffi depends the estimated probabilities become quite noisy number elements set a b small for reason estimate standard deviation p jffi notice estimate empirical average binomial variable either given couple satisfied conditions ffi the standard deviation estimated easily generally speaking p jffi increases laxer output test ffi approaches stricter input condition let us define p maximum ffi p jffi p max ffigt p jffi the dependability index defined p represents much data passes continuity test input information available this dependability index measures much remaining continuity information associated involving input this index averaged respect probability p p it clear therefore average positive quantities furthermore system deterministic dependability zero certain number inputs sum averages saturates if system also noisefree sum for greater embedding dimension refers results obtained using method statistical variable selection statistical variable selection feature selection encompasses number techniques aimed choosing relevant subset input variables regression classification problem as rest document limit considerations related regression problem even though methods discussed apply classification well variable selection seen part data analysis problem selection discard variable tells us relevance associated measurement modelled system in general setting purely combinatorial problem given v possible variables v possible subsets including empty set full set variables given performance measure prediction error optimal scheme test subset choose one gives best performance it easy see extensive scheme viable number variables rather low identifying v models variables requires much computation a number techniques devised overcome combinatorial limit some use iterative locally optimal technique construct estimate relevant subset number steps we refer stepwise selection methods con fused stepwise regression subset methods address in forward selection start empty set variables at step select candidate variable using selection criteria check whether variable added set iterate given stop condition reached on contrary backward elimination methods start full set input variables at step least significant variable selected according selection criteria if variable irrelevant removed process iterated stop condition reached it easy devise examples inclusion variable causes previously included variable become irrelevant it thus seems appropriate consider running backward elimination time new variable added forward selection this combination ap proaches known stepwise regression linear regression con
we used genetic programming evolve b h topology sizing numerical values component analog electrical circuit correctly classify incoming analog electrical signal three categories then r e p e r r e f u r c e w dynamically changed adding new source run the p p e r e c r b e h w h e
an essential component intelligent agent ability observe encode use information environment traditional approaches genetic programming focused evolving functional reactive programs minimal use state this paper presents approach investigating evolution learning planning memory using genetic programming the approach uses multiphasic fitness environment enforces use memory allows fairly straightforward comprehension evolved representations an illustrative problem gold collection used demonstrate usefulness approach the results indicate approach evolve programs store simple representations environments use representations produce simple plans
interest genetic algorithms expanding rapidly this paper reviews software environments programming genetic algorithms ga as background initially preview genetic algorithms models programming next classify ga software environments three main categories applicationoriented algorithmoriented toolkits for category ga programming environment review common features present case study leading environment
neural network pruning methods level individual network parameters eg connection weights improve generalization shown empirical study however open problem pruning methods known today eg obd obs autoprune epsiprune selection number parameters removed pruning step pruning strength this work presents pruning method lprune automatically adapts pruning strength evolution weights loss generalization training the method requires algorithm parameter adjustment user results statistical significance tests comparing autoprune lprune static networks early stopping given based extensive experimentation different problems the results indicate training pruning often significantly better rarely significantly worse training early stopping without pruning furthermore lprune often superior autoprune superior obd diagnosis tasks unless severe pruning early training process required
the relative performance different methods classifier learning varies across domains some recent instance based learning ibl methods ibmvdm use similarity measures based conditional class probabilities these probabilities key component naive bayes methods given commonality approach interest consider differences two methods linked relative performance different domains here interpret naive bayes ibl like framework identifying differences naive bayes ibmvdm framework experiments variants ibmvdm lie naive bayes framework conducted sixteen domains the results strongly suggest relative performance naive bayes ibmvdm linked extent class satisfactorily represented single instance ibl framework however factor appears significant
assert demonstrates theory refinement techniques developed machine learning used ef fectively build student models intelligent tutoring systems this application unique since inverts normal goal theory refinement correcting errors knowledge base introducing a comprehensive experiment involving lar ge number students interacting automated tutor teaching concepts c programming used evaluate approach this experiment demonstrated ability theory refinement generate accurate student models raw induction well ability resulting models support individualized feedback actually improves students subsequent performance carr b goldstein i overlays theory modeling computer aided instruction t echnical report a i memo cambridge ma mit sandberg j barnard y education technology what know and ai artificial intelligence communications
the monitoring control dynamic system depends crucially ability reason current status future trajectory in case stochastic system tasks typically involve use belief statea probability distribution state process given point time unfortunately state spaces complex processes large making explicit representation belief state intractable even dynamic bayesian networks dbns process represented compactly representation belief state intractable we investigate idea utilizing compact approximation true belief state analyze conditions errors due approximations taken lifetime process accumulate make answers completely irrelevant we show error belief state contracts exponentially process evolves thus even multiple approximations error process remains bounded indefinitely we show additional structure dbn used design approximation scheme improving performance significantly we demonstrate applicability ideas context monitoring task showing orders magnitude faster inference achieved small degradation accuracy
nonaxiomatic reasoning system nars designed generalpurpose intelligent reasoning system adaptive works insufficient knowledge resources this paper focuses components nars contribute systems induction capacity shows traditional problems induction addressed system the nars approach induction uses termoriented formal language experiencegrounded semantics consistently interprets various types uncertainty an induction rule generates conclusions common instance terms revision rule combines evidence different sources in nars induction types inference deduction abduction based semantic foundation cooperate inference activities system the systems control mechanism makes knowledgedriven contextdependent inference possible
although casebased reasoning cbr natural formulation many problems previous work cbr applied design made apparent elements cbr paradigm prevented widely applied at time evaluating constraint satisfaction techniques design found commonality motivation repairbased constraint satisfaction problems csp case adaptation this led us combine two methodologies order gain advantages csp casebased reasoning allowing cbr widely flexibly applied in combining two methodologies found unexpected synergy commonality approaches this paper describes synergy commonality emerged combined casebased constraintbased reasoning gives brief overview continuing future work exploiting emergent synergy combining reasoning modes
in paper study dual version ridge regression procedure it allows us perform nonlinear regression constructing linear regression function high dimensional feature space the feature space representation result large increase number parameters used algorithm in order combat curse dimensionality algorithm allows use kernel functions used support vector methods we also discuss powerful family kernel functions constructed using anova decomposition method kernel corresponding splines infinite number nodes this paper introduces regression estimation algorithm combination two elements dual version ridge regression applied anova enhancement infinitenode splines experimental results presented based boston housing data set indicate performance algorithm relative algorithms
a twoeye visual environment used training network bcm neurons we study effect misalignment synaptic density functions two eyes formation orientation selectivity ocular dominance lateral inhibition network the visual environment use composed natural images we show bcm rule natural image environment binocular cortical misalignment sufficient producing networks orientation selective cells ocular dominance columns this work extension previous single cell misalignment model shouval et al
the von mises distribution maximum entropy distribution it corresponds distribution angle compass needle uniform magnetic field direction concentration parameter the concentration parameter ratio field strength temperature thermal fluctuations previously obtained bayesian estimator von mises distribution parameters using informationtheoretic minimum message length mml principle here examine variety bayesian estimation techniques examining posterior distribution polar cartesian coordinates we compare mml estimator fellow bayesian techniques range classical estimators we find bayesian estimators outperform classical estimators
neuralnetwork ensembles shown accurate classification techniques previous work shown effective ensemble consist networks highly correct ones make errors different parts input space well most existing techniques however indirectly address problem creating set networks in paper present technique called addemup uses genetic algorithms directly search accurate diverse set trained networks addemup works first creating initial population uses genetic operators continually create new networks keeping set networks accurate possible disagreeing much possible experiments three dna problems show addemup able generate set trained networks accurate several existing approaches experiments also show addemup able effectively incorporate prior knowledge available improve quality ensemble
classifier induction algorithms differ inductive hypotheses represent search space hypotheses no classifier better another problems selective superiority this paper empirically compares six classifier induction algorithms diagnosis equine colic prediction mortality the classification based simultaneously analyzing sixteen features measured patient the relative merits algorithms linear regression decision trees nearest neighbor classifiers model class selection system logistic regression without feature selection neural nets qualitatively discussed generalization accuracies quantitatively analyzed
using multiparent diagonal scanning crossover gas reproduction operators obtain adjustable arity hereby sexuality becomes graded feature instead boolean one our main objective relate performance gas extent sexuality used reproduction less arbitrary functions reported current literature we investigate ga behaviour kauffmans nklandscapes allow systematic characterization user control ruggedness fitness landscape we test gas varying extent sexuality ranging asexual sexual our tests performed two types nklandscapes landscapes random landscapes nearest neighbour epistasis for landscape types selected landscapes range ruggednesses the results confirm superiority sexual recombination mildly epistatic problems
this paper discusses unsupervised learning problem an important part unsupervised learning problem determining number constituent groups components classes best describes data we apply minimum message length mml criterion unsupervised learning problem modifying earlier mml application we give empirical comparison criteria prominent literature estimating number components data set we conclude minimum message length criterion performs better alternatives data considered unsupervised learning tasks
the minimum message length mml technique applied problem estimating parameters multivariate gaussian model correlation structure modelled single common factor implicit estimator equations derived compared obtained maximum likelihood ml analysis unlike ml mml estimators remain consistent used estimate factor loadings factor scores tests simulated data show mml estimates av erage accurate ml estimates former exist if data show little evidence factor mml estimate collapses it shown condition existence mml estimate essentially log likelihood ratio favour factor model exceed value expected null nofactor hypotheses
this paper firstly provides reappraisal development techniques inverting deduction secondly introduces modedirected inverse entailment mdie generalisation enhancement previous approaches thirdly describes implementation mdie progol system progol implemented c available anonymous ftp the reassessment previous techniques terms inverse entailment leads new results learning positive data inverting implication pairs clauses
firstorder learning involves finding clauseform definition relation examples relation relevant background information in paper particular firstorder learning system modified customize finding definitions functional relations this restriction leads faster learning times cases definitions higher predictive accuracy other firstorder learning systems might benefit similar specialization
technical report d epartement dinformatique et recherche op erationnelle universit e de montr eal abstract boosting general method improving performance learning algorithm consistently generates classifiers need perform slightly better random guessing a recently proposed promising boosting algorithm adaboost it applied great success several benchmark machine learning problems using rather simple learning algorithms particular decision trees in paper use adaboost improve performances neural networks applied character recognition tasks we compare training methods based sampling training set weighting cost function our system achieves error data base online handwritten digits writers adaptive boosting multilayer network achieved error uci letters offline characters data set
the ability inductive learning system find good solution given problem dependent upon representation used features problem systems perform constructive induction able change representation constructing new features we describe important realworld problem finding genes dna believe offers interesting challenge constructiveinduction researchers we report experiments demonstrate two different input representations task result significantly different generalization performance neural networks decision trees neural symbolic methods constructive induction fail bridge gap two representations we believe realworld domain provides interesting challenge problem constructive induction relationship two representations well known representational shift involved construct ing better representation imposing
dimacs technical report july a preliminary version paper appeared proceedings eurocolt conference published volume lecture notes artificial intelligence pages springerverlag the journal version appear algoritmica email beimeldimacsrutgersedu httpdimacsrutgersedubeimel part research done author phd student technion email eyalkcstechnionacil httpwwwcstechnionacileyalk this research supported technion vpr fund japan technion society research fund
this paper demonstrates capabilities foidl inductive logic programming ilp system whose distinguishing characteristics ability produce firstorder decision lists use output completeness assumption substitute negative examples use intensional background knowledge the development foidl originally motivated problem learning generate past tense english verbs however paper demonstrates superior performance two different sets benchmark ilp problems tests finite element mesh design problem show foidls decision lists enable produce generally accurate results range methods previously applied problem tests selection listprocessing problems bratkos introductory prolog text demonstrate combination implicit negatives intensionality allow foidl learn correct programs far fewer examples foil
further results controllability abstract this paper studies controllability properties recurrent neural networks the new contributions extension previous result slightly different model formulation proof necessary sufficient condition analysis lowdimensional case recurrent neural networks fl
c flmit media lab perceptual computing learning common sense technical report nov abstract we present algorithms coupling training hidden markov models hmms model interacting processes demonstrate superiority conventional hmms vision task classifying twohanded actions hmms perhaps successful framework perceptual computing modeling classifying dynamic behaviors popular offer dynamic time warping training algorithm clear bayesian semantics however markovian framework makes strong restrictive assumptions system generating signalthat single process small number states extremely limited state memory the singleprocess model often inappropriate vision speech applications resulting low ceilings model performance coupled hmms provide efficient way resolve many problems offer superior training speeds model likelihoods robustness initial conditions
the general framework reinforcement learning proposed several researchers solution optimization problems realization adaptive control schemes to allow efficient application reinforcement learning either areas necessary solve structural temporal credit assignment problem in paper concentrate latter usually tackled use learning algorithms employ discounted rewards we argue realistic problems kind solution satisfactory since address effect noise originating different experiences allow easy explanation parameters involved learning process as possible solution propose keep delayed reward undiscounted discount actual adaptation rate empirical results show dependent kind discount used amore stable convergence even increase performance obtained
for certain classes problems defined twodimensional domains grid structure optimization problems involving assignment grid cells processors present nonlinear network model problem partitioning tasks among processors minimize interprocessor communication minimizing interprocessor communication context shown equivalent tiling domain minimize total tile perimeter tile corresponds collection tasks assigned processor a tight lower bound perimeter tile function area developed we show generate minimumperimeter tiles by using assignments corresponding nearrectangular minimumperimeter tiles closed form solutions developed certain classes domains we conclude computational results parallel highlevel genetic algorithms produced good sometimes provably optimal solutions large perimeter minimization problems
we report successful application td value function approximation task jobshop scheduling our scheduling problems based problem scheduling payload processing steps nasa space shuttle program the value function approximated layer feedforward network sigmoid units a onestep lookahead greedy algorithm using learned evaluation function outperforms best existing algorithm task iterative repair method incorporating simulated annealing to understand reasons performance improvement paper introduces several measurements learning process discusses several hypotheses suggested measurements we conclude use value function approximation source difficulty method fact may explain success method independent use value iteration additional experiments required discriminate among hypotheses
several metrics used empirical studies explore mechanisms convergence genetic algorithms the metric designed measure consistency arbitrary ranking hyperplanes partition respect target string walsh coefficients calculated small functions order characterize sources linear nonlinear interactions a simple deception measure also developed look closely effects increasing nonlinearity functions correlations metric deception measure discussed relationships convergence behavior simple genetic algorithm studied large sets functions varying degrees nonlinearity
exercises problems ordered increasing order difficulty teaching problemsolving exercises widely used pedagogic technique a computational reason knowledge gained solving simple problems useful efficiently solving difficult problems we adopt approach learning exercises acquire searchcontrol knowledge form goaldecomposition rules drules drules first order learned using new generalizeandtest algorithm based inductive logic programming techniques we demonstrate feasibility approach applying two planning mains
foveal vision features imagers graded acuity coupled context sensitive sensor gaze control analogous prevalent throughout vertebrate vision foveal vision operates efficiently uniform acuity vision resolution treated dynamically allocatable resource requires refined visual attention mechanism we demonstrate reinforcement learning rl significantly improves performance foveal visual attention overall vision system task model based target recognition a simulated foveal vision system shown classify targets fewer fixations learning strategies acquisition visual information relevant task learning generalize strategies ambiguous unexpected scenario conditions
a horn definition set horn clauses head literal in paper consider learning nonrecursive functionfree firstorder horn definitions we show class exactly learnable equivalence membership queries it follows class pac learnable using examples membership queries our results shown applicable learning efficient goaldecomposition rules planning domains
speedup learning study improving problemsolving performance experience outside guidance we describe system successfully combines best features explanationbased learning empirical learning learn goal decomposition rules examples successful problem solving membership queries we demonstrate system efficiently learn effective decomposition rules three different domains our results suggest theoryguided empirical learning overcome problems purely explanationbased learning purely empirical learning effective speedup learning method
the authors coworkers recently proved general theorems global stabilization linear systems subject control saturation this paper develops detail explicit design linearized equations longitudinal flight control f aircraft tests obtained controller original nonlinear model this paper represents first detailed derivation controller using techniques question results encouraging
when casebased planner retrieving previous case preparation solving new similar problem often aware implicit features new problem situation determine particular case may successfully applied this means cases may fail improve planners performance by detecting explaining case failures occur retrieval may improved incrementally in paper provide definition case failure casebased planner dersnlp derivation replay snlp solves new problems replaying previous plan derivations we provide explanationbased learning ebl techniques detecting constructing reasons case failure we also describe case library organized incorporate failure information produced finally present empirical study demonstrates effectiveness approach improving performance dersnlp
we describe analyze mixture model supervised learning probabilistic transducers we devise online learning algorithm efficiently infers structure estimates parameters probabilistic transducer mixture theoretical analysis comparative simulations indicate learning algorithm tracks best transducer arbitrarily large possibly infinite pool models we also present application model inducing noun phrase recognizer
differentiation nodes competitive learning network conventionally achieved competition basis neural activity simple inhibitory mechanisms limited sparse representations decorrelation factorization schemes support distributed representations computationally unattractive by letting neural plasticity mediate competitive interaction instead obtain diffuse nonadaptive alternatives fully distributed representations we use technique simplify improve binary information gain optimization algorithm feature extraction schraudolph sejnowski approach could used improve learning algorithms
it shown bayesian training backpropagation neural networks feasibly performed hybrid monte carlo method this approach allows true predictive distribution test case given set training cases approximated arbitrarily closely contrast previous approaches approximate posterior weight distribution gaussian in work hybrid monte carlo method implemented conjunction simulated annealing order speed relaxation good region parameter space the method applied test problem demonstrating produce good predictions well indication uncertainty predictions appropriate weight scaling factors found automatically by applying known techniques calculation free energy differences also possible compare merits different network architectures the work described also applicable wide variety statistical models neural networks
former layouts contain much knowhow architects a generic automatic way formalize knowhow order use computer would save lot effort money however seems way the access knowhow layouts developing generic software tool reuse former layouts consider every part architectual domain things like personal style tools used today consider small parts architectual domain any personal style ignored isnt possible build basic tool adjusted content former layouts may extended incremently modeling much domain desirable this paper describe reuse tool perform task focusing topological geometrical binary relations
an important difficult prediction task many domains particularly medical decision making prognosis prognosis presents unique set problems learning system outputs unknown this paper presents new approach prognostic prediction using ideas nonparametric statistics fully utilize available information neural architecture the technique applied breast cancer prognosis resulting flexible accurate models may play role prevent ing unnecessary surgeries
in paper new approach presented transfers basic idea evolution strategies ess gas mutation rates changed endogeneous items adapting search process first experimental results presented indicate environmentdependent selfadaptation appropriate settings mutation rate possible even gas
previous teaching models learning theory community batch models that models teacher generated single set helpful examples present learner in paper present interactive model learner ability ask queries query learning model angluin we show model least powerful previous teaching models we also show anything learnable queries even randomized learner teachable model in previous teaching models classes shown teachable known efficiently learnable an important concept class known learnable dnf formulas we demonstrate power approach providing deterministic teacher learner class dnf formulas the learner makes equivalence queries hypotheses also dnf formulas
a neuralnetwork ensemble successful technique outputs set separately trained neural network combined form one unified prediction an effective ensemble consist set networks highly correct ones make errors different parts input space well however existing techniques indirectly address problem creating set we present algorithm called addemup uses genetic algorithms explicitly search highly diverse set accurate trained networks addemup works first creating initial population uses genetic operators continually create new networks keeping set networks highly accurate disagreeing much possible experiments four realworld domains show addemup able generate set trained networks accurate several existing ensemble approaches experiments also show addemup able effectively incorporate prior knowledge available improve quality ensemble
default logic encounters conceptual difficulties representing common sense reasoning tasks we argue try formulate modular default rules presumed work circumstances we need take account importance context continuously evolving reasoning process sequential thresholding quantitative counterpart default logic makes explicit role context plays construction nonmonotonic extension we present semantic characterization generic nonmonotonic reasoning well instantiations pertaining default logic sequential thresholding this provides link two mechanisms well way integrate two beneficial
the problem maximizing expected total discounted reward completely observable markovian environment ie markov decision process mdp models particular class sequential decision problems algorithms developed making optimal decisions mdps given either mdp specification opportunity interact mdp time recently sequential decisionmaking problems studied prompting development new algorithms analyses we describe new generalized model subsumes mdps well many recent variations we prove basic results concerning model develop generalizations value iteration policy iteration modelbased reinforcementlearning qlearning used make optimal decisions generalized model various assumptions applications theory particular models described including riskaverse mdps explorationsensitive mdps sarsa qlearning spreading twoplayer games approximate max picking via sampling central results contraction property value operator stochasticapproximation theorem reduces asynchronous convergence synchronous convergence
incorporating declarative bias prior knowledge learning active research topic machine learning treestructured bias specifies prior knowledge tree relevance relationships attributes this paper presents learning algorithm implements treestructured bias ie learns target function probably approximately correctly random examples membership queries obeys given treestructured bias the theoretical predictions paper em pirically validated
we introduce large family boltzmann machines trained using standard gradient descent the networks one layers hidden units treelike connectivity we show implement supervised learning algorithm boltzmann machines exactly without resort simulated meanfield annealing the stochastic averages yield gradients weight space computed technique decimation we present results problems n bit parity detection hidden symmetries
the data describing resolutions telephone network local loop troubles wish learn rules dispatching technicians notoriously unreliable anecdotes abound detailing reasons resolution entered technician would valid ranging sympathy fear ignorance negligence management pressure in paper describe four different approaches dealing problem bad data order first determine whether machine learning promise domain determine well machine learning might perform we offer evidence machine learning help build dispatching method perform better system currently place
we study notions bias variance classification rules following efron develop decomposition prediction error natural components then derive bootstrap estimates components illustrate used describe error behaviour classifier practice in process also obtain bootstrap estimate error bagged classifier
this paper deals systems obtained linear timeinvariant continuousor discretetime devices followed function provides sign output such systems appear naturally study quantized observations well signal processing neural network theory results given observability minimal realizations systemtheoretic concepts certain major differences exist linear case results generalize surprisingly straightforward manner
casebased reasoning cbr paradigm close designer behavior conceptual design seems fruitable computer aideddesign approach library design cases available the goal paper presents general framework casebased retrieval system repro supports chemical process design the crucial problems like case representation structural similarity measure widely described the presented experimental results expert evaluation shows usefulness described system real world problems the papers ends discussion concerning research problems future work
hollands analysis sources power genetic algorithms served guidance applications genetic algorithms years the technique applying recombination operator crossover population individuals key power neverless number contradictory results concerning crossover operators respect overall performance recently example genetic algorithms used design neural network modules control circuits in studies genetic algorithm without crossover outperformed genetic algorithm crossover this report reexamines studies concludes results caused small population size new results presented illustrate effectiveness crossover population size larger from performance view results indicate better neural networks evolved shorter time genetic algorithm uses crossover
in paper explore use genetic algorithms gas construct system called gabil continually learns refines concept classification rules interac tion environment the performance system compared two concept learners newgem c suite target concepts from comparison identify strategies responsible success concept learners we implement subset strategies within gabil produce multistrategy concept learner finally multistrategy concept learner enhanced allowing gas adaptively select appropriate strategies
suppose learning task select one hypothesis set hypotheses may example generated multiple applications randomized learning algorithm a common approach evaluate hypothesis set previously unseen crossvalidation data select hypothesis lowest crossvalidation error but crossvalidation data partially corrupted noise set hypotheses selecting large folklore also warns overfitting crossvalidation data klockars sax tukey tukey in paper explain overfitting really occurs show surprising result overcome selecting hypothesis higher crossvalidation error others lower crossvalidation errors we give reasons selecting hypothesis lowest crossvalidation error propose new algorithm loocvcv uses computationally efficient form leaveoneout crossvalidation select hypothesis finally present experimental results one domain show loocvcv consistently beating picking hypothesis lowest crossvalidation error even using reasonably large crossvalidation sets
we investigate learning membership equivalence queries assuming information provided learner incomplete by incomplete mean membership queries may answered i dont know this model worstcase version incomplete membership query model angluin slonim it attempts model practical learning situations including experiment lang baum describe teacher may unable answer reliably queries critical learning algorithm we present algorithms learn monotone kterm dnf membership queries learn monotone dnf membership equivalence queries compared complete information case query complexity increases additive term linear number i dont know answers received we also observe blowup number queries general exponential new model incomplete membership model
we show wellknown lyapunov sufficient condition inputtostate stability also necessary settling positively open question raised several authors past years additional characterizations iss property including one terms nonlinear stability margins also provided
determination consistent initial conditions important aspect solution differential algebraic equations daes specification inconsistent initial conditions even slightly inconsistent often leads failure initialization problem in paper present successive linear programming slp approach solution dae derivative array equations initialization problem the slp formulation handles roundoff errors inconsistent user specifications among others allows reliable convergence strategies incorporate variable bounds trust region concepts a new consistent set initial conditions obtained minimizing deviation variable values specified ones for problems discontinuities caused step change input functions new criterion presented identifying subset variables continuous across discontinuity the lp formulation applied determine consistent set initial conditions solution problem domain discontinuity numerous example problems solved illustrate concepts
in field optimization machine learning techniques efficient promising tools like genetic algorithms gas hillclimbing designed in field evolving nondeterminism end model proposes inventive way explore space states combined use simulated coevolution remedies drawbacks previous techniques even allow model outperform difficult problems this new model applied sorting network problem reference problem challenged many computer scientists original oneplayer game named solitaire for first problem end model able build scratch sorting networks good best known input problem it even improved one comparator years old result input problem for solitaire game end evolved strategy comparable human designed strategy
in field optimization machine learning techniques efficient promising tools like genetic algorithms gas hillclimbing designed in field evolving nondeterminism end model presented paper proposes inventive way explore space states using simulated incremental coevolution organisms remedies drawbacks previous techniques even allow model outperform difficult problems this new model applied sorting network problem reference problem challenged many computer scientists original oneplayer game named solitaire for first problem end model able build scratch sorting networks good best known input problem it even improved one comparator years old result input problem for solitaire game end evolved strategy comparable human designed strategy
most casebased reasoning systems used single best similar case basis solution for many problems however single exact solution rather range acceptable answers we use cases basis solution also indicate boundaries within solution found we solve problems choosing point within boundaries in paper i discuss use cases illustrations chiron system i implemented domain personal income tax planning
we used genetic programming develop efficient image processing software the ultimate goal work detect certain signs breast cancer detected current segmentation classification methods traditional techniques relatively good job segmenting classifying smallscale features mammograms microcalcification clusters our stronglytyped genetic programs work multiresolution representation mammogram aimed handling features medium large scales stellated lesions architectural distortions the main problem efficiency we employ program optimizations speed evolution process factor ten in paper present genetic programming system describe optimization techniques
we propose two algorithms constructing training compact feedforward networks linear threshold units the shift procedure constructs networks single hidden layer pti constructs multilayered networks the resulting networks guaranteed perform given task binary realvalued inputs the various experimental results reported tasks binary real inputs indicate methods compare favorably alternative procedures deriving similar strategies terms size resulting networks generalization properties
the naive bayesian classifier simple effective attribute independence assumption often violated real world a number approaches developed seek alleviate problem a bayesian tree learning algorithm builds decision tree generates local bayesian classifier leaf instead predicting single class however bayesian tree learning still suffers replication fragmentation problems tree learning while inferred bayesian trees demonstrate low average prediction error rates reason believe error rates higher leaves training examples this paper proposes novel lazy bayesian tree learning algorithm for test example conceptually builds appropriate bayesian tree in practice one path local bayesian classifier leaf created experiments wide variety realworld artificial domains show new algorithm significantly lower overall prediction error rates naive bayesian classifier c bayesian tree learning algorithm
the problem learning bayesian networks hidden variables known hard problem even simpler task learning conditional probabilities bayesian network hidden variables hard in paper present approach learns conditional probabilities bayesian network hidden variables transforming multilayer feedforward neural network ann the conditional probabilities mapped onto weights ann learned using standard backpropagation techniques to avoid problem exponentially large anns focus bayesian networks noisyor noisyand nodes experiments real world classification problems demonstrate effectiveness technique
computational models natural systems often contain free parameters must set optimize predictive accuracy models this processcalled calibrationcan viewed form supervised learning presence prior knowledge in view fixed aspects model constitute prior knowledge goal learn correct values free parameters we report series attempts learn parameter values global vegetation model called mapss mapped atmosphereplantsoil system developed collaborator ron neilson unfortunately attempts apply standard machine learning methodsspecifically global error functions gradient descent searchdo work mapss constraints introduced structure model prior knowledge create difficult nonlinear optimization problem successful calibration mapss required taking divideandconquer approach subsets parameters calibrated others held constant this approach made possible carefully selecting training sets exercised portions model designing error functions part desirable properties the automated calibration tool developed currently applied calibrate mapss global climate data set
evolutionary learning methods found useful several areas development intelligent robots in approach described evolutionary algorithms used explore alternative robot behaviors within simulation model way reducing overall knowledge engineering effort this paper presents initial results applying samuel genetic learning system collision avoidance navigation task mobile robots
the bias variance real valued random variable using squared error loss well understood however recent developments classification techniques become desirable extend concepts general random variables loss functions the misclassification loss function categorical random variables particular interest we explore concepts variance bias develop decomposition prediction error functions systematic variable parts predictor after providing examples conclude discussion various definitions proposed
retrieving relevant cases crucial component casebased reasoning systems the task use userdefined query retrieve useful information ie exact matches partial matches close querydefined request according certain measures the difficulty stems fact may easy may even impossible specify query requests precisely completely resulting situation known fuzzyquerying it usually problem small domains large repositories store various information multifunctional information bases federated databases request specification becomes bottleneck thus flexible retrieval algorithm required allowing imprecise query specification changing viewpoint efficient database techniques exists locating exact matches finding relevant partial matches might problem this document proposes contextbased similarity basis flexible retrieval historical bacground research similarity assessment presented used motivation formal definition contextbased similarity we also describe similaritybased retrieval system multifunctinal information bases
in earlier paper introduced new boosting algorithm called adaboost theoretically used significantly reduce error learning algorithm consistently generates classifiers whose performance little better random guessing we also introduced related notion pseudoloss method forcing learning algorithm multilabel concepts concentrate labels hardest discriminate in paper describe experiments carried assess well adaboost without pseudoloss performs real learning problems we performed two sets experiments the first set compared boosting breimans bagging method used aggregate various classifiers including decision trees single attributevalue tests we compared performance two methods collection machinelearning benchmarks in second set experiments studied detail performance boosting using nearestneighbor classifier ocr problem
quantization parameters perceptron central problem hardware implementation neural networks using numerical technology an interesting property neural networks used classifiers ability provide robustness input noise this paper presents efficient learning algorithms maximization robustness perceptron especially designed tackle combinatorial problem arising discrete weights
machine learning techniques used extract knowledge data stored medical databases in application various machine learning algorithms used extract diagnostic knowledge support diagnosis sport injuries the applied methods include variants assistant algorithm topdown induction decision trees variants bayesian classifier the available dataset insufficent reliable diagnosis sport injuries considered system consequently expertdefined diagnostic rules added used preclassifiers generators additional training instances injuries training examples experimental results show classification accuracy explanation capability naive bayesian classifier fuzzy discretization numerical attributes superior methods estimated appro priate practical use
this paper introduces new machine learning taskmodel calibrationand presents method solving particularly difficult model calibration task arose part global climate change research project the model calibration task problem training free parameters scientific model order optimize accuracy model making future predictions it form supervised learning examples presence prior knowledge an obvious approach solving calibration problems formulate global optimization problems goal find values free parameters minimize error model training data unfortunately global optimization approach becomes computationally infeasible model highly nonlinear this paper presents new divideandconquer method analyzes model identify series smaller optimization problems whose sequential solution solves global calibration problem this paper argues methods kindrather global optimization techniqueswill required order agents large amounts prior knowledge learn efficiently
we describe principles functionalities dlab declarative language bias dlab used inductive learning systems define syntactically traverse efficiently finite subspaces first order clausal logic set propositional formulae association rules horn clauses full clauses a prolog implementation dlab available ftp access keywords declarative language bias concept learning knowledge dis covery
this paper compares representational capabilities one hidden layer two hidden layer nets consisting feedforward interconnections linear threshold units it remarked certain problems two hidden layers required contrary might principle expected known approximation theorems the differences based numerical accuracy number units needed capabilities feature extraction rather much basic classification direct inverse problems the former correspond approximation continuous functions latter concerned approximating onesided inverses continuous functions often encountered context inverse kinematics determination control questions a general result given showing nonlinear control systems stabilized using two hidden layers general using one
discovery involves collaboration among many intelligent activities however little known form collaboration occurs in paper framework proposed autonomous systems learn discover environment within framework many intelligent activities perception action exploration experimentation learning problem solving new term construction integrated coherent way the framework presented detail implemented system called live evaluated performance live several discovery tasks the conclusion autonomous learning environment feasible approach integrating activities involved discovery process
support vector machines used time series prediction compared radial basis function networks we make use two different cost functions support vectors training insensitive loss ii hubers robust loss function discuss choose regularization parameters models two applications considered data noisy normal uniform noise mackey glass equation b santa fe competition set d in cases support vector machines show excellent performance in case b support vector approach improves best known result benchmark factor
in paper evaluate classification accuracy four statistical three neural network classifiers two image based pattern classification problems these fingerprint classification optical character recognition ocr isolated handprinted digits the evaluation results reported useful designers practical systems two important commercial applications for ocr problem karhunenloeve kl transform images used generate input feature set similarly fingerprint problem kl transform ridge directions used generate input feature set the statistical classifiers used euclidean minimum distance quadratic minimum distance normal knearest neighbor the neural network classifiers used multilayer perceptron radial basis function probabilistic the ocr data consisted digit images training digit images testing the fingerprint data consisted training testing images in addition evaluation accuracy multilayer perceptron radial basis function networks evaluated size generalization capability for evaluated datasets best accuracy obtained either problem provided probabilistic neural network minimum classification error ocr fingerprints
the problem trajectory tracking presence input constraints considered the desired trajectory reparameterized slower time scale order avoid input saturation necessary conditions reparameterizing function must satisfy derived the deviation nominal trajectory minimized formulating problem optimal control problem
genetic programming applied task finding cliques graph nodes graph represented tree structures manipulated form candidate cliques the intrinsic properties clique detection complicates design good fitness evaluation we analyze properties show clique detector found better finding maximum clique graph set cliques
computer models casebased reasoning cbr generally guide case adaptation using fixed set adaptation rules a difficult practical problem identify knowledge required guide adaptation particular tasks likewise open issue cbr cognitive model case adaptation knowledge learned we describe new approach acquiring case adaptation knowledge in approach adaptation problems initially solved reasoning scratch using abstract rules structural transformations general memory search heuristics traces processing used successful rulebased adaptation stored cases enable future adaptation done casebased reasoning when similar adaptation problems encountered future adaptation cases provide task domainspecific guidance case adaptation process we present tenets approach concerning relationship memory search case adaptation memory search process storage reuse cases representing adaptation episodes these points discussed context ongoing research dial computer model learns case adaptation knowledge casebased disaster response planning
the development multistrategy learning systems based clear understanding roles applicability conditions different learning strategies to end chapter introduces inferential theory learning provides conceptual framework explaining logical capabilities learning strategies ie competence viewing learning process modifying learners knowledge exploring learners experience theory postulates process described search knowledge space triggered learners experience guided learning goals the search operators instantiations knowledge transmutations generic patterns knowledge change transmutations may employ basic type inferencededuction induction analogy several fundamental knowledge transmutations described novel general way generalization abstraction explanation similization counterparts specialization concretion prediction dissimilization respectively generalization enlarges reference set description set entities described abstraction reduces amount detail reference set explanation generates premises explain imply given properties reference set similization transfers knowledge one reference set similar reference set using concepts theory multistrategy taskadaptive learning mtl methodology outlined illustrated b example mtl dynamically adapts strategies learning task defined input information learners background knowledge learning goal it aims synergistically integrating whole range inferential learning strategies empirical generalization constructive induction deductive generalization explanation prediction abstraction similization
the support vector sv machine novel type learning machine based statistical learning theory contains polynomial classifiers neural networks radial basis function rbf networks special cases in rbf case sv algorithm automatically determines centers weights threshold minimize upper bound expected test error the present study devoted experimental comparison machines classical approach centers determined kmeans clustering weights found using error backpropagation we consider three machines namely classical rbf machine sv machine gaussian kernel hybrid system centers determined sv method weights trained error backpropagation our results show us postal service database handwritten digits sv machine achieves highest test accuracy followed hybrid approach the sv approach thus theoretically wellfounded also superior practical application this report describes research done center biological computational learning artificial intelligence laboratory massachusetts institute technology att bell laboratories att research lucent technologies bell laboratories support center provided part grant national science foundation contract asc bs thanks mit hospitality threeweek visit march work started at time study bs cb vv att bell laboratories nj ks fg pn tp massachusetts institute technology ks department information systems computer science national university singapore lower kent ridge road singapore cb pn lucent technologies bell laboratories nj vv att research nj bs supported studienstiftung des deutschen volkes cb supported arpa onr contract number nc we thank a smola useful discussions please direct correspondence bernhard scholkopf bsmpiktuebmpgde maxplanckinstitut fur biologische kybernetik spemannstr tubingen germany
ensembles classifiers eg decision trees often exhibit greater predictive accuracy single classifiers alone bagging boosting two standard ways generating combining multiple classifiers unfortunately increase predictive performance usually linked dramatic decrease intelligibility ensembles less black boxes comparable neural networks so far attempts pruning ensembles successful approximately reducing ensembles half this paper describes different approach tries keep ensemblesizes small induction already also limits complexity single classifiers rigorously single classifiers decisionstumps prespecified maximal depth they combined majority voting ensembles induced pruned simple hillclimbing procedure these ensembles reasonably transformed equivalent decision trees we conduct empirical evaluation investigate predictive accuracies classifier complexities
just input state stability iss generalizes idea finite gains respect supremum norms new notion integral input state stability iiss generalizes concept finite gain using integral norm inputs in paper obtain necessary sufficient characterization iiss property expressed terms dissipation inequalities
the use graphs represent independence structure multivariate probability models pursued relatively independent fashion across wide variety research disciplines since beginning century this paper provides brief overview current status research particular attention recent developments served unify seemingly disparate topics probabilistic expert systems statistical physics image analysis genetics decoding errorcorrecting codes kalman filters speech recognition markov models
in probabilitybased reasoning system bayes theorem variations often used revise systems beliefs however explicit conditions implicit conditions probability assignments properly distinguished follows bayes theorem generally applicable revision rule upon properly distinguishing belief revision belief updating see jeffreys rule variations revision rules either without distinctions limitation bayesian approach often ignored underestimated revision general form done bayesian approach probability distribution function alone contain information needed operation
at beginning paper three binary term logics defined the first based inheritance relation the second third suggest novel way process extension intension also interesting relations aristotles syllogistic logic based three simple systems nonaxiomatic logic defined it termoriented language experiencegrounded semantics it uniformly represents processes randomness fuzziness ignorance it also uniformly carries deduction abduction induction revision
an inference graph many derivation strategies particular ordering steps involved reducing given query sequence database retrievals an optimal strategy given distribution queries complete strategy whose expected cost minimal expected cost depends conditional probabilities requested retrieval succeeds given member class queries posed this paper describes pao algorithm first uses set training examples approximate probability values uses estimates produce probably approximately optimal strategy ie given ffi gt pao produces strategy whose cost within cost optimal strategy probability greater ffi this paper also shows obtain strategies time polynomial ffi size inference graph many important classes graphs including andor trees
nars uses new form term logic extended syllogism several types uncertainties represented processed deduction induction abduction revision carried unified format the system works asynchronously parallel way the memory system dynamically organized also interpreted network
uncertainty artificial intelligence active research field several approaches suggested studied dealing various types uncertainty however hard rank approaches general usually aimed special application environment this paper begins defining environment show existing approaches used situation then new approach nonaxiomatic reasoning system introduced work environment the system designed assumption systems knowledge resources usually insufficient handle tasks imposed environment the system consistently represent several types uncertainty carry multiple operations uncertainties finally new approach compared previous approaches terms uncertainty representation interpretation
many realworld time series multistationary underlying data generating process dgp switches different stationary subprocesses modes operation an important problem modeling systems discover underlying switching process entails identifying number subprocesses dynamics subprocess for many time series problem illdefined since often obvious means distinguish different subprocesses we discuss use nonlinear gated experts perform segmentation system identification time series unlike standard gated experts methods however use concepts statistical physics enhance segmentation highnoise problems experts required
systems learn examples often create disjunctive concept definition the disjuncts concept definition cover training examples referred small disjuncts the problem small disjuncts error prone large disjuncts may necessary achieve high level predictive accuracy holte acker porter this paper extends previous work done problem small disjuncts taking noise account it investigates assertion hard learn noisy data difficult distinguish noise true exceptions in process evaluating assertion insights gained mechanisms noise affects learning two domains investigated the experimental results paper suggest shapiros chess endgame domain shapiro wisconsin breast cancer domain wolberg assertion true least low levels class noise
a training set data used construct rule predicting future responses what error rate rule the traditional answer question given crossvalidation the crossvalidation estimate prediction error nearly unbiased highly variable this article discusses bootstrap estimates prediction error thought smoothed versions crossvalidation a particular bootstrap method rule shown substantially outperform crossvalidation catalog simulation experiments besides providing point estimates also consider estimating variability error rate estimate all results nonparametric apply possible prediction rule however study classification problems loss detail our simulations include smooth prediction rules like fishers linear discriminant function unsmooth ones like nearest neighbors
in paper discuss application memorybased learning mbl fast np chunking we first discuss application fast decision tree variant mbl igtree dataset described ramshaw marcus consists roughly test train items in second series experiments used architecture two cascaded igtrees in second level cascaded classifier added context predictions extra features incorrect predictions first level corrected yielding generalisation accuracy training testing times order seconds minutes
we examine issue consistency new perspective to avoid overfitting training data considerable number current systems sacrificed goal learning hypotheses perfectly consistent training instances setting new goal hypothesis simplicity occams razor instead using simplicity goal developed novel approach addresses consistency directly in words concept learner explicit goal selecting appropriate degree consistency training data we begin paper exploring concept learning less perfect consistency next describe system adapt degree consistency response feedback predictive accuracy test data finally present results initial experiments begin address question tightly hypotheses fit training data different problems
handling np complete problems gas great challenge in particular presence constraints makes finding solutions hard ga in paper present problem independent constraint handling mechanism stepwise adaptation weights saw apply solving sat problem our experiments prove saw mechanism substantially increases ga performance furthermore compare sawing ga best heuristic technique could trace wgsat conclude ga superior heuristic method
artificial neural network seem promising regression classification especially large covariate spaces these methods represent nonlinear function composition low dimensional ridge functions therefore appear less sensitive dimensionality covariate space however due non uniqueness global minimum existence possibly many local minima model revealed network non stable we introduce method interpret neural network results uses novel robustification techniques this results robust interpretation model employed network simulated data known models used demonstrate interpretability results demonstrate effects different regularization methods robustness model graphical methods introduced present interpretation results we demonstrate interaction covariates revealed from study conclude interpretation method works well nn models may sometimes misinterpreted especially approximations true model less robust
in paper describe different ways select transform features using evolutionary computation the features intended serve inputs feedforward network the first way selection features using standard genetic algorithm solution found specifies whether certain feature present we show prediction unemployment rates various european countries succesfull approach in fact kind selection features special case socalled functional links functional links transform input pattern space new pattern space as functional links one use polynomials general functions both found using evolutionary computation polynomial functional links found evolving coding powers polynomial for symbolic functions use genetic programming genetic programming finds symbolic functions applied inputs we compare workings latter two methods two artificial datasets realworld medical image dataset
this paper reports development realistic knowledgebased application using mobal system some problems requirements resulting industrialcaliber tasks formulated a stepbystep account construction knowledge base task demonstrates interleaved use several learning algorithms concert inference engine graphical interface fulfill requirements design analysis revision refinement extension working model combined one incremental process this illustrates balanced cooperative modeling approach the case study taken telecommunications domain precisely deals security management telecommunications networks mobal would used part security management tool acquiring validating refining security policy the modeling approach compared approaches kads standalone machine learning
source separation consists recovering set independent signals mixtures unknown coefficients observed this paper introduces class adaptive algorithms source separation implements adaptive version equivariant estimation henceforth called easi equivariant adaptive separation via independence the easi algorithms based idea serial updating specific form matrix updates systematically yields algorithms simple parallelizable structure real complex mixtures most importantly performance easi algorithm depend mixing matrix in particular convergence rates stability conditions interference rejection levels depend normalized distributions source signals close form expressions quantities given via asymptotic performance analysis this completed numerical experiments illustrating effectiveness proposed approach
ensembles decision trees often exhibit greater predictive accuracy single trees alone bagging boosting two standard ways generating combining multiple trees boosting empirically determined effective two recently proposed may produces diverse trees bagging this paper reports empirical findings strongly support hypothesis we enforce greater decision tree diversity bagging simple modification underlying decision tree learner utilizes randomlygenerated decision stumps predefined depth starting point tree induction the modified procedure yields competitive results still retaining one attractive properties bagging iterations independent additionally also investigate possible integration bagging boosting all ensemblegenerating procedures compared empirically various domains
arcing the edge leo breiman technical report statistics department university california berkeley ca abstract recent work shown adaptively reweighting training set growing classifier using new weights combining classifiers constructed date significantly decrease generalization error procedures type called arcing breiman the first successful arcing procedure introduced freund schapire called adaboost in effort explain adaboost works schapire etal derived bound generalization error convex combination classifiers terms margin we introduce function called edge differs margin two classes a framework understanding arcing algorithms defined in framework see arcing algorithms currently literature optimization algorithms minimize function edge a relation derived optimal reduction maximum value edge pac concept weak learner two algorithms described achieve optimal reduction tests synthetic real data cast doubt schapire etal there recent empirical evidence significant reductions generalization error gotten growing number different classifiers training set letting vote best class freund schapire proposed algorithm called adaboost adaptively reweights training set way based past history misclassifications constructs new classifier using current weights uses misclassification rate classifier determine size vote in number empirical studies many data sets using trees cart c base classifier drucker cortes quinlan freud schapire breiman adaboost produced dramatic decreases generalization error compared using single tree error rates reduced point tests wellknown data sets gave result cart plus adaboost significantly better commonly used classification methods breiman meanwhile empirical results showed methods adaptive resampling reweighting combining called arcing breiman also led low test set error rates an algorithm called arcx breiman gave error rates almost identical adaboost ji ma worked classifiers consisting randomly selected hyperplanes using different method adaptive resampling unweighted voting also got low error rates thus least three arcing algorithms extant give excellent classification accuracy explanation
in order sequence tasks job shop problem jsp number machines related technological machine order jobs new representation technique mathematically known permutation repetition presented the main advantage single chromosome representation analogy permutation scheme traveling salesman problem tsp produce illegal sets operation sequences infeasible symbolic solutions as consequence representation scheme new crossover operator preserving initial scheme structure permutations repetition sketched its behavior similar well known ordercrossover simple permutation schemes actually gox operator permutations repetition arises generalisation ox computational experiments show gox passes information couple parent solutions efficiently offspring solutions together new representation gox support cooperative aspect genetic search scheduling problems strongly
recently bell sejnowski presented approach blind source separation based information maximization principle we extend approach general cases sources may delayed respect we present network architecture capable coping sources derive adaptation equations delays weights network maximizing information transferred network examples using wideband sources speech presented illustrate algorithm
the reference class problem probability theory multiple inheritances extensions problem nonmonotonic logics referred special cases conflicting beliefs the current solution accepted two domains specificity priority principle by analyzing example several factors ignored principle found relevant priority reference class a new approach nonaxiomatic reasoning system nars discussed factors taken account it argued solution provided nars better solutions provided probability theory nonmonotonic logics
this paper discusses application modern signal processing technique known independent component analysis ica blind source separation multivariate financial time series portfolio stocks the key idea ica linearly map observed multivariate time series new space statistically independent components ics this viewed factorization portfolio since joint probabilities become simple products coordinate system ics we apply ica three years daily returns largest japanese stocks compare results obtained using principal component analysis the results indicate estimated ics fall two categories infrequent large shocks responsible major changes stock prices ii frequent smaller fluctuations contributing little overall level stocks we show overall stock price reconstructed surprisingly well using small number thresholded weighted ics in contrast using shocks derived principal components instead independent components reconstructed price less similar original one independent component analysis potentially powerful method analyzing understanding driving mechanisms financial markets there promising applications risk management since ica focuses higher order statistics
this paper concerns empirical basis causation addresses following issues we propose minimalmodel semantics causation show contrary common folklore genuine causal influences distinguished spurious covariations following standard norms inductive reasoning we also establish sound characterization conditions distinction possible we provide effective algorithm inferred causation show large class data algorithm uncover direction causal influences defined finally ad dress issue nontemporal causation
this paper presents method using qualitative models guide inductive learning our objectives induce rules accurate also explainable respect qualitative model reduce learning time exploiting domain knowledge learning process such explainability essential practical application inductive technology integrating results learning back existing knowledgebase we apply method two process control problems water tank network ore grinding process used mining industry surprisingly addition achieving explainability classificational accuracy induced rules also increased we show value qualitative models quantified terms equivalence additional training examples finally discuss possible extensions
explanation based learning typically considered symbolic learning method an explanation based learning method utilizes purely neural network representations called ebnn recently developed shown several desirable properties including robustness errors domain theory this paper briefly summarizes ebnn algorithm explores correspondence neural network based ebl method ebl methods based symbolic representations
the multiparent scanning crossover generalizing traditional uniform crossover diagonal crossover generalizing point npoint crossovers introduced in subsequent publications see several aspects multiparent recombination discussed due space limitations however full overview experimental results showing performance multiparent gas numerical optimization problems never published this technical report meant fill gap make results available
this report documents nacodae navy conversational decision aids environment developed navy center applied research artificial intelligence ncarai branch naval research laboratory nacodae software prototype developed practical advances casebased reasoning project funded office naval research purpose assisting navy dod personnel decision aids tasks system maintenance operational training crisis response planning logistics fault diagnosis target classification meteorological nowcasting implemented java nacodae used machine containing java virtual machine eg pcs unix this document describes exemplifies nacodaes capabilities our goal transition tool operational personnel continue enhancement user feedback testing recent research advances casebased reasoning related areas
a new generation sensor rich massively distributed autonomous systems developed potential unprecedented performance smart buildings reconfigurable factories adaptive traffic systems remote earth ecosystem monitoring to achieve high performance massive systems need accurately model environment sensor information accomplishing grand scale requires automating art largescale modeling this paper presents formalization decompositional modelbased learning dml method developed observing modelers expertise decomposing large scale model estimation tasks the method exploits striking analogy learning consistencybased diagnosis moriarty implementation dml applied thermal modeling smart building demonstrating significant improvement learning rate
traditional machine vision assumes vision system recovers complete labeled description world marr recently several researchers criticized model proposed alternative model considers perception distributed collection taskspecific taskdriven visual routines aloimonos ullman some researchers argued natural living systems visual routines product natural selection ramachandran so far researchers handcoded taskspecific visual routines actual implementations eg chapman in paper propose alternative approach visual routines simple tasks evolved using artificial evolution approach we present results series runs actual camera images simple routines evolved using genetic programming techniques koza the results obtained promising evolved routines able correctly classify images better best algorithm able write hand
combinatorial explosion inferences always central problem artificial intelligence although inferences drawn reasoners knowledge available inputs large potentially infinite inferential resources available reasoning system limited with limited inferential capacity many potential inferences reasoners must somehow control process inference not inferences equally useful given reasoning system any reasoning system goals form utility function acts based beliefs indirectly assigns utility beliefs given limits process inference variation utility inferences clear reasoner ought draw inferences valuable this paper presents approach problem makes utility potential belief explicit part inference process the method generate explicit desires knowledge the question focus attention thereby transformed two related problems how explicit desires knowledge used control inference facilitate resourceconstrained goal pursuit general where desires knowledge come we present theory knowledge goals desires knowledge use processes understanding learning the theory illustrated using two case studies natural language understanding program learns reading novel unusual newspaper stories differential diagnosis program improves accuracy experience
this paper presents theory motivational analysis construction volitional explanations describe planning behavior agents we discuss content explanations well process understander builds explanations explanations constructed decision models describe planning process agent goes considering whether perform action decision models represented explanation patterns standard patterns causality based previous experiences understander we discuss nature explanation patterns use representing decision models process retrieved used evaluated
an evolutionary approach developing improved neural network architectures presented it shown possible use genetic algorithms construction backpropagation networks real world tasks therefore network representation developed certain properties results various application presented
this paper describes reasoner improve understanding incompletely understood domain application already knows novel problems domain recent work ai dealt issue using past explanations stored reasoners memory understand novel situations however process assumes past explanations well understood provide good lessons used future situations this assumption usually false one learning novel domain since situations encountered previously domain might understood completely instead reasonable assume reasoner would gaps knowledge base by reasoning new situation reasoner able fill gaps new information came reorganize explanations memory gradually evolve better understanding domain we present story understanding program retrieves past explanations situations already memory uses build explanations understand novel stories terrorism in system refines understanding domain filling gaps explanations elaborating explanations learning new indices explanations this type incremental learning since system improves explanatory knowledge domain incremental fashion rather learning new xps whole
we present method analysis nonstationary time series multiple operating modes in particular possible detect model switching dynamics less abrupt time consuming drift one mode another this achieved two steps first unsupervised training method provides prediction experts inherent dynamical modes then trained experts used hidden markov model allows model drifts an application physiological wakesleep data demonstrates analysis modeling realworld time series improved drift paradigm taken account
addressed kbann translates theory neuralnet refines using backpropagation retranslates result back rules adding extra hidden units connections initial network however would require predetermining num in paper presented constructive induction techniques recently added either theory refinement system intermediate concept utilization employs existing rules theory derive higherlevel features use induction intermediate concept creation employs inverse resolution introduce new intermediate concepts order fill gaps theory span multiple levels these revisions allow either make use imperfect domain theories ways typical previous work constructive induction theory refinement as result either able handle wider range theory imperfections existing theory refinement system
a number reinforcement learning algorithms developed guaranteed converge optimal solution used lookup tables it shown however algorithms easily become unstable implemented directly general functionapproximation system sigmoidal multilayer perceptron radialbasisfunction system memorybased learning system even linear functionapproximation system a new class algorithms residual gradient algorithms proposed perform gradient descent mean squared bellman residual guaranteeing convergence it shown however may learn slowly cases a larger class algorithms residual algorithms proposed guaranteed convergence residual gradient algorithms yet retain fast learning speed direct algorithms in fact direct residual gradient algorithms shown special cases residual algorithms shown residual algorithms combine advantages approach the direct residual gradient residual forms value iteration qlearning advantage learning presented theoretical analysis given explaining properties algorithms simulation results given demonstrate properties
this note describes useful adaptation peak seeking regime used unsupervised learning processes competitive learning kmeans the adaptation enables learning capture loworder probability effects thus fully capture probabilistic structure training data
experiment design execution central activity natural sciences the seqer system provides general architecture integration automated planning techniques variety domain knowledge order plan scientific experiments these planning techniques include rulebased methods especially use derivational analogy derivational analogy allows planning experience captured cases reused analogy also allows system function absence strong domain knowledge cases efficiently flexibly retrieved large casebase using massively parallel methods
finding good monitoring strategies important process design embedded agent we describe nature monitoring problem point makes difficult show periodic monitoring strategies often easiest derive always appropriate we demonstrate mathematically empirically wide class problems socalled cupcake problems exists simple strategy interval reduction outperforms periodic monitoring we also show features environment may influence choice optimal strategy the paper concludes thoughts monitoring strategy taxonomy defining features might
this paper discusses issues related bayesian network model learning unbalanced binary classification tasks in general primary focus current research bayesian network learning systems eg k variants creation bayesian network structure fits database best it turns applied specific purpose mind classification performance network models may poor we demonstrate bayesian network models created meet specific goal purpose intended model we first present goaloriented algorithm constructing bayesian networks predicting uncollectibles telecommunications riskmanagement datasets second argue demonstrate current bayesian network learning methods may fail perform satisfactorily real life applications since learn models tailored specific goal purpose third discuss performance goal oriented k variant
we calculated analytical expressions bias variance estimators provided various temporal difference value estimation algorithms change oine updates trials absorbing markov chains using lookup table representations we illustrate classes learning curve behavior various chains show manner td sensitive choice step size eligibility trace parameters
the problem minimizing number misclassified points plane attempting separate two point sets intersecting convex hulls ndimensional real space formulated linear program equilibrium constraints lpec this general lpec converted exact penalty problem quadratic objective linear constraints a frankwolfetype algorithm proposed penalty problem terminates stationary point global solution novel aspects approach include a linear complementarity formulation step function counts misclassifications ii exact penalty formulation without boundedness nondegeneracy constraint qualification assumptions iii an exact solution extraction sequence minimizers penalty function finite value penalty parameter general lpec explicitly exact solution lpec uncoupled constraints iv a parametric quadratic programming formulation lpec associated misclassification minimization problem
in paper introduce new approach problem optimal compression source code produces multiple codewords given symbol it may seem sensible codeword use case shortest one however proposed free energy approach random codeword selection yields effective codeword length less shortest codeword length if random choices boltzmann distributed effective length optimal given source code the expectationmaximization parameter estimation algorithms minimize effective codeword length we illustrate performance free energy coding simple problem compression factor two gained using new method
as bayesian networks influence diagrams used widely importance efficient explanation mechanism becomes apparent we focus predictive explanations ones designed explain predictions recommendations probabilistic systems we analyze issues involved defining computing evaluating explanations present algorithm compute
tech report department computer science monash university clayton vic australia abstract this paper continues introduction minimum encoding inductive inference given oliver hand this series papers written objective providing introduction area statisticians we describe message length estimates used wallaces minimum message length mml inference rissanens minimum description length mdl inference the differences message length estimates two approaches explained the implications differences applications discussed
we determine study sustained performance cns training evaluation large multilayered feedforward neural networks using sophisticated coding node machine would achieve giga connections per second gcps giga connection updates per second gcups during recall machine would archieve peak multiplyaccumulate performance the training large nets less efficient recall factor the benchmark parallelized machine code optimized analyzing performance starting optimal parallel algorithm cns specific optimizations still reduce run time factor recall factor training our analysis also yields strategies code optimization the cns still design therefore model run time behavior memory system interconnection network this gives us option changing parameters cns system order analyze performance impact
in paper present interactive casebased approach crisis response provides users ability rapidly develop good responses allowing retain ultimate control decisionmaking process we implemented approach inca interactive crisis assistant planning scheduling crisis domains inca relies casebased methods seed response development process initial candidate solutions drawn previous cases the human user interacts inca adapt solutions current situation we discuss interactive approach crisis response using artificial hazardous materials domain hazmat developed purpose evaluating candidate assistant mechanisms crisis response
mixedinitiative systems present challenge finding effective level interaction humans computers machine learning presents promising approach problem form systems automatically adapt behavior accommodate different users in paper present empirical study learning user models adaptive assistant crisis scheduling we describe problem domain scheduling assistant present initial formulation adaptive assistants learning task results baseline study after report results three subsequent experiments investigate effects problem reformulation representation augmentation the results suggest problem reformulation leads significantly better accuracy without sacrificing usefulness learned behavior the studies also raise several interesting issues adaptive assistance scheduling
we consider bayesian informationtheoretic approaches determining noninformative prior distributions parametric model family the informationtheoretic approaches based recently modified definition stochastic complexity rissanen minimum message length mml approach wallace the bayesian alternatives include uniform prior equivalent sample size priors in order able empirically compare different approaches practice methods instantiated model family practical importance family bayesian networks
intelligent information retrieval iir requires inference the number inferences drawn even simple reasoner large inferential resources available practical computer system limited this problem one long faced ai researchers in paper present method used two recent machine learning programs control inference relevant design iir systems the key feature approach use explicit representations desired knowledge call knowledge goals our theory addresses representation knowledge goals methods generating transforming goals heuristics selecting among potential inferences order feasibly satisfy goals in view iir becomes kind planning decisions infer infer infer based representations desired knowledge well internal representations systems inferential abilities current state the theory illustrated using two case studies natural language understanding program learns reading novel newspaper stories differential diagnosis program improves accuracy experience we conclude making several suggestions machine learning framework integrated existing information retrieval methods
this paper attempts bridge fields machine learning robotics distributed ai it discusses use communication reducing undesirable effects locality fully distributed multiagent systems multiple agentsrobots learning parallel interacting two key problems hidden state credit assignment addressed applying local undirected broadcast communication dual role sensing reinforcement the methodology demonstrated two multirobot learning experiments the first describes learning tightlycoupled coordination task two robots second looselycoupled task four robots learning social rules communication used share sensory data overcome hidden state share reinforcement overcome credit assignment problem agents bridge gap localindividual globalgroup payoff
this paper investigates power genetic algorithms solving maxclique problem we measure performance standard genetic algorithm elementary set problem instances consisting embedded cliques random graphs we indicate need improvement introduce new genetic algorithm multiphase annealed ga exhibits superior performance problem set as scale problem size test hard benchmark instances notice degraded performance algorithm caused premature convergence local minima to alleviate problem sequence modifications implemented ranging changes input representation systematic local search the recent version called union ga incorporates features union crossover greedy replacement diversity enhancement it shows marked speedup number iterations required find given solution well improvement clique size found we discuss issues related simd implementation genetic algorithms thinking machines cm necessitated intrinsically high time complexity on serial algorithm computing one iteration our preliminary conclusions genetic algorithm needs heavily customized work well clique problem ga computationally expensive use recommended known find larger cliques algorithms although customization effort bringing forth continued improvements clear evidence time ga better success circumventing local minima
for many types learners one compute statistically optimal way select data we review techniques used feedforward neural networks mackay cohn we show principles may used select data two alternative statisticallybased learning architectures mixtures gaussians locally weighted regression while techniques neural networks expensive approximate techniques mixtures gaussians locally weighted regression efficient accurate this report describes research done center biological computational learning artificial intelligence laboratory massachusetts institute technology support center provided part grant national science foundation contract asc the authors also funded mcdonnellpew foundation atr human information processing laboratories siemens corporate research nsf grant cda grant n office naval research michael i jordan nsf presidential young investigator a version paper appears g tesauro d touretzky j alspector eds advances neural information processing systems morgan kaufmann san francisco ca
we consider standard problem learning concept random examples here learning curve defined expected error learners hypotheses function training sample size haussler littlestone warmuth shown distribution free setting smallest expected error learner achieve worst case concept class c converges rationally zero error ie fit training sample size however recently cohn tesauro demonstrated exponential convergence often observed experimental settings ie average error decreasing e fit by addressing simple nonuniformity original analysis paper shows dichotomy rational exponential worst case learning curves recovered distribution free theory these results support experimental findings cohn tesauro finite concept classes consistent learner achieves exponential convergence even worst case continuous concept classes learner exhibit subrational convergence every target concept domain distribution a precise boundary rational exponential convergence drawn simple concept chains here show somewhere dense chains always force rational convergence worst case exponential convergence always achieved nowhere dense chains
concepts learned neural networks difficult understand represented using large assemblages realvalued parameters one approach understanding trained neural networks extract symbolic rules describe classification behavior there several existing ruleextraction approaches operate searching rules we present novel method casts rule extraction search problem instead learning problem in addition learning training examples method exploits property networks efficiently queried we describe algorithms extracting conjunctive m ofn rules present experiments show method efficient conventional searchbased approaches
this paper presents fast algorithm provides optimal near optimal solutions minimum perimeter problem rectangular grid the minimum perimeter problem partition grid size m n p equal area regions minimizing total perimeter regions the approach taken divide grid stripes filled completely integer number regions this striping method gives rise knapsack integer program efficiently solved existing codes the solution knapsack problem used generate grid region assignments an implementation algorithm partitioned grid regions provably optimal solution less one second with sufficient memory hold m n grid array extremely large minimum perimeter problems solved easily
this paper presents evaluates two algorithms incrementally constructing radial basis function networks class neural networks looks suitable adtaptive control applications popular backpropagation networks the first algorithm derived previous method developed fritzke second one inspired cart algorithm developed breiman generation regression trees both algorithms proved work well number tests exhibit comparable performances an evaluation standard case study mackeyglass temporal series reported
number prototypes used represent class position prototype within class membership function associated prototype this paper proposes novel evolutionary approach data clustering classification overcomes many limitations traditional systems the approach rests optimisation number positions fuzzy prototypes using realvalued genetic algorithm ga because ga acts classes system benefits naturally global information possible class interactions in addition concept receptive field prototype used replace classical distancebased membership function infinite fuzzy support multidimensional gaussian function centred prototype unique variance dimension reflecting tightness cluster hence notion nearestneighbour replaced nearest attracting prototype nap the proposed model completely selfoptimising fuzzy system called ganap most data clustering algorithms including popular kmeans algorithm require priori knowledge problem domain fix number starting positions prototypes although knowledge may assumed domains whose dimensionality fairly small whose underlying structure relatively intuitive clearly much less accessible hyperdimensional settings number input parameters may large classical systems also suffer fact define clusters one class time hence account made potential interactions among classes these drawbacks compounded fact ensuing classification typically based fixed distancebased membership function prototypes this paper proposes novel approach data clustering classification overcomes aforementioned limitations traditional systems the model based genetic evolution fuzzy prototypes a realvalued genetic algorithm ga used optimise number positions prototypes because ga acts classes measures fitness classification accuracy system naturally profits global information class interaction the concept receptive field prototype also presented used replace classical fixed distancebased function infinite fuzzy support membership function the new membership function inspired used hidden layer rbf networks it consists multidimensional gaussian function centred prototype unique variance dimension reflects tightness cluster during classification notion nearestneighbour replaced nearest attracting prototype nap the proposed model completely selfoptimising fuzzy system called ganap
in paper study performance gradient descent applied problem online linear prediction arbitrary inner product spaces we prove worstcase bounds sum squared prediction errors various assumptions concerning amount priori information sequence predict the algorithms use variants extensions online gradient descent whereas algorithms always predict using linear functions hypotheses none results requires data linearly related in fact bounds proved total prediction loss typically expressed function total loss best fixed linear predictor bounded norm all upper bounds tight within constants matching lower bounds provided cases finally apply results problem online prediction classes smooth functions
we study online learning classes functions single real variable formed bounds various norms functions derivatives we determine best bounds obtainable worstcase sum squared errors also absolute errors several classes we prove upper bounds classes smooth functions loss functions prove upper lower bounds terms number trials
nearestneighbor algorithms known depend heavily distance metric in paper investigate use weighted euclidean metric weight feature comes small set options we describe diet algorithm directs search space discrete weights using crossvalidation error evaluation function although large set possible weights reduce learners bias also lead increased variance overfitting our empirical study shows many data sets advantage weighting features increasing number possible weights beyond two zero one little benefit sometimes degrades performance
in context machine learning examples paper deals problem estimating quality attributes without dependencies among kira rendell ab developed algorithm called relief shown efficient estimating attributes original relief deal discrete continuous attributes limited twoclass problems in paper relief analysed extended deal noisy incomplete multiclass data sets the extensions verified various artificial one well known realworld problem
in paper present averagecase analysis nearest neighbor algorithm simple induction method studied many researchers our analysis assumes conjunctive target concept noisefree boolean attributes uniform distribution instance space we calculate probability algorithm encounter test instance distance prototype concept along probability nearest stored training case distance e test instance from compute probability correct classification function number observed training cases number relevant attributes number irrelevant attributes we also explore behavioral implications analysis presenting predicted learning curves artificial domains give experimental results domains check reasoning
in order better understand life helpful look beyond envelop life know a simple model coevolution implemented addition genes longevity mutation rate individuals this made possible lineage evolve immortal it also allowed evolution mutation extremely high mutation rates the model shows individuals interact sort zerosum game lineages maintain relatively high mutation rates however individuals engage interactions greater consequences one individual interaction lineages tend evolve relatively low mutation rates this model suggests different genes may evolved different mutation rates adaptations varying pressures interactions genes
difficult we face problem using architecture based learning classifier systems description learning technique used organizational structure proposed present experiments show behaviour acquisition achieved our simulated robot learns structural properties animal behavioural organization proposed ethologists after
in paper present probabilistic formalization instancebased learning approach in bayesian framework moving construction explicit hypothesis datadriven instancebased learning approach equivalent averaging possibly infinitely many individual models the general bayesian instancebased learning framework described paper applied set assumptions defining parametric model family discrete prediction task number simultaneously predicted attributes small includes example classification tasks prevalent machine learning literature to illustrate use suggested general framework practice show approach implemented special case strong independence assumptions underlying called naive bayes classifier the resulting bayesian instancebased classifier validated empirically public domain data sets results compared performance traditional naive bayes classifier the results suggest bayesian instancebased learning approach yields better results traditional naive bayes classifier especially cases amount training data small
we present comparative study genetic algorithms search properties treated combinatorial optimization technique this done context nphard problem maxsat comparison relative metropolis process extension simulated annealing our contribution twofold first show large difficult maxsat instances contribution crossover search process marginal little lost dispensed altogether running mutation selection enlarged metropolis process second show problem instances genetic search consistently performs worse simulated annealing subject similar resource bounds the correspondence two algorithms made precise via decomposition argument provides framework interpreting results
in constructive induction ci learners problem representation modified normal part learning process this may necessary initial representation inadequate inappropriate however distinction constructive nonconstructive methods appears highly ambiguous several conventional definitions process constructive induction appear include conceivable learning processes in paper i argue process constructive learning identified relational learning ie i suggest
probabilistic models recently utilized optimization large combinatorial search problems however complex probabilistic models attempt capture interparameter dependencies prohibitive computational costs the algorithm presented paper termed comit provides method using probabilistic models conjunction fast search techniques we show comit used two different fast search algorithms hillclimbing populationbased incremental learning pbil the resulting algorithms maintain many benefits probabilistic modeling far less computational expense extensive empirical results provided comit successfully applied jobshop scheduling traveling salesman knapsack problems this paper also presents review probabilistic modeling combi natorial optimization
current systems field inductive logic programming ilp use primarily sake efficiency heuristically guided search techniques such greedy algorithms suffer local optimization problem present paper describes system named sfoil tries alleviate problem using stochastic search method based generalization simulated annealing called markovian neural network various tests performed benchmark realworld domains the results show advantages weaknesses stochastic approach
in many optimization problems structure solutions reflects complex relationships different input parameters for example experience may tell us certain parameters closely related explored independently similarly experience may establish subset parameters must take particular values any search cost landscape take advantage relationships we present mimic framework analyze global structure optimization landscape a novel efficient algorithm estimation structure derived we use knowledge structure guide randomized search solution space turn refine estimate structure our technique obtains significant speed gains randomized optimization procedures
we analyze generalization behavior xcs classifier system environments generalizations done experimental results presented paper evidence generalization mechanism xcs prevent learning even simple tasks environments we present new operator named specify contributes solution problem xcs specify operator named xcss compared xcs terms performance generalization capabilities different types environments experimental results show xcss deal greater variety environments robust xcs respect population size
in paper present computationally efficient method inducing selective bayesian network classifiers our approach use informationtheoretic metrics efficiently select subset attributes learn classifier we explore three conditional informationtheoretic metrics extensions metrics used extensively decision tree learning namely quinlans gain gain ratio metrics mantarass distance metric we experimentally show algorithms based gain ratio distance metric learn selective bayesian networks predictive accuracies good better learned existing selective bayesian network induction approaches kas significantly lower computational cost we prove subsetselection phase informationbased algorithms polynomial complexity compared worstcase exponential time complexity corresponding phase kas
the effectiveness casebased reasoning system known depend critically similarity measure however clear whether elusive esoteric similarity measures might improve performance casebased reasoner substituted commonly used measures this paper therefore deals problem choosing best similarity measure limited context instancebased learning classifications discrete example space we consider fixed similarity measures learnt ones in former case give definition similarity measure believe optimal wrt current prior distribution target concepts prove optimality within restricted class similarity measures we show optimal similarity measure instantiated specific prior distributions conclude simple similarity measure good cases in section show definition leads naturally conjecture
multiarmed bandits may viewed decompositionallystructured markov decision processes mdps potentially large state sets a particularly elegant methodology computing optimal policies developed twenty ago gittins gittins jones gittins approach reduces problem finding optimal policies original mdp sequence lowdimensional stopping problems whose solutions determine optimal policy socalled gittins indices katehakis veinott katehakis veinott shown gittins index task state may interpreted particular component maximumvalue function associated restartini process simple mdp standard solution methods computing optimal policies successive approximation apply this paper explores problem learning gittins indices online without aid process model suggests utilizing taskstatespecific qlearning agents solve respective restartinstatei subproblems includes example online reinforcement learning approach applied simple problem stochastic schedulingone instance drawn wide class problems may formulated bandit problems
we analyze performance topdown algorithms decision tree learning employed widely used c cart software packages our main result proof algorithms boosting algorithms by mean functions label internal nodes decision tree weakly approximate unknown target function topdown algorithms study amplify weak advantage build tree achieving desired level accuracy the bounds obtain amplification show interesting dependence splitting criterion used topdown algorithm more precisely functions used label internal nodes error fl approximations target function splitting criteria used cart c trees size ofl ologfl respectively suffice drive error thus example small constant advantage random guessing amplified larger constant advantage trees constant size for new splitting criterion suggested analysis much stronger fl a preliminary version paper appears proceedings twentyeighth annual acm symposium theory computing pages acm press authors addresses m kearns att research mountain avenue room a murray hill new jersey electronic mail mkearnsresearchattcom y mansour department computer science tel aviv university tel aviv israel electronic mail mansourmathtauacil y mansour supported part israel science foundation administered israel academy science humanities grant israeli ministry science technology
the paper describes counter example hypothesis states greedy decision tree generation algorithm constructs binary decision trees branches single attributevalue pair rather values selected attribute always lead tree fewer leaves given training set we show also relieff less myopic impurity functions enables induction algorithm generates binary decision trees reconstruct optimal smallest decision trees cases
realworld problems often difficult solved single monolithic system there many examples natural artificial systems show modular approach reduce total complexity system solving difficult problem satisfactorily the success modular artificial neural networks speech image processing typical example however designing modular system difficult task it relies heavily human experts prior knowledge problem there systematic automatic way form modular system problem this paper proposes novel evolutionary learning approach designing modular system automatically without human intervention our starting point speciation using technique based fitness sharing while speciation genetic algorithms new effort made towards using speciated population complete modular system we harness specialized expertise species entire population rather single individual introducing gating algorithm we demonstrate approach automatic modularization improving coevolutionary game learning following earlier researchers learn play iterated prisoners dilemma we review problems earlier coevolutionary learning explain poor generalization ability sudden mass extinctions the generalization ability approach significantly better past efforts using specialized expertise entire speciated population though gating algorithm instead best individual main contributor improvement
in paper describe approach representing using improving sensory skills physical domains we present icarus architecture represents control knowledge terms durative states sequences states the system operates cycles activating state matches environmental situation letting state control behavior conditions fail finding another matching state higher priority information probability conditions remain satisfied minimizes demands sensing knowledge durations states likely successors three statistical learning methods let system gradually reduce sensory load gains experience domain we report experimental evaluations ability three simulated physical tasks flying aircraft steering truck balancing pole our experiments include lesion studies identify reduction sensing due learning mechanisms others examine effect domain characteristics
we follow axelrod using genetic algorithm play iterated prisoners dilemma each member population ie strategy evaluated performs members current population this creates dynamic environment algorithm optimising moving target instead usual evaluation fixed set strategies causing arms race innovation we conduct two sets experiments the first set investigates conditions evolve best strategies the second set studies robustness strategies thus evolved strategies useful round robin population effective wide variety opponents our results indicate population nearly always converged generations time bias population almost always stabilised our results confirm cooperation almost always becomes dominant strategy we also confirm seeding population expert strategies best done small amounts leave initial population plenty genetic diversity the lack robustness strategies produced round robin evaluation demonstrated examples population nave cooperators exploited defectfirst strategy this causes sudden ephemeral decline populations average score recovers less nave cooperators emerge well exploiting strategies this example runaway evolution brought back reality suitable mutation reminiscent punctuated equilibria we find way reduce navity make ga population play extra
unsupervised learning algorithms based convex conic encoders proposed the encoders find closest convex conic combination basis vectors input the learning algorithms produce basis vectors minimize reconstruction error encoders the convex algorithm develops locally linear models input conic algorithm discovers features both algorithms used model handwritten digits compared vector quantization principal component analysis the neural network implementations involve feedback connections project reconstruction back input layer
although recurrent neural nets moderately successful learning emulate finitestate machines fsms continuous internal state dynamics neural net well matched discrete behavior fsm we describe architecture called dolce allows discrete states evolve net learning progresses dolce consists standard recurrent neural net trained gradient descent adaptive clustering technique quantizes state space dolce based assumption finite set discrete internal states required task actual network state belongs set corrupted noise due inaccuracy weights dolce learns recover discrete state maximum posteriori probability noisy state simulations show dolce leads significant improvement generalization performance earlier neural net approaches fsm induction
we propose statistical mechanical framework modeling discrete time series maximum likelihood estimation done via boltzmann learning onedimensional networks tied weights we call networks boltzmann chains show contain hidden markov models hmms special case our framework also motivates new architectures address particular shortcomings hmms we look two architectures parallel chains model feature sets disparate time scales looped networks model longterm dependencies hidden states for networks show implement boltzmann learning rule exactly polynomial time without resort simulated meanfield annealing the necessary computations done exact decimation procedures statistical mechanics
vector quantization lossy coding technique encoding set vectors different sources image speech the design vector quantizers yields lowest distortion one challenging problems field source coding however problem known difficult the conventional solution technique works process iterative refinements yield locally optimal results in paper design evaluate three versions genetic algorithms computing vector quantizers our preliminary study gaussianmarkov sources showed genetic approach outperforms conventional technique cases
we discuss approach constructing composite features induction decision trees the composite features correspond mofn concepts there three goals research first explore family greedy methods building mofn concepts one gs described paper second show concepts formed internal nodes decision trees serving bias learner finally evaluate method several artificially generated naturally occurring data sets determine effects bias
we present new approach called first order regression for handling numerical information inductive logic programming ilp for combination ilp numerical regression firstorder logic descriptions induced carve subspaces amenable numerical regression among realvalued variables the program fors implementation idea numerical regression focused distinguished continuous argument target predicate we show viewed generalisation usual ilp problem applications fors several realworld data sets described prediction mutagenicity chemicals modelling liquid dynamics surge tank predicting roughness steel grinding finite element mesh design operators skill reconstruction electric discharge machining a comparison fors performance previous results domains indicates fors effective tool ilp applications involve numerical data
tech report gitcogsci abstract this paper identifies goal handling processes begin account kind processes involved invention we identify new kinds goals special properties mechanisms processing goals well means integrating opportunism deliberation social interaction goalplan processes we focus invention goals address significant enterprises associated inventor invention goals represent seed goals expert around whole knowledge expert gets reorganized grows less opportunistically invention goals reflect idiosyncrasy thematic goals among experts they constantly increase sensitivity individuals particular events might contribute satisfaction our exploration based welldocumented example invention telephone alexander graham bell we propose mechanisms explain bells early thematic goals gave rise new goals invent multiple telegraph telephone new goals interacted opportunistically finally describe computational model alec accounts role goals invention
in order better understand life helpful look beyond envelop life know a simple model coevolution implemented addition gene mutation rate individual this allowed mutation rate evolve lineage the model shows individuals interact sort zerosum game lineages maintain relatively high mutation rates however individuals engage interactions greater consequences one individual interaction lineages tend evolve relatively low mutation rates this model suggests different genes may evolved different mutation rates adaptations varying pressures interactions genes
in many learning tasks dataquery neither free constant cost often cost query depends distance current location state space desired query point much gained instances keeping track length shortest path state every first action take paths with information learning agent efficiently explore environment calculating every step action move towards region greatest estimated exploration benefit balancing exploration potential reachable states encountered far currently estimated distances
we examine representational capabilities firstorder secondorder single layer recurrent neural networks slrnns hardlimiting neurons we show secondorder slrnn strictly powerful firstorder slrnn however firstorder slrnn augmented output layers feedforward neurons implement finitestate recognizer statesplitting employed when state split divided two equivalent states the judicious use statesplitting allows efficient implementation finitestate recognizers using augmented firstorder slrnns
learning past tense english verbs seemingly minor aspect language acquisition generated heated debates since become landmark task testing adequacy cognitive modeling several artificial neural networks anns implemented challenge better symbolic models posed in paper present generalpurpose symbolic pattern associator spa based upon decisiontree learning algorithm id we conduct extensive headtohead comparisons generalization ability ann models spa different representations we conclude spa generalizes past tense unseen verbs better ann models wide margin offer insights case we also discuss new default strategy decisiontree learning algorithms
as probabilistic systems gain popularity coming wider use need mechanism explains systems findings recommendations becomes critical the system also need mechanism ordering competing explanations we examine two representative approaches explanation literature one due g ardenfors one due pearland show suffer significant problems we propose approach defining notion better explanation combines features together recent work pearl others causality
a cooperative coevolutionary approach learning complex structures presented although preliminary nature appears number advantages noncoevolutionary approaches the cooperative coevolutionary approach encourages parallel evolution substructures interact useful ways form complex higher level structures the architecture designed general enough permit inclusion appropriate priori knowledge form initial biases towards particular kinds decompositions a brief summary initial results obtained testing architecture several problem domains presented shows significant speedup traditional noncoevolutionary approaches
an intelligent system must able adapt learn correct update model environment incrementally deliberately in complex environments many parameters interactions cost sampling possible range states test results action executions practical approach we present practical approach based continuous selective interaction environment pinpoints type fault domain knowledge causes unexpected behavior environment resorts experimentation additional information needed correct systems knowledge
determining architecture neural network important issue learning task for recurrent neural networks general methods exist permit estimation number layers hidden neurons size layers number weights we present simple pruning heuristic significantly improves generalization performance trained recurrent networks we illustrate heuristic training fully recurrent neural network positive negative strings regular grammar we also show rules extracted networks trained recognize strings rules extracted pruning consistent rules learned this performance improvement obtained pruning retraining networks simulations shown training pruning recurrent neural net strings generated two regular grammars randomlygenerated state grammar state triple parity grammar further simulations indicate pruning method gives generalization performance superior obtained training weight decay
we investigate structure model selection problems via biasvariance decomposition in particular characterize essential structure model selection task bias variance profiles generates sequence hypothesis classes this leads new understanding complexitypenalization methods first penalty terms effect postulate particular profile variances function model complexity postulated true profiles match systematic underfitting overfitting results depending whether penalty terms large small second usually best penalize according true variances task therefore fixed penalization strategy optimal across problems we use biasvariance characterization identify notion easy hard model selection problems in particular show variance profile grows rapidly relation biases standard model selection techniques become prone significant errors this happen example regression independent variables drawn widetailed distributions finally discuss new model selection strategy dramatically outperforms standard complexitypenalization holdout methods hard tasks
we consider problem combine collection general regression fit vectors order obtain better predictive model the individual fits may subset linear regression ridge regression something complex like neural network we develop general framework problem examine recent crossvalidationbased proposal called stacking context combination methods based bootstrap analytic methods also derived compared number examples including best subsets regression regression trees finally apply ideas classification problems estimated combination weights yield insight structure problem
we compare performance explanation abilities several machine learning algorithms problem predicting femoral neck fracture recovery among different algorithms semi naive bayesian classifier assistantr seem appropriate we analyze combination decisions several classifiers solving prediction problem show combined classifier improves performance explanation ability
parallel genetic algorithms often reported yield better performance genetic algorithms use single large panmictic population in case island model genetic algorithm informally argued multiple subpopulations helps preserve genetic diversity since island potentially follow different search trajectory search space on hand linearly separable functions often used test island model genetic algorithms possible island models particular well suited separable problems we look island models track multiple search trajectories using infinite population models simple genetic algorithm we also introduce simple model better understanding island model genetic algorithms may advantage processing linearly separable problems
bootstrap samples noise shown effective smoothness capacity control technique training feedforward networks statistical methods generalized additive models it shown noisy bootstrap performs best conjunction weight decay regularization ensemble averaging the twospiral problem highly nonlinear noisefree data used demonstrate findings the combination noisy bootstrap ensemble averaging also shown useful generalized additive modeling also demonstrated well known cleveland heart data
new approaches prior specification structuring autoregressive time series models introduced developed we focus defining classes prior distributions parameters latent variables related latent components autoregressive model observed time series these new priors naturally permit incorporation qualitative quantitative prior information number relative importance physically meaningful components represent low frequency trends quasiperiodic subprocesses high frequency residual noise components observed series the class priors also naturally incorporates uncertainty model order hence leads posterior analysis model order assessment resulting posterior predictive inferences incorporate full uncertainties model order well model parameters analysis also formally incorporates uncertainty leads inferences unknown initial values time series predictions future values posterior analysis involves easily implemented iterative simulation methods developed described one motivating applied field climatology evaluation latent structure especially quasiperiodic structure critical importance connection issues global climatic variability we explore analysis data southern oscillation index soi one several series central recent highprofile debates atmospheric sciences recent apparent trends climatic indicators
summary we detail illustrate time series analysis spectral inference autoregressive models focus underlying latent structure time series decompositions a novel class priors parameters latent components leads new class smoothness priors autoregressive coefficients provides formal inference model order including high order models leads incorporation uncertainty model order summary inferences the class prior models also allows subsets unit roots hence leads inference sustained though stochastically timevarying periodicities time series applications analysis frequency composition time series time spectral domains illustrated study time series astronomy this analyses demonstrates impact utility new class priors addressing model order uncertainty allowing unit root structure time domain decomposition time series estimated latent components provides important alternative view component spectral characteristics series in addition data analysis illustrates utility smoothness prior allowance unit root structure inference spectral densities in particular framework overcomes supposed problems spectral estimation autoregressive models using traditional model fitting methods
this paper presents new method training multilayer perceptron networks called dmp dynamic multilayer perceptron the method based upon divide conquer approach builds networks form binary trees dynamically allocating nodes layers needed the individual nodes network trained using gentetic algorithm the method capable handling realvalued inputs proof given concerning convergence properties basic model simulation results show dmp performs favorably comparison learning algorithms
neurodraughts draughts playing program similar approach neurogammon neurochess tesauro thrun it uses artificial neural network trained method temporal difference learning learn selfplay play game draughts this paper discusses relative contribution board representation search depth training regime architecture run time parameters strength tdplayer produced system keywords temporal difference learning input representation search draughts
in last decade research machine learning developed variety powerful tools inductive learning data analysis on hand research international relations developed variety different conflict databases mostly analyzed classical statistical methods as databases general symbolic nature provide interesting domain application machine learning algorithms this paper gives short overview available conflict databases subsequently concentrates application machine learning methods analysis interpretation databases
in survey review work machine learning methods handling data sets containing large amounts irrelevant information we focus two key issues problem selecting relevant features problem selecting relevant examples we describe advances made topics empirical theoretical work machine learning present general framework use compare different methods we close challenges future work area
we describe illustrate bayesian approaches modelling analysis multiple nonstationary time series this begins univariate models collections related time series assumedly driven underlying unobservable processes referred dynamic latent factor processes we focus models factor processes hence observed time series modelled timevarying autoregressions capable flexibly representing ranges observed nonstationary characteristics we highlight concepts new methods time series decomposition infer characteristics latent components time series relate univariate decomposition analyses underlying multivariate dynamic factor structure our motivating application analysis multiple eeg traces ongoing eeg study duke in study individuals undergoing ect therapy generate multiple eeg traces various scalp locations physiological interest lies identifying dependencies dissimilarities across series in addition multivariate nonstationary aspects series area provides illustration new results decomposition time series latent physically interpretable components illustrated data analysis one eeg data set the paper also discusses current future research directions fl this research supported part national science foundation grant dms the eeg data context arose discussions dr andrew krystal duke university medical center continued interactions valuable address correspondence institute statistics decision sciences duke university durham nc usa httpwwwstatdukeedu
the subsumption problem crucial efficiency ilp learning systems we discuss two subsumption algorithms based strategies preselecting suitable matching literals the class clauses subsumption becomes polynomial superset deterministic clauses we map general problem subsumption certain problem finding clique fixed size graph return show specialization pruning strategy carraghan pardalos clique algorithm provides dramatic reduction subsumption search space we also present empirical results mesh design data set
casebased planning involves storing individual instances problemsolving episodes using tackle new planning problems this paper concerned derivation replay main component form casebased planning called derivational analogy da prior study implementations derivation replay based within statespace planning we motivated acknowledged superiority partialorder po planners plan generation here demonstrate planspace planning also advantage replay we argue decoupling planning derivation order execution order plan steps provided partialorder planners enables exploit guidance previous cases efficient straightforward fashion we validate hypothesis focused empirical comparison
it wellknown fact propositional learning algorithms require good features perform well practice so major step data engineering inductive learning construction good features domain experts these features often represent properties structured objects property typically occurrence certain substructure certain properties to partly automate process feature engineering devised algorithm searches features defined substructures the algorithm stochastically conducts topdown search firstorder clauses clause represents binary feature it differs existing algorithms search classblind capable considering clauses context almost arbitrary length size preliminary experiments favorable support view approach promising
a neural network trend predictor gold bullion market presented a simple recurrent neural network trained recognize turning points gold market based todate history ten market indices the network tested data held back training significant amount predictive power observed the turning point predictions used time transactions gold bullion gold mining company stock index markets obtain significant paper profit test period the training data consisted daily closing prices ten input markets period five years the turning point targets labeled training phase without help financial expert thus experiment shows useful predictions made without use extensive market data knowledge
automating learning causal models sample data key step toward incorporating machine learning automation decisionmaking reasoning uncertainty this paper presents bayesian approach discovery causal models using minimum message length mml method we developed encoding search methods discovering linear causal models the initial experimental results presented paper show mml induction approach recover causal models generated data quite accurate reflections original models results compare favorably tetrad ii program spirtes et al even algorithm supplied prior temporal information mml
we present new algorithm associative reinforcement learning the algorithm based upon idea matching networks output probability probability distribution derived environments reward signal this probability matching algorithm shown perform faster less susceptible local minima previously existing algorithms we use probability matching train mixture experts networks architecture reinforcement learning rules fail converge reliably even simple problems this architecture particularly well suited algorithm compute arbitrarily complex functions yet calculation output probability simple
many casebased reasoning algorithms retrieve cases using derivative knearest neighbor knn classifier whose similarity function sensitive irrelevant interacting noisy features many proposed methods reducing sensitivity parameterize knns similarity function feature weights we focus methods automatically assign weight settings using little domainspecific knowledge our goal predict relative capabilities methods specific dataset characteristics we introduce fivedimensional framework categorizes automated weightsetting methods empirically compare methods along one dimensions summarize results four hypotheses describe additional evidence supports our investigation revealed methods correctly assign low weights completely irrelevant features methods use performance feedback demonstrate three advantages methods ie require less preprocessing better tolerate interacting features crease learning rate
this paper deals problem learning characteristic concept descriptions examples describes new generalization approach implemented system cola the approach tries take advantage information induced descriptions unclassified objects using conceptual clustering algorithm experimental results various realworld domains strongly support hypothesis new approach delivers correct possibly comprehesible concept descriptions exisiting methods induced concept descriptions also used classify objects belong concepts present training data set this paper describes generalization approach implemented cola presents experimental results obtained relational propositional real world data set
local selection ls simple selection scheme evolutionary algorithms individual fitnesses compared fixed threshold rather decide gets reproduce ls coupled fitness functions stemming consumption shared environmental resources maintains diversity way similar fitness sharing however generally efficient fitness sharing lends parallel implementations distributed tasks while ls prone premature convergence applies minimal selection pressure upon population ls therefore appropriate stronger selection schemes certain problem classes this papers characterizes one broad class problems ls consistently performs tournament selection
the potential combined ersjers sar images land cover classification demonstrated raco test site michigan recent papers articles our goal develop classification algorithm stable terms applicability different geographical regions unlike optical remote sensing techniques radar remote sensing provide calibrated data image signal solely determined physical structural electrical properties targets earths surface near subsurface hence classifier based radar signatures object classes applicable new calibrated images without need train classifier this article discusses design applicability classification algorithm based calibrated radar signatures measured ers cband vv polarized jers lband hh polarized sar image data the applicability compared two different test sites raco michigan cedar creek lter site minnesota it found classes separate well certain boundary conditions like comparable seasonality soil moisture conditions observed
presenting analyzing results ai experiments data averaging data snooping proceedings fourteenth national conference artificial intelligence aaai aaai press menlo park california pp copyright aaai presenting analyzing results ai experiments abstract experimental results reported machine learning ai literature misleading this paper investigates common processes data averaging reporting results terms mean standard deviation results multiple trials data snooping context neural networks one popular ai machine learning models both processes result misleading results inaccurate conclusions we demonstrate easily happen propose techniques avoiding important problems for data averaging common presentation assumes distribution individual results gaussian however investigate distribution common problems find often approximate gaussian distribution may symmetric may multimodal we show assuming gaussian distributions significantly affect interpretation results especially comparison studies for controlled task find distribution performance skewed towards better performance smoother target functions skewed towards worse performance complex target functions we propose new guidelines reporting performance provide information actual distribution eg boxwhiskers plots for data snooping demonstrate optimization performance via experimentation multiple parameters lead significance assigned results due chance we suggest precise descriptions experimental techniques important evaluation results need aware potential data snooping biases formulating experimental techniques eg selecting test procedure additionally important rely appropriate statistical tests ensure assumptions made tests valid eg normality distribution
a brief survey biological research noncoding dna presented there growing interest effects noncoding segments evolutionary algorithms eas to better understand conduct research noncoding segments eas important understand biological background work this paper begins review basic genetics terminology describes different types noncoding dna surveys recent intron research
we use simulated soccer study multiagent learning each teams players agents share action set policy may behave differently due positiondependent inputs all agents making team rewarded punished collectively case goals we conduct simulations varying team sizes compare two learning algorithms tdq learning linear neural networks tdq probabilistic incremental program evolution pipe tdq based evaluation functions efs mapping inputaction pairs expected reward pipe searches policy space directly pipe uses adaptive probabilistic prototype trees synthesize programs calculate action probabilities current inputs our results show tdq encounters several difficulties learning appropriate shared efs pipe however depend efs find good policies faster reliably this suggests multiagent learning scenarios direct search policy space offer advantages efbased approaches
a novel supervised learning method presented combining linear discriminant functions neural networks the proposed method results treestructured hybrid architecture due constructive learning binary tree hierarchical architecture automatically generated controlled growing process specific supervised learning task unlike classic decision tree linear discriminant functions merely employed intermediate level tree heuristically partitioning large complicated task several smaller simpler subtasks proposed method these subtasks dealt component neural networks leaves tree accordingly for constructive learning growing creditassignment algorithms developed serve hybrid architecture the proposed architecture provides efficient way apply existing neural networks eg multilayered perceptron solving large scale problem we already applied proposed method universal approximation problem several benchmark classification problems order evaluate performance simulation results shown proposed method yields better results faster training comparison multilayered perceptron
machine learning knowledge engineering always strongly related introduction new representations knowledge engineering created gap this paper describes research aimed applying machine learning techniques current knowledge engineering representations we propose system redesigns part knowledge based system called control knowledge we claim strong similarity redesign knowledge based systems incremental machine learning finally relate work existing research
often learning data one attaches penalty term standard error term attempt prefer simple models prevent overfitting current penalty terms neural networks however often take account weight interaction this critical drawback since effective number parameters network usually differs dramatically total number possible parameters in paper present penalty term uses principal component analysis help detect functional redundancy neural network results show new algorithm gives much accurate estimate network complexity standard approaches as result new term able improve techniques make use penalty term weight decay weight pruning feature selection bayesian predictionrisk tech niques
the dmp dynamic multilayer perceptron network training method based upon divide conquer approach builds networks form binary trees dynamically allocating nodes layers needed this paper introduces dmp method compares preformance dmp using standard delta rule training method training individual nodes performance dmp using genetic algorithm training while basic model require use genetic algorithm training individual nodes results show convergence properties dmp enhanced use genetic algorithm appropriate fitness function
in late developed one early casebased design systems called kritik kritik autonomously generated preliminary conceptual qualitative designs physical devices retrieving adapting past designs stored case memory each case system associated structurebehaviorfunction sbf device model explained structure device accomplished functions these casespecific device models guided process modifying past design meet functional specification new design problem the device models also enabled verification design modifications kritik new complete implementation kritik in paper take retrospective view kritik in early papers described kritik integrating casebased modelbased reasoning in integration kritik also grounds computational process casebased reasoning sbf content theory device comprehension the sbf models provide methods many specific tasks casebased design design adaptation verification also provide vocabulary whole process casebased design retrieval old cases storage new ones this grounding believe essential building wellconstrained theories casebased design
much current research learning bayesian networks fails effectively deal missing data most methods assume data complete make data complete using fairly adhoc methods methods deal missing data learn conditional probabilities assuming structure known we present principled approach learn bayesian network structure well conditional probabilities incomplete data the proposed algorithm iterative method uses combination expectationmaximization em imputation techniques results presented synthetic data sets show performance new algorithm much better adhoc methods handling missing data
researchers field distributed artificial intelligence dai developing efficient mechanisms coordinate activities multiple autonomous agents the need coordination arises agents share resources expertise required achieve goals previous work area includes using sophisticated information exchange protocols investigating heuristics negotiation developing formal models possibilities conflict cooperation among agent interests in order handle changing requirements continuous dynamic environments propose learning means provide additional possibilities effective coordination we use reinforcement learning techniques block pushing problem show agents learn complimentary policies follow desired path without knowledge we theoretically analyze experimentally verify effects learning rate system convergence demonstrate benefits using learned coordination knowledge similar problems reinforcement learning based coordination achieved cooperative noncooperative domains domains noisy communication channels stochastic characteristics present formidable challenge using coordination schemes
the performance error backpropagation bp id learning algorithms compared task mapping english text phonemes stresses under distributed output code developed sejnowski rosenberg shown bp consistently outperforms id task several percentage points three hypotheses explaining difference explored id overfitting training data b bp able share hidden units across several output units hence learn output units better c bp captures statistical information id we conclude hypothesis c correct by augmenting id simple statistical learning procedure performance bp approached matched more complex statistical procedures improve performance bp id substantially a study residual errors suggests still substantial room improvement learning methods texttospeech mapping
we thank steen ladegaard knudsen assistance programming analysis running simulations scott baden assistance vectorizing code cray ymp division engineering block grant time cray san diego supercomputer center members pdpnlp guru research groups ucsd helpful comments earlier versions work
there lot recent interest socalled steady state genetic algorithms gas among things replace individuals typically generation fixed size population size n understanding advantages andor disadvantages replacing fraction population generation rather entire population goal earliest ga research in spite considerable progress understanding gas since proscons overlapping generations remains somewhat cloudy issue however recent theoretical empirical results provide background much clearer understanding issue in paper review combine extend results way significantly sharpens insight
daily experience shows real world meaning many concepts heavily depends implicit context changes context cause less radical changes concepts incremental concept learning domains requires ability recognize adapt changes this paper presents solution incremental learning tasks domain provides explicit clues current context eg attributes characteristic values we present general twolevel learning model realization system named metalb learn detect certain types contextual clues react accordingly context change suspected the model consists base level learner performs regular online learning classification task metalearner identifies potential contextual clues context learning detection occur regular online learning without separate training phases context recognition experiments synthetic domains well realworld problem show metalb robust variety dimensions produces substantial improvement simple objectlevel learning situations changing contexts
this paper investigates memory issues influence long term creative problem solving design activity taking casebased reasoning perspective our exploration based welldocumented example invention telephone alexander graham bell we abstract bells reasoning understanding mechanisms appear time longterm creative design we identify understanding mechanism responsible analogical anticipation design constraints analogical evaluation beside casebased design but already understood design satisfy opportunistically suspended design problems still active background the new mechanisms integrated computational model alec accounts creative
intelligent human agents exist cooperative social environment facilitates learning they learn trialanderror also cooperation sharing instantaneous information episodic experience learned knowledge the key investigations paper given number reinforcement learning agents cooperative agents outperform independent agents communicate learning what price cooperation using independent agents benchmark cooperative agents studied following ways sharing sensation sharing episodes sharing learned policies this paper shows additional sensation another agent beneficial used efficiently b sharing learned policies episodes among agents speeds learning cost communication c joint tasks agents engaging partnership significantly outperform independent agents although may learn slowly beginning these tradeoffs limited multiagent reinforcement learning
we describe sfoil descendant foil uses advanced stochastic search heuristic application learning compose twovoice counterpoint the application required learning ary relation training instances sfoil able efficiently deal learning task knowledge one complex learning task solved ilp system this demonstrates ilp systems scale real databases topdown ilp systems use covering approach advanced search strategies appropriate knowledge discovery databases promising investigation
the human visual system sensitive relative motion objects absolute motion an understanding motion perception requires understanding neural circuits group moving visual elements relative one another based upon hierarchical reference frames we modeled visual relative motion perception using neural network architecture groups visual elements according gestalt commonfate principles exploits information behavior group predict behavior individual elements a simple competitive neural circuit binds visual elements together representation visual object information spiking pattern neurons allows transfer bindings object representation location location neural circuit object moves the model exhibits characteristics human object grouping solves key neural circuit design problems visual relative motion perception
a knowledgelevel analysis complex tasks like diagnosis design give us better understanding tasks terms goals aim achieve different ways achieve goals in paper present knowledgelevel analysis redesign redesign viewed family methods based common principles number dimensions along redesign problem solving methods vary distinguished by examining problemsolving behavior number existing redesign systems approaches came collection problemsolving methods redesign developed taskmethod structure redesign in constructing system redesign large number knowledgerelated choices decisions made in order describe relevant choices redesign problem solving extend current notion possible relations tasks methods psm architecture the realization task problemsolving method decomposition problemsolving method subtasks common relations psm architecture however suggest extend relations notions task refinement method refinement these notions represent intermediate decisions taskmethod structure competence task method refined without immediately paying attention operationalization terms subtasks explicit representation kind intermediate decisions helps make represent decisions piecemeal fashion
in bayesian density estimation prediction using dirichlet process mixtures standard exponential family distributions precision total mass parameter mixing dirichlet process critical hyperparameter strongly influences resulting inferences numbers mixture components this note shows respect flexible class prior distributions parameter posterior may represented simple conditional form easily simulated as result inference key quantity may developed tandem existing routine gibbs sampling algorithms fitting mixture models the concept data augmentation important ever developing extension existing algorithm a final section notes simple asymptotic approx imation posterior
in paper describe study applying knowledgebased neural networks problem diagnosing faults local telephone loops currently nynex uses expert system called max aid human experts diagnosing faults however effective learning algorithm place max would allow easy portability different maintenance centers easy updating phone equipment changes we find machine learning algorithms better accuracy max ii neural networks perform better decision trees iii neural network ensembles perform better standard neural networks iv knowledgebased neural networks perform better standard neural networks v ensemble knowledgebased neural networks performs best
human vision systems integrate information nonlocally across long spatial ranges for example moving stimulus appears smeared viewed briefly ms yet sharp viewed longer exposure ms burr this suggests visual systems combine information along trajectory matches motion stimulus our selforganizing neural network model shows developmental exposure moving stimuli direct formation horizontal trajectoryspecific motion integration pathways unsmear representations moving stimuli these results account burrs data potentially also model phenomena visual inertia
explaining away common pattern reasoning confirmation one cause observed believed event reduces need invoke alternative causes the opposite explaining away also occur confirmation one cause increases belief another we provide general qualitative probabilistic analysis intercausal reasoning identify property interaction among causes product synergy determines form reasoning appropriate product synergy extends qualitative probabilistic network qpn formalism support qualitative intercausal inference directions change probabilistic belief the intercausal relation also justifies occams razor facilitating pruning search likely diagnoses portions paper originally appeared proceedings second international conference principles knowledge representation reasoning supported national science foundation grant iri carnegie mellon rockwell international science center
we introduce new technique enables learner without access hidden information learn nearly well learner access hidden information we apply technique solve open problem maass turan showing concept class f least number queries sufficient learning f algorithm access arbitrary equivalence queries factor log least number queries sufficient learning f algorithm access arbitrary equivalence queries membership queries previously known results imply log bound best possible we describe analogous results two generalizations model function learning apply results bound difficulty learning harder models terms difficulty learning easier model we bound difficulty learning unions k concepts class f terms difficulty learning f we bound difficulty learning noisy environment deterministic algorithms terms difficulty learning noisefree environment we apply variant technique develop algorithm transformation allows probabilistic learning algorithms nearly optimally cope noise a second variant enables us improve general lower bound turan paclearning model queries finally show logarithmically many membership queries never help obtain computationally efficient learning algorithms fl supported air force office scientific research grant fj most work done author tu graz supported lise meitner fellowship fonds zur forderung der wissenschaftlichen forschung austria
in paper describe evolution discretetime recurrent neural network control real mobile robot in experiments evolutionary procedure carried entirely physical robot without human intervention we show autonomous development set behaviors locating battery charger periodically returning achieved lifting constraints design robotenvironment interactions employed preliminary experiment the emergent homing behavior based autonomous development internal neural topographic map predesigned allows robot choose appropriate trajectory function location remaining energy
genetic programming methodology program development consisting special form genetic algorithm capable handling parse trees representing programs successfully applied variety problems in paper new approach construction neural networks based genetic programming presented a linear chromosome combined graph representation network new operators introduced allow evolution architecture weights simultaneously without need local weight optimization this paper describes approach operators reports results application model several binary classification problems
conceptual analogy ca general approach applies conceptual clustering concept representations facilitate efficient use past experiences cases analogical reasoning borner the approach developed implemented syn see also borner borner faauer support design supply nets building engineering this paper sketches task outlines nearestneighborbased agglomerative conceptual clustering applied organizing large amounts structured cases case classes provides concept representation used characterize case classes shows analogous solution new problems based concepts available however main purpose paper evaluate ca terms reasoning efficiency capability derive solutions go beyond cases case base still preserve quality cases
accurate fast estimation probability density functions crucial satisfactory computational performance many scientific problems when type density known priori problem becomes statistical estimation parameters observed values in nonparametric case usual estimators make use kernel functions if x j j n sequence iid random variables estimated probability density function f n kernel method computation values f n x f n x f n x n requires on operations since kernel needs evaluated every x j we propose sequence special weight functions nonparametric estimation f requires almost linear time slowly growing function increases without bound n method requires om n arithmetic operations we derive conditions convergence number metrics turn similar required convergence kernel based methods we also discuss experiments different distributions compare efficiency accuracy computations kernel based estimators various values n
we propose active learning method hiddenunit reduction devised specially multilayer perceptrons mlp first review active learning method point many fisherinformationbased methods applied mlp critical problem information matrix may singular to solve problem derive singularity condition information matrix propose active learning technique applicable
stable neural network control estimation may viewed formally merging concepts nonlinear dynamic systems theory tools multivariate approximation theory this paper extends earlier results adaptive control estimation nonlinear systems using gaussian radial basis functions online generation irregularly sampled networks using tools multiresolution analysis wavelet theory this yields much compact efficient system representations preserving global closedloop stability approximation models employing basis functions localized space spatial frequency admit measure approximated functions spatial frequency content directly dependent reconstruction error as result models afford means adaptively selecting basis functions according local spatial frequency content approximated function an algorithm stable online adaptation output weights simultaneously node configuration class nonparametric models wavelet basis functions presented an asymptotic bound error networks reconstruction derived shown dependent solely minimum approximation error associated steady state node configuration in addition prior bounds temporal bandwidth system identified controlled used develop criterion online selection radial ridge wavelet basis functions thus reducing rate increase networks size dimension state vector experimental results obtained using network predict path unknown light bluff object thrown air activevision based robotic catching system given illustrate networks performance simple realtime application
research bias machine learning algorithms generally concerned impact bias predictive accuracy we believe factors also play role evaluation bias one factor stability algorithm words repeatability results if obtain two sets data phenomenon underlying probability distribution would like learning algorithm induce approximately concepts sets data this paper introduces method quantifying stability based measure agreement concepts we also discuss relationships among stability predictive accuracy bias
in many genetic algorithms applications objective find nearoptimal solution using limited amount computation given requirements difficult find good balance exploration exploitation usually balance found tuning various parameters like selective pressure population size mutation crossover rate genetic algorithm as alternative propose simultaneous tuning selective pressure disruptiveness recombination operators our experiments show combination proper selective pressure highly disruptive recombination operator yields superior performance the reduction mechanism used steadystate ga strong influence optimal crossover disruptiveness using worst fitness deletion strategy building blocks present current best individuals always preserved this releases crossover operator burden maintain good building blocks allows us tune crossover disruptiveness improve search better individuals
crossvalidation frequently used intuitively pleasing technique estimating accuracy theories learned machine learning algorithms during testing machine learning algorithm foil new databases prokaryotic rna transcription promoters developed crossvalidation displayed interesting phenomenon one theory found repeatedly responsible little crossvalidation error whereas theories found infrequently tend responsible majority crossvalidation error it tempting believe frequently found theory modal theory may accurate classifier unseen data theories however experiments showed modal theories accurate unseen data theories found less frequently crossvalidation modal theories may useful predicting crossvalidation poor estimate true accuracy we offer explanations for correspondence department computer science engineering university california san
one significant cost factors robotics applications design development realtime robot control software control theory helps linear controllers developed doesnt sufficiently support generation nonlinear controllers although many cases compliance control nonlinear control essential achieving high performance this paper discusses machine learning applied design nonlinear controllers several alternative function approximators including multilayer perceptrons mlp radial basis function networks rbfns fuzzy controllers analyzed compared leading definition two major families open field function function approximators locally receptive field function approximators it shown rbfns fuzzy controllers bear strong similarities symbolic interpretation this characteristics allows applying symbolic statistic learning algorithms synthesize network layout set examples possibly background knowledge three integrated learning algorithms two original described evaluated experimental test cases the first test case provided robot kuka ir engaged pegintohole task whereas second represented classical prediction task mackeyglass time series from experimental comparison appears fuzzy controllers rbfns synthesised examples excellent approximators practice even accurate mlps
this paper discusses use evolutionary computation evolve behaviors exhibit emergent intelligent behavior genetic algorithms used learn navigation collision avoidance behaviors robots the learning performed simulation resulting behaviors used control actual robot some emergent behavior described detail
judgments similarity soundness important aspects human analogical processing this paper explores judgments modeled using sme simulation gentners structuremapping theory we focus structural evaluation explicating several principles psychologically plausible algorithms follow we introduce specificity conjecture claims naturalistic representations include preponderance appearance loworder information we demonstrate via computational experiments conjecture affects structural evaluation performed including choice normalization technique systematicity preference implemented
genetic algorithms used solve hard optimization problems ranging travelling salesman problem quadratic assignment problem we show simple genetic algorithm used solve optimization problem derived conjunctive normal form problem by separating populations small subpopulations parallel genetic algorithms exploits inherent parallelism genetic algorithms prevents premature convergence genetic algorithms using hillclimbing conduct genetic search space local optima hillclimbing less computationally expensive genetic search we examine effectiveness techniques improving quality solutions cnf problems
icsim simulator structured connectionism development icsi structured connectionism characterized need flexibility efficiency support design reuse modular substructure we take position fast objectoriented language like sather appropriate implementation medium achieve goals the core icsim consists hierarchy classes correspond simulation entities new connectionist models realized combining specializing preexisting classes whenever possible auxillary functionality separated functional modules order keep basic hierarchy clean simple possible
in recent years researchers made considerable progress worstcase analysis inductive learning tasks theoretical results impact practice must deal average case in paper present averagecase analysis simple algorithm induces onelevel decision trees concepts defined single relevant attribute given knowledge number training instances number irrelevant attributes amount class attribute noise class attribute distributions derive expected classification accuracy entire instance space we examine predictions analysis different settings domain parameters comparing exper imental results check reasoning
we compare performance several machine learning algorithms problem prognostics femoral neck fracture recovery knearest neighbours algorithm seminaive bayesian classifier backpropagation weight elimination learning multilayered neural networks lfc lookahead feature construction algorithm assistanti assistantr algorithms top induction decision trees using information gain relieff search heuristics respectively we compare prognostic accuracy explanation ability different classifiers among different algorithms seminaive bayesian classifier assistantr seem appropriate we analyze combination decisions several classifiers solving prediction problems show combined classifier improves performance explanation ability
the structuremapping engine sme successfully modeled several aspects human consistent interpretations analogy while useful theoretical explorations aspect algorithm psychologically implausible computationally inefficient sme contains mechanism focusing interpretations relevant analogizers goals this paper describes modifications sme overcome flaws we describe greedy merge algorithm efficiently computes approximate best interpretation generate alternate interpretations necessary we describe pragmatic marking technique focuses mapping produce relevant yet novel inferences we illustrate techniques via example evaluate performance using empirical data theoretical analysis analogical processing however two significant drawbacks sme constructs structurally
donoho johnstones waveshrink procedure proven valuable signal denoising nonparametric regression waveshrink based principle shrinking wavelet coefficients towards zero remove noise waveshrink broad asymptotic nearoptimality properties in paper introduce new shrinkage scheme semisoft generalizes hard soft shrinkage we study properties shrinkage functions demonstrate semisoft shrinkage offers advantages hard shrinkage uniformly smaller risk less sensitivity small perturbations data soft shrinkage smaller bias overall l risk we also construct approximate pointwise confidence intervals waveshrink address problem threshold selection
one key issues discrete continuous class prediction machine learning general seems problem estimating quality attributes heuristic measures mostly assume independence attributes therefore successfully used domains strong dependencies attributes relief extension relieff statistical methods capable correctly estimating quality attributes classification problems strong dependencies attributes following analysis relieff extended continuous class problems regressional relieff rrelieff relieff provide unified view estimation quality attributes the experiments show rrelieff successfully estimates quality attributes used nonmyopic learning regression trees
we investigate abduction induction integrated common learning framework notion abductive concept learning acl acl extension inductive logic programming ilp case background target theory abductive logic programs abductive notion entailment used coverage relation in framework possible learn incomplete information examples exploiting hypothetical reasoning abduction the paper presents basic framework acl main characteristics illustrates potential addressing several problems ilp learning incomplete information multiple predicate learning an algorithm acl developed suitably extending topdown ilp method concept learning integrating abductive proof procedure abductive logic programming alp a prototype system developed applied learning problems incomplete information the particular role integrity constraints acl investigated showing acl hybrid learning framework integrates explanatory discriminant descriptive characteristic settings ilp
in markov decision process mdp formalization reinforcement learning single adaptive agent interacts environment defined probabilistic transition function in solipsistic view secondary agents part environment therefore fixed behavior the framework markov games allows us widen view include multiple adaptive agents interacting competing goals this paper considers step direction exactly two agents diametrically opposed goals share environment it describes qlearninglike algorithm finding optimal policies demonstrates application simple twoplayer game optimal policy probabilistic
genetic programming promising new method automatically generating functions algorithms natural selection in contrast learning methods genetic programmings automatic programming makes natural approach developing algorithmic robot behaviors in paper present overview apply genetic programming behaviorbased team coordination robocup soccer server domain the result handcoded soccer algorithm team softbots learned play reasonable game soccer
we evolved artificial neural networks control wandering behavior small robots the task touch many squares grid possible fixed period time a number simulated robots embodied small lego trademark robot controlled motorola trademark processor performance compared simulations we observed evolution effective means program control b progress characterized sharply stepped periods improvement separated periods stasis corresponded levels behavioralcomputational complexity c simulated realized robots behaved quite similarly realized robots cases outperforming simulated ones introducing random noise simulations improved fit somewhat hybrid simulatedembodied selection regimes evolutionary robots discussed
the predatorprey domain utilized conduct research distributed artificial intelligence genetic programming used evolve behavioral strategies predator agents to utility predator strategies prey population allowed evolve time the expected competitive learning cycle surface this failing investigated simple prey algorithm surfaces consistently able evade capture predator algorithms
genetic algorithms evolution strategies main representatives class algorithms based model natural evolution discussed wrt basic working mechanisms differences application possibilities the mechanism selfadaptation strategy parameters within evolution strategies emphasized turns major difference genetic algorithms since allows online adaptation strategy parameters without exogenous control
this paper explores two boosting techniques costsensitive tree classification situation misclassification costs change often ideally one would like one induction use induced model different misclassification costs thus demands robustness induced model cost changes combining multiple trees gives robust predictions change we demonstrate ordinary boosting combined minimum expected cost criterion select prediction class good solution situation we also introduce variant ordinary boosting procedure utilizes cost information training we show proposed technique performs better ordinary boosting terms misclassification cost however technique requires induce set new trees every time cost changes our empirical investigation also reveals interesting behavior boosting decision trees costsensitive classification
previous approaches multiagent reinforcement learning either limited heuristic nature the main reason agents animats environment continually changes learning animats keep changing traditional reinforcement learning algorithms properly deal their convergence theorems require repeatable trials strong typically markovian assumptions environment in paper however use novel general sound method multiple reinforcement learning animats living single life limited computational resources unrestricted changing environment the method called incremental selfimprovement is schmidhuber is properly takes account whatever animat learns point may affect learning conditions animats later point the learning algorithm isbased animat embedded policy animat improve performance principle also improve way improves etc at certain times animats life is uses reinforcementtime ratios estimate single training example namely entire life far previously learned things still useful selectively keeps gets rid start appearing harmful is based efficient stackbased backtracking procedure guaranteed make animats learning history history longterm reinforcement accelerations experiments demonstrate is effectiveness in one experiment is learns sequence complex function approximation problems in another multiagent system consisting three coevolving isbased animats chasing learns interesting stochastic predator prey strategies
the breeder genetic algorithm bga depends set control parameters genetic operators in paper shown strategy adaptation competing subpopulations makes bga robust efficient each subpopulation uses different strategy competes subpopulations numerical results pre sented number test functions
we present computational approach acquisition problem schemes learning application analogical problem solving our work background automatic program construction relies concept recursive program schemes in contrast usual approach cognitive modelling computational models designed fit specific data propose framework describe certain empirically established characteristics human problem solving learning uniform formally sound way
genetic algorithms gas play major role many artificiallife systems often little detailed understanding ga performs little theoretical basis characterize types fitness landscapes lead successful ga performance in paper propose strategy addressing issues our strategy consists defining set features fitness landscapes particularly relevant ga experimentally studying various configurations features affect gas performance along number dimensions in paper informally describe initial set proposed feature classes describe detail one class royal road functions present initial experimental results concerning role crossover building blocks landscapes constructed features class
we consider question how one act goal learn much possible building theoretical results fedorov mackay apply techniques optimal experiment design oed guide queryaction selection neural network learner we demonstrate techniques allow learner minimize generalization error exploring domain efficiently completely we conclude panacea oedbased queryaction much offer especially domains high computational costs tolerated this report describes research done center biological computational learning artificial intelligence laboratory massachusetts institute technology support center provided part grant national science foundation contract asc the author also funded atr human information processing laboratories siemens corporate research nsf grant cda
cbet software tool interactive exploration case base cbet integrated environment provides range browsing display functions make possible knowledge extraction set cases cbet motivated application training firemen here cases describe past forest fire fighting interventions cbet used detect dependencies data acquire practical planning competences visualize complex data clustering similar cases in cbet well rooted machine learning techniques selecting relevant features clustering cases forecasting unknown values adapted reused case base exploration
recent studies planning comparing plan reuse plan generation shown tasks may degree computational complexity even deal similar problems the aim paper show kind results apply also diagnosis we propose theoretical complexity analysis coupled experimental tests intended evaluate adequacy adaptation strategies reuse solutions past diagnostic problems order build solution problem solved results analysis show even diagnosis reuse falls complexity class diagnosis generation npcomplete problems practical advantages obtained exploiting hybrid architecture combining casebased modelbased diagnostic problem solving unifying framework
the representation hidden variable models attractor neural networks studied memories stored dynamical attractor continuous manifold fixed points illustrated linear nonlinear networks hidden neurons pattern analysis synthesis forms pattern completion recall stored memory analysis synthesis linear network performed bottomup topdown connections in nonlinear network analysis computation additionally requires rectification nonlinearity inner product inhibition hidden neurons one popular approach sensory processing based generative models assume sensory input patterns synthesized underlying hidden variables for example sounds speech synthesized sequence phonemes images face synthesized pose lighting variables hidden variables useful constitute simpler representation variables visible sensory input using generative model sensory processing requires method pattern analysis given sensory input pattern analysis recovery hidden variables synthesized in words analysis synthesis inverses there number approaches pattern analysis in analysisbysynthesis synthetic model embedded inside negative feedback loop another approach construct separate analysis model this paper explores third approach visiblehidden pairs embedded attractive fixed points attractors state space recurrent neural network the attractors regarded memories stored network analysis synthesis forms pattern completion recall memory the approach illustrated linear nonlinear network architectures in networks synthetic model linear principal
technical report no c jonathan oliver shortened appeared ai statistics abstract in paper examine decision graphs generalization decision trees we present inference scheme construct decision graphs using minimum message length principle empirical tests demonstrate scheme compares favourably decision tree inference schemes this work provides metric comparing relative merit decision tree decision graph formalisms particular domain
for agent living nondeterministic markov environment nme theory fastest way acquiring information statistical properties the answer design optimal sequences experiments performing action sequences maximize expected information gain this notion implemented combining concepts information theory reinforcement learning experiments show resulting method reinforcement driven information acquisition explore certain nmes much faster conventional random exploration
we consider learnability membership queries presence incomplete information in incomplete boundary query model introduced blum et al assumed membership queries instances near boundary target concept may receive dont know answer we show zeroone threshold functions efficiently learnable model the learning algorithm uses split graphs boundary region radius generalization split hypergraphs give splitfinding algorithm boundary region constant radius greater we use notion indistinguishability concepts appropriate model
most techniques verification validation directed functional properties programs however properties programs also essential this paper describes model average computing time kads knowledgebased system based structure an example taken existing knowledgebased system used demonstrate use costmodel designing system
realistic complex planning situations require mixedinitiative planning framework human automated planners interact mutually construct desired plan ideally joint cooperation potential achieving better plans either human machine create alone human planners often take casebased approach planning relying past experience planning retrieving adapting past planning cases planning analogical reasoning generative casebased planning combined prodigyanalogy provides suitable framework study mixedinitiative integration however human user engaged planning loop creates variety new research questions the challenges found creating mixedinitiative planning system fall three categories planning paradigms differ human machine planning visualization plan planning process complex necessary task human users range across spectrum experience respect planning domain underlying planning technology this paper presents approach three problems designing interface incorporate human process planning analogical reasoning prodigyanalogy the interface allows user follow generative casebased planning supports visualization plan planning rationale addresses variance experience user allowing user control presentation information this research sponsored part darparl knowledge based planning scheduling initiative grant number f a short version document appeared cox m t veloso m m supporting combined human machine planning an interface planning analogical reasoning in d b leake e plaza eds casebased reasoning research development second international conference casebased reasoning pp berlin springerverlag
one major goals early concept learners find hypotheses perfectly consistent training data it believed goal would indirectly achieve high degree predictive accuracy set test data later research partially disproved belief however issue consistency yet resolved completely we examine issue consistency new perspective to avoid overfitting training data considerable number current systems sacrificed goal learning hypotheses perfectly consistent training instances setting goal hypothesis simplicity occams razor instead using simplicity goal developed novel approach addresses consistency directly in words concept learner explicit goal selecting appropriate degree consistency training data we begin paper exploring concept learning less perfect consistency next describe system adapt degree consistency response feedback predictive accuracy test data finally present results initial experiments begin address question tightly hypotheses fit training data different problems
in contribution propose new solution problem blind separation sources one dimensional signals images case waveform sources unknown also number for purpose multilayer neural networks associated adaptive learning algorithms developed the primary source signals nongaussian distribution ie subgaussian andor supergaussian computer experiments presented demonstrate validity high performance proposed approach
a new learning algorithm derived performs online stochastic gradient ascent mutual information outputs inputs network in absence priori knowledge signal noise components input propagation information depends calibrating network nonlinearities detailed higherorder moments input density functions by incidentally minimising mutual information outputs well maximising individual entropies network factorises input independent components as example application achieved nearperfect separation ten digitally mixed speech signals our simulations lead us believe network performs better blind separation heraultjutten network reflecting fact derived rigorously mutual information objective
we present method maintaining mixtures prunings prediction decision tree extends nodebased prunings bun wst hs larger class edgebased prunings the method includes efficient online weight allocation algorithm used prediction compression classification although set edgebased prunings given tree much larger nodebased prunings algorithm similar space time complexity previous mixture algorithms trees using general online framework freund schapire fs prove algorithm maintains correctly mixture weights edgebased prunings bounded loss function we also give similar algorithm logarithmic loss function corresponding weight allocation algorithm finally describe experiments comparing nodebased edgebased mixture models estimating probability next word english text show ad vantages edgebased models
markov chain monte carlo mcmc methods including gibbs sampler metropolishastings algorithm commonly used bayesian statistics sampling complicated highdimensional posterior distributions a continuing source uncertainty long sampler must run order converge approximately target stationary distribution rosenthal b presents method compute rigorous theoretical upper bounds number iterations required achieve specified degree convergence total variation distance verifying drift minorization conditions we propose use auxiliary simulations estimate numerical values needed rosenthals theorem our simulation method makes possible compute quantitative convergence bounds models requisite analytical computations would prohibitively difficult impossible on hand although method appears perform well example problems provide guarantees offered analytical proof acknowledgements we thank brad carlin assistance encouragement
uncertainty may taken characterize inferences conclusions premises three under treatments uncertainty inference never characterized uncertainty we explore signiflcance uncertainty premises conclusion argument involves uncertainty we argue uncertainty characterize conclusion inference natural interplay uncertainty premises uncertainty procedure argument we show possible principle incorporate uncertainty premises rendering uncertainty arguments deductively valid but argue reect human argument computationally costly gain simplicity obtained allowing uncertainty inference sometimes outweigh loss exibility entails keywords uncertainty inference logic argument decision premises
in field operation research artificial intelligence several stochastic search algorithms designed based theory global random search zhigljavsky basically techniques iteratively sample search space respect probability distribution updated according result previous samples predefined strategy genetic algorithms gas goldberg greedy randomized adaptive search procedures grasp feo resende two particular instances paradigm in paper present sage search algorithm based fundamental mechanisms techniques however addresses class problems difficult design transformation operators perform local search intrinsic constraints definition problem for problems procedural approach natural way construct solutions resulting state space represented tree dag the aim paper describe underlying heuristics used sage address problems belonging class the performance sage analyzed problem grammar induction successful application problems recent abbadingo dfa learning competition presented
summary we analyze hierarchical bayes model related usual empirical bayes formulation jamesstein estimators we consider running gibbs sampler model using previous results convergence rates markov chains provide rigorous numerical reasonable bounds running time gibbs sampler suitable range prior distributions we apply results baseball data efron morris for different range prior distributions prove gibbs sampler fail converge use information prove case associated posterior distribution nonnormalizable acknowledgements i grateful jun liu suggesting project neal madras suggesting use submartingale convergence theorem herein i thank kate cowles richard tweedie helpful conversations thank referees useful comments
evolutionary algorithms often presented general purpose search methods yet also know search method better another possible problems fact often good deal problem specific information involved choice problem representation search operators in paper explore general properties representations relate neighborhood search methods in particular looked expected number local optima neighborhood search operator averaged overall possible representations the number local optima neighborhood search operator standard binary standard binary reflected gray codes developed explored one measure problem complexity we also relate number local optima another metric designed provide one measure complexity respect simple genetic algorithm choosing good representation vital component solving search problem however choosing good representation problem difficult choosing good search algorithm problem wolpert macreadys no free lunch nfl theorem proves search algorithm better possible discrete functions radcliffe surry extend notions also cover idea representations equivalent behavior considered average possible functions to understand results first outline simple assumptions behind theorem first assume optimization problem discrete describes combinatorial optimization problemsand really optimization problems solved computers since computers finite precision second ignore fact resample points space the no free lunch result stated follows
we investigate effectiveness connectionist networks predicting future continuation temporal sequences the problem overfitting particularly serious short records noisy data addressed method weightelimination term penalizing network complexity added usual cost function backpropagation the ultimate goal prediction accuracy we analyze two time series on benchmark sunspot series networks outperform traditional statistical approaches we show network performance deteriorate input units needed weightelimination also manages extract part dynamics notoriously noisy currency exchange rates makes network solution interpretable
we present detailed analysis evolution genetic programming gp populations using problem finding program returns maximum possible value given terminal function set depth limit program tree known max problem we confirm basic message gathercole ross crossover together program size restrictions responsible premature convergence suboptimal solution we show happen even population retains high level variety show many cases evolution suboptimal solution solution possible sufficient time allowed in cases theoretical models presented compared actual runs
the main operations inductive logic programming ilp generalization specialization make sense generality order in ilp three important generality orders subsumption implication implication relative background knowledge the two languages used often languages clauses languages horn clauses this gives total six different ordered languages in paper give systematic treatment existence nonexistence least generalizations greatest specializations finite sets clauses six ordered sets we survey results already obtained others also contribute answers our main new results firstly existence computable least generalization implication every finite set clauses containing least one nontautologous functionfree clause among necessarily functionfree clauses secondly show least generalization need exist relative implication even set generalized background knowledge functionfree thirdly give complete discussion existence nonexistence greatest specializations six ordered languages
this articles discusses developments bayesian time series modelling analysis relevant studies time series physical engineering sciences with illustrations references discuss bayesian inference computation various statespace models examples analysing quasiperiodic series isolation modelling various components error time series decompositions time series significant latent subseries nonlinear time series models based mixtures autoregressions problems errors uncertainties timing observations development nonlinear models based stochastic deformations time scales
some areas recent development current interest time series noted discussion bayesian modelling efforts motivated substantial practical problems the areas include nonlinear autoregressive time series modelling measurement error structures statespace modelling time series issues timing uncertainties time deformations some discussion needs opportunities work nonsemiparametric models robustness issues given context
we present method unsupervised segmentation data streams originating different unknown sources alternate time we use architecture consisting competing neural networks memory included order resolve ambiguities inputoutput relations in order obtain maximal specialization competition adiabatically increased training our method achieves almost perfect identification segmentation case switching chaotic dynamics input manifolds overlap inputoutput relations ambiguous only small dataset needed training proceedure applications time series complex systems demonstrate potential relevance approach time series analysis shortterm prediction
we outline differential theory learning statistical pattern classification when applied neural networks theory leads efficient differential learning strategy based classification figureofmerit cfm objective functions differential learning guarantees highest probability generalization classifier limited functional complexity trained limited number examples the theory significant two reasons we demonstrate importance differential learnings efficiency simple pattern recognition task lends closedform analysis we conclude practical application theory differentially trained perceptron diagnoses crippling joint disorder magnetic resonance images better probabilistically trained counterpart complex probabilistically trained multilayer perceptrons the recent renaissance connectionism led considerable amount research regarding generalization neural network pattern classifiers trained supervised fashion most research done computational learning theorists statisticians intent matching functional complexity classifier size training sample order avoid wellknown curse dimensionality see example work barron baum haussler vapnik much summarized yet relatively little attention paid effect objective function used drive supervised learning procedure discrimination generalization fl copyright c fl j b hampshire ii b v k v kumar rights reserved copyright automatically extended ieee submission accepted presentationpublication this research funded air force office scientific research grant afosr supported supercomputing grant national science foundations pittsburgh supercomputing center grant ccrp the views conclusions contained submission authors interpreted representing official policies either expressed implied us air force national science foundation us government
anaplastic thyroid carcinoma rare aggressive tumor many factors might influence survival patients suggested the aim study determine factors known time admission hospital might predict survival patients anaplastic thyroid carcinoma our aim also assess relative importance factors identify potentially useful decision regression trees generated machine learning algorithms our study included patients females males mean age years anaplastic thyroid carcinoma treated institute oncology ljubljana patients classified categories according attributes sex age history physical findings extent disease admission tumor morphology in paper compare machine learning approach previous statistical evaluations problem univariate multivariate analysis show provide thorough analysis improve understanding data
we study behavior family learning algorithms based suttons method temporal differences in online learning framework learning takes place sequence trials goal learning algorithm estimate discounted sum reinforcements received future in setting able prove general upper bounds performance slightly modified version suttons socalled td algorithm these bounds stated terms performance best linear predictor given training sequence proved without making statistical assumptions kind process producing learners observed training sequence we also prove lower bounds performance algorithm learning problem give similar analysis closely related problem learning predict model learner must produce predictions whole batch observations receiving reinforcement
the common use static binary placevalue codes realvalued parameters phenotype hollands genetic algorithm ga forces either sacrifice representational precision efficiency search vice versa dynamic parameter encoding dpe mechanism avoids dilemma using convergence statistics derived ga population adaptively control mapping fixedlength binary genes real values dpe shown empirically effective amenable analysis explore problem premature convergence gas two convergence models
traditionally genetic algorithms relied upon point crossover operators many recent empirical studies however shown benefits higher numbers crossover points some intriguing recent work focused uniform crossover involves average l crossover points strings length l despite theoretical analysis however appears difficult predict particular crossover form optimal given problem this paper describes adaptive genetic algorithm decides runs form optimal
this paper reviews application gibbs sampling cointegrated var system aggregate imports import prices belgium modelled using two cointegrating relations gibbs sampling techniques used estimate bayesian perspective cointegrating relations weights var system extensive use spectral analysis made get insight convergence issues
we provide generic monte carlo method find alternative maximum expected utility decision analysis we define artificial distribution product space alternatives states show optimal alternative mode implied marginal distribution alternatives after drawing sample artificial distribution may use exploratory data analysis tools approximately identify optimal alternative we illustrate method important types influence diagrams decision analysis influence diagrams markov chain monte carlo simulation
this paper describes new samplingbased heuristic tree search named sage presents analysis performance problem grammar induction this last work inspired abbadingo dfa learning competition took place mars november sage ended one two winners competition the second winning algorithm first proposed rodney price implements new evidencedriven heuristic state merging our version heuristic also described paper compared sage
conversational casebased reasoning ccbr successfully used assist case retrieval tasks however behavioral limitations ccbr motivate search integrations reasoning approaches this paper briefly describes groups ongoing efforts towards enhancing inferencing behaviors conversational casebased reasoning development tool named nacodae in particular focus integrating nacodae machine learning modelbased reasoning generative planning modules this paper defines ccbr briefly summarizes integrations explains enhance overall system our research focuses enhancing performance conversational casebased reasoning ccbr systems aha breslow ccbr form casebased reasoning users initiate problem solving conversations entering initial problem description natural language text this text assumed partial rather complete problem description the ccbr system assists eliciting refinements description suggesting solutions its primary purpose provide focus attention user quickly provide solutions problem figure summarizes ccbr problem solving cycle cases ccbr library three components
the identification design implementation strategies cooperation central research issue field distributed artificial intelligence dai we propose novel approach construction cooperation strategies group problem solvers based genetic programming gp paradigm gps class adaptive algorithms used evolve solution structures optimize given evaluation criterion our approach based designing representation cooperation strategies manipulated gps we present results experiments predatorprey domain extensively studied easytodescribe difficulttosolve cooperation problem domain the key aspect approach minimal reliance domain knowledge human intervention construction good cooperation strategies promising comparison results prior systems lend credence viability ap proach
recently new approach involves form simulated evolution proposed building autonomous robots however still clear approach may adequate face real life problems in paper show control systems perform nontrivial sequence behaviors obtained methodology carefully designing conditions evolutionary process operates in experiment described paper mobile robot trained locate recognize grasp target object the controller robot evolved simulation downloaded tested real robot
it open chromosomal dimension performs best although higherdimensional encodings whether real imaginary preserve geographical gene linkages suspect high dimension would perform desirably we studying question dimension encoding best given instance it likely optimal dimension somehow dependent chromosome size input graph topology interactions flexibility crossover yet unknown the interaction considerations number cuts used crossover also open issue in relocating genes onto multidimensional chromosome simplest way via sequential assignment rowmajor order section showed performance improves dfsrowmajor reembedding used two threedimensional encodings we suspect phenomenon consistent higherdimensional cases hope perform detailed investigations future although dfs reordering proved helpful linear encodings multidimensional encodings believe dfsrowmajor reembedding good approach multidimensional cases since rowmajor embedding simplistic we considering alternative dimensional dimensional reembeddings hopefully provide improvement t n bui b r moon hyperplane synthesis genetic algorithms in fifth international conference genetic algorithms pages july t n bui b r moon analyzing hyperplane synthesis genetic algorithms using clustered schemata in international conference evolutionary computation oct lecture notes computer science springerverlag
increasing attention paid reinforcement learning algorithms recent years partly due successes theoretical analysis behavior markov environments if markov assumption removed however neither generally algorithms analyses continue usable we propose analyze new learning algorithm solve certain class nonmarkov decision problems our algorithm applies problems environment markov learner restricted access state information the algorithm involves montecarlo policy evaluation combined policy improvement method similar markov decision problems guaranteed converge local maximum the algorithm operates space stochastic policies space yield policy performs considerably better deterministic policy although space stochastic policies continuouseven discrete action spaceour algorithm computationally tractable
markov chain monte carlo mcmc algorithms revolutionized bayesian practice in simplest form ie parameters updated one time however often slow converge applied highdimensional statistical models a remedy problem block parameters groups updated simultaneously using either gibbs metropolishastings step in paper construct several partially fully blocked mcmc algorithms minimizing autocorrelation mcmc samples arising important classes longitudinal data models we exploit identity used chib context bayes factor computation show parameters general linear mixed model may updated single block improving convergence producing essentially independent draws posterior parameters interest we also investigate value blocking nongaussian mixed models well class binary response data longitudinal models we illustrate approaches detail three realdata examples
this paper presents comparison two feature selection methods importance score is based greedylike search genetic algorithmbased ga method order better understand strengths limitations area application the results experiments show strong relation nature data behavior systems the importance score method efficient dealing little noise small number interacting features genetic algorithms provide robust solution expense increased computational effort keywords feature selection machine learning genetic algorithms search
this paper describes training algorithm simple synchrony networks ssns reports experiments language learning using recursive grammar the ssn new connectionist architecture combining technique learning patterns across time simple recurrent networks srns temporal synchrony variable binding tsvb the use tsvb means ssn learn entities training set generalise information entities test set in experiments network trained sentences one embedded clause words restricted certain classes constituent during testing network generalises information learned sentences three embedded clauses words appearing constituent these results demonstrate ssns learn generalisations across syntactic constituents
we use directed search techniques space computer programs learn recursive sequences positive integers specifically integer sequences squares x cubes x factorial x fibonacci numbers studied given small finite prefix sequence show three directed searchesmachinelanguage genetic programming crossover exhaustive iterative hill climbing hybrid crossover hill climbingcan automatically discover programs exactly reproduce finite target prefix moreover correctly produce remaining sequence underlying machines precision our machinelanguage representation genericit contains instructions arithmetic register manipulation comparison control flow we also introduce output instruction allows variablelength sequences result values importantly representation contain recursive operators recursion needed automatically synthesized primitive instructions for fixed set search parameters eg instruction set program size fitness criteria compare efficiencies three directed search techniques four sequence problems for parameter set evolutionarybased search always outperforms exhaustive hill climbing well undirected random search since prefix target sequence variable experiments posit approach sequence induction potentially quite general
this paper studies problem establishes desired implication analytic systems several cases compact state space ii poisson stability condition iii generic sense in addition paper studies accessibility properties control sets recently introduced context dynamical systems studies finally various examples counterexamples provided relating various lie algebras introduced past work
this paper demonstrates use graphs mathematical tool expressing independencies formal language communicating processing causal information decision analysis we show complex information external interventions organized represented graphically conversely graphical representation used facilitate quantitative predictions effects interventions we first review theory bayesian networks show directed acyclic graphs dags offer economical scheme representing conditional independence assumptions deducing displaying logical consequences assumptions we introduce manipulative account causation show dag defines simple transformation tells us probability distribution change result external interventions system using transformation possible quantify nonexperimental data effects external interventions specify conditions randomized experiments necessary as example show effect smoking lung cancer quantified nonexperimental data using minimal set qualitative assumptions finally paper offers graphical interpretation rubins model causal effects demonstrates equivalence manipulative account causation we exemplify tradeoffs two approaches deriving nonparametric bounds treatment effects conditions imperfect compliance fl portions paper presented th session international statistical institute florence italy august september
an interesting classical result due jackson allows polynomialtime learning function class dnf using membership queries since practical learning situations access membership oracle unrealistic paper explores possibility quantum computation might allow learning algorithm dnf relies example queries a natural extension fourierbased learning quantum domain presented the algorithm requires example oracle runs o n time result appears classically impossible the algorithm unique among quantum algorithms assume priori knowledge function operate superposition includes possible basis states
a number exact algorithms developed perform probabilistic inference bayesian belief networks recent years these algorithms use graphtheoretic techniques analyze exploit network topology in paper examine problem efficient probabilistic inference belief network combinatorial optimization problem finding optimal factoring given algebraic expression set probability distributions we define combinatorial optimization problem optimal factoring problem discuss application problem belief networks we show optimal factoring provides insight key elements efficient probabilistic inference present simple easily implemented algorithms excellent performance we also show use algebraic perspective permits significant extension belief net representation
a global classification currently known protein sequences performed every protein sequence partitioned segments amino acids dynamicprogramming distance calculated pair segments this space segments first embedded euclidean space small metric distortion a novel selforganized crossvalidated clustering algorithm applied embedded space euclidean distances the resulting hierarchical tree clusters offers new representation protein sequences families compares favorably updated classifications based functional structural protein data motifs domains zinc finger ef hand homeobox egflike others automatically correctly identified a novel representation protein families introduced functional biological kinship protein families deduced demonstrated transporters family
explanation important issue building computerbased interactive design environments human designer knowledge system may cooperatively solve design problem we consider two related problems explaining systems reasoning design generated system in particular analyze content explanations design reasoning design solutions domain physical devices we describe two complementary languages taskmethodknowledge models explaining design reasoning structurebehaviorfunction models explaining device designs interactive kritik computer program uses representations visually illustrate systems reasoning result design episode the explanation design reasoning interactive kritik context evolving design solution similarly explanation design solution context design reasoning
in paper describe implementation backpropagation algorithm means object oriented library arch the use library relieve user details specific parallel programming paradigm time allows greater portability generated code to provide comparision existing solutions survey relevant implementations algorithm proposed far literature dedicated general purpose computers extensive experimental results show use library hurt performance simulator contrary implementation connection machine cm comparable fastest category
typical home comfort systems utilize rudimentary forms energy management conservation the sophisticated technology common use today automatic setback thermostat tremendous potential remains improving efficiency electric gas usage however home residents ignorant physics energy utilization design environmental control strategies neither energy management experts ignorant behavior patterns inhabitants adaptive control seems alternative we begun building adaptive control system infer appropriate rules operation home comfort systems based lifestyle inhabitants energy conservation goals recent research demonstrated potential neural networks intelligent control we constructing prototype control system actual residence using neural network reinforcement learning prediction techniques the residence equipped sensors provide information environmental conditions eg temperatures ambient lighting level sound motion room actuators control gas furnace electric space heaters gas hot water heater lighting motorized blinds ceiling fans dampers heating ducts this paper presents overview project stands
today great interest discovering methods allow faster design development realtime control software control theory helps linear controllers developed support generation in paper discussed machine learning applied function locally receptive field function approximators three integrated learning algorithms two original described tried two experimental test cases the first test case provided industrial robot kuka ir engaged pegintohole task second classical prediction task mackeyglass chaotic series from experimental comparison appears fuzzy controllers rbfns synthesised examples excellent approximators even accurate mlps nonlinear controllers many cases compliant motion control
the term soft computing sc represents combination emerging problemsolving technologies fuzzy logic fl probabilistic reasoning pr neural networks nns genetic algorithms gas each technologies provide us complementary reasoning searching methods solve complex realworld problems after brief description technologies analyze useful combinations use fl control gas nns parameters application gas evolve nns topologies weights tune fl controllers implementation fl controllers nns tuned backpropagationtype algorithms
two issues intelligent navigation robot addressed work first robots ability learn representation local environment use representation identify local environment this done first extracting features sensors informative distances obstacles various directions using features reduced ring representation rrr local environment derived as robot navigates learns rrr signatures new environment types encounters for purpose identification ring matching criteria proposed robot tries match rrr sensory input one rrrs library the second issue addressed learning hill climbing control laws local environments unlike conventional neurocontrollers reinforcement learning framework robot first learns model environment learns control law terms neural network proposed the reinforcement function generated sensory inputs robot control action taken three key results shown work the robot able build library rrr signatures perfectly even significant sensor noise eight different local environmets it able identify local environment accuracy library build robot able learn adequate hill climbing control laws take distinctive state local environment five different environment types
as artificial neural networks anns gain popularity variety application domains critical models run fast generate results real time although number implementations neural networks available sequential machines implementations require inordinate amount time train run anns especially ann models large one approach speeding implementation anns implement parallel machines this paper surveys area parallel environments implementations anns prescribes desired characteristics look implementations
we derive distributionfree uniform test error bounds improve vctype bounds validation we show use knowledge test inputs improve bounds the bounds sharp require intense computation we introduce method trade sharpness speed computation also compute bounds several test cases
connectionist research firmly established within scientific community especially within multidisciplinary field cognitive science this diversity however created environment makes difficult connectionist researchers remain aware recent advances field let alone understand field developed this paper attempts address problem providing brief guide connectionist research the paper begins defining basic tenets connectionism next development connectionist research traced commencing connectionisms philosophical predecessors moving early psychological neuropsychological influences followed mathematical computing contributions connectionist research current research reviewed focusing specifically different types network architectures learning rules use the paper concludes suggesting neural network researchat least cognitive scienceshould move towards models incorporate relevant functional principles inherent neurobiological systems
scheiblechner proposes probabilistic axiomatization measurement called isop isotonic ordinal probabilistic models replaces raschs specific objectivity assumptions two interesting ordinal assumptions special cases scheiblechners model include standard unidimensional factor analysis models loadings held constant rasch model binary item responses closely related doublymonotone item response models mokken see also mokken lewis sijtsma molenaar sijtsma junker sijtsma hemker more generally strictly unidimensional latent variable models considered detail holland rosenbaum ellis van den wollenberg junker the purpose note provide connections current research foundations nonparametric latent variable item response modeling missing scheiblechners paper point important related work hemker et al ab ellis junker junker ellis we also discuss counterexamples three major theorems paper by carrying three tasks hope provide researchers interested foundations measurement item response modeling opportunity give isop approach careful attention deserves
the sensorimotor integration system viewed observer attempting estimate state state environment integrating multiple sources information we describe computational framework capturing notion specific models integration adaptation result psychophysical results two sensorimotor systems subserving integration adaptation visuoauditory maps estimation state hand arm movements presented analyzed within framework these results suggest spatial information visual auditory systems integrated reduce variance localization the effects remapping relation visual auditory space predicted simple learning rule the temporal propagation errors estimating hands state captured linear dynamic observer providing evidence existence internal model simulates dynamic behavior arm
several researchers demonstrated complex action sequences learned neuroevolution ie evolving neural networks genetic algorithms however complex general behavior evading predators avoiding obstacles tied specific environments turns difficult evolve often system discovers mechanical strategies moving back forth help agent cope effective appear believable would generalize new environments the problem general strategy difficult evolution system discover directly this paper proposes approach complex general behavior learned incrementally starting simpler behavior gradually making task challenging general the task transitions implemented successive stages deltacoding ie evolving modifications allows even converged populations adapt new task the method tested stochastic dynamic task prey capture compared direct evolution the incremental approach evolves effective general behavior also scale harder tasks
go difficult game computers master best go programs still weaker average human player since traditional game playing techniques proven inadequate new approaches computer go need studied this paper presents new approach learning play go the sane symbiotic adaptive neuroevolution method used evolve networks capable playing go small boards preprogrammed go knowledge on fi go board networks able defeat simple computer opponent evolved within hundred generations most significantly networks exhibited several aspects general go playing suggests approach could scale well
recent studies floating building block representation genetic algorithm ga suggest many advantages using floating representation this paper investigates behavior ga floating representation problems response three different types pressures reduction amount genetic material available ga problem solving process functions negativevalued building blocks randomizing noncoding segments results indicate gas performance floating representation problems robust significant reductions genetic material genome length may made relatively small decrease performance the ga effectively solve problems negative building blocks randomizing noncoding segments appears improve rather harm ga performance
this work initiated junker visiting university utrecht support carnegie mellon university faculty development grant generous hospitality social sciences faculty university utrecht additional support provided office naval research cognitive sciences division grant nk national institute mental health training grant mh
we analyze simple hillclimbing algorithm rmhc previously shown outperform genetic algorithm ga simple royal road function we analyze idealized genetic algorithm iga significantly faster rmhc gives lower bound ga speed we identify features iga give rise speedup discuss features incorporated real ga
we consider recently proposed parallel variable distribution pvd algorithm ferris mangasarian solving optimization problems variables distributed among p processors each processor primary responsibility updating block variables allowing remaining secondary variables change restricted fashion along easily computable directions we propose useful generalizations consist general unconstrained case replacing exact global solution subproblems certain natural sufficient descent condition convex case inexact subproblem solution pvd algorithm these modifications key features algorithm analyzed the proposed modified algorithms practical make easier achieve good load balancing among parallel processors we present general framework analysis class algorithms derive new improved linear convergence results problems weak sharp minima order strongly convex problems we also show nonmonotone synchronization schemes admissible improves flexibility pvd approach
a paradigm statistical mechanics financial markets smfm fit multivariate financial markets using adaptive simulated annealing asa global optimization algorithm perform maximum likelihood fits lagrangians defined path integrals multivariate conditional probabilities canonical momenta thereby derived used technical indicators recursive asa optimization process tune trading rules these trading rules used outofsample data demonstrate profit smfm model illustrate markets likely efficient this methodology extended systems eg electroencephalography this approach complex systems emphasizes utility blending intuitive powerful mathematicalphysics formalism generate indicators used aitype rulebased models management
the computational power formal models networks spiking neurons compared neural network models based mcculloch pitts neurons ie threshold gates respectively sigmoidal gates in particular shown networks spiking neurons computationally powerful neural network models a concrete biologically relevant function exhibited computed single spiking neuron biologically reasonable values parameters requires hundreds hidden units sigmoidal neural net this article assume prior knowledge spiking neurons contains extensive list references currently available literature computations networks spiking neurons relevant results neuro biology
we compare genetic algorithms ga functional search method very fast simulated reannealing vfsr efficient search strategy also statistically guaranteed find function optima ga previously demonstrated competitive standard boltzmanntype simulated annealing techniques presenting suite six standard test functions ga vfsr codes previous studies without additional fine tuning strongly suggests vfsr expected orders magnitude efficient ga
in recent years machine learning research started addressing problem known theory refinement the goal theory refinement learner modify incomplete incorrect rule base representing domain theory make consistent set input training examples this paper presents major revision either propositional theory refinement system two issues discussed first show run time efficiency greatly improved changing exhaustive scheme computing repairs iterative greedy method second show extend either refine mofn rules the resulting algorithm neither new either order magnitude faster produces significantly accurate results theories fit mofn format to demonstrate advantages neither present experimental results two realworld domains
we consider problem belief aggregation given group individual agents probabilistic beliefs set uncertain events formulate sensible consensus aggregate probability distribution events researchers proposed many aggregation methods although question best general consensus consensus we develop marketbased approach problem agents bet uncertain events buying selling securities contingent outcomes each agent acts market maximize expected utility given securities prices limited activity risk aversion the equilibrium prices goods market represent aggregate beliefs for agents constant risk aversion demonstrate aggregate probability exhibits several desirable properties related independently motivated techniques we argue marketbased approach provides plausible mechanism belief aggregation multiagent systems directly addresses selfmotivated agent incentives participation truthfulness provide decisiontheoretic foundation expert weights often employed centralized pooling techniques
predictability minimization pm schmidhuber exhibits various intuitive theoretical advantages many methods unsupervised redundancy reduction so far however toy applications pm in paper apply semilinear pm static real world images find without teacher without significant preprocessing system automatically learns generate distributed representations based wellknown feature detectors orientation sensitive edge detectors offcenteronsurroundlike structures thus extracting simple features related considered useful image preprocessing compression
a learners modifiable components called policy an algorithm modifies policy learning algorithm if learning algorithm modifiable components represented part policy speak selfmodifying policy smp smps modify way modify etc they interest situations initial learning algorithm improved experience call learning learn how force stochastic smp trigger better better selfmodifications the successstory algorithm ssa addresses question lifelong reinforcement learning context during learners lifetime ssa occasionally called times computed according smp ssa uses backtracking undo smpgenerated smpmodifications empirically observed trigger lifelong reward accelerations measured current ssa call evaluates longterm effects smpmodifications setting stage later smpmodifications smpmodifications survive ssa represent lifelong success history until next ssa call build basis additional smpmodifications solely selfmodifications smpssabased learners solve complex task partially observable environment poe whose state space far bigger reported poe literature
we study task sequences allow speeding learners average reward intake appropriate shifts inductive bias changes learners policy to evaluate longterm effects bias shifts setting stage later bias shifts use successstory algorithm ssa ssa occasionally called times may depend policy it uses backtracking undo bias shifts empirically observed trigger longterm reward accelerations measured current ssa call bias shifts survive ssa represent lifelong success history until next ssa call considered useful build basis additional bias shifts ssa allows plugging wide variety learning algorithms we plug novel adaptive extension levin search method embedding learners policy modification strategy within policy incremental selfimprovement our inductive transfer case studies involve complex partially observable environments traditional reinforcement learning fails
the inductive logic programming system lopster created demonstrate advantage basing induction logical implication rather subsumption lopsters subunification procedures allow induce recursive relations using minimum number examples whereas inductive logic programming algorithms based subsumption require many examples solve induction tasks however lopsters input examples must carefully chosen must along inverse resolution path we hypothesize extension lopster efficiently induce recursive relations without requirement we introduce generalization lopster named crustacean capability empirically evaluate ability induce recursive relations
submitted nips td popular family algorithms approximate policy evaluation large mdps td works incrementally updating value function observed transition it two major drawbacks makes inefficient use data requires user manually tune stepsize schedule good performance for case linear value function approximations leastsquares td lstd algorithm bradtke barto eliminates stepsize parameters improves data efficiency this paper extends bradtke bartos work three significant ways first presents simpler derivation lstd algorithm second generalizes arbitrary values extreme resulting algorithm shown practical formulation supervised linear regression third presents novel intuitive interpretation lstd modelbased reinforcement learning technique
technical report no department statistics university toronto abstract simulated annealing moving tractable distribution distribution interest via sequence intermediate distributions traditionally used inexact method handling isolated modes markov chain samplers here shown one use markov chain transitions annealing sequence define importance sampler the markov chain aspect allows method perform acceptably even highdimensional problems finding good importance sampling distributions would otherwise difficult use importance weights ensures estimates found converge correct values number annealing runs increases this annealed importance sampling procedure resembles second half previouslystudied tempered transitions seen generalization recentlyproposed variant sequential importance sampling it also related thermodynamic integration methods estimating ratios normalizing constants annealed importance sampling attractive isolated modes present estimates normalizing constants required may also generally useful since independent sampling allows one bypass problems assessing convergence autocorrelation markov chain samplers
the genetic programming optimization method gp elaborated john koza koza variant genetic algorithms the search space problem domain consists computer programs represented parse trees crossover operator realized exchange subtrees empirical analyses show large parts trees never used evaluated means parts trees irrelevant solution redundant this paper concerned identification redundancy occuring gp it starts mathematical description behavior gp conclusions drawn description among others explain size problem denotes phenomenon average size trees population grows time
a year ago new metaheuristic graph coloring problems introduced costa hertz dubuis they shown computer experiments clear indication benefits approach graph coloring many applications specially areas scheduling assignments timetabling the metaheuristic classified memetic algorithm since based population search periods local optimization interspersed phases new configurations created earlier welldeveloped configurations local minima previous iterative improvement process the new population created using crossover operators genetic algorithms in paper discuss methodology inspired competitive analysis may relevant problem designing better crossover operators resumo no ultimo ano uma nova metaheurstica para problema de coloracao em grafos foi apresentada por costa hertz e dubuis eles mostraram com experimentos computacionais algumas indicacoes claras dos benefcios desta nova tecnica coloracao em grafos tem muitas aplicacoes especialmente na area de programacao de tarefas localizacao e horario a metaheurstica pode ser classificada como algoritmo memetico desde que seja baseada em uma busca de populacao cujos perodos de otimizacao local sao intercalados com fases onde novas configuracoes sao criadas partir de boas configuracoes ou mnimos locais de iteracoes anteriores a nova populacao e criada usando operacoes de crossover como em algoritmos geneticos neste artigo apresentamos como uma metodologia baseada em competitive analysis pode ser relevante para construir operacoes de crossover
for wellknown concept decision trees used inductive inference study natural concept equivalence two decision trees equivalent represent hypothesis we present simple efficient algorithm establish whether two decision trees equivalent the complexity algorithm bounded product sizes decision trees the hypothesis represented decision tree essentially boolean function like proposition although every boolean function represented way show disjunctions conjunctions decision trees efficiently represented decision trees simply shaped propositions may require exponential size representation de cision trees
we propose assess relevance theories synaptic modification models feature extraction human vision using masks derived synaptic weight patterns occlude parts stimulus images psychophysical experiments in experiment reported found mask derived principal component analysis object images effective reducing generalization performance human subjects mask derived another method feature extraction bcm based higherorder statistics images
a two dimensional timedependent duffing oscillator model macroscopic neocortex exhibits chaos ranges parameters we embed model moderate noise typical context presented real neocortex using pathint nonmontecarlo pathintegral algorithm particularly adept handling nonlinear fokkerplanck systems this approach shows promise investigate whether chaos neocortex predicted models survive noisy contexts
the purpose architecture optimization schemes improve generalization in presentation suggest estimate weight saliency associated change generalization error weight pruned we detail implementation on storage scheme extending obd well on scheme extending obs we illustrate viability approach pre diction chaotic time series
in paper employ genetic programming paradigm enable computer learn play strategies ancient egyptian boardgame senet evolving board evaluation functions formulating problem terms board evaluation functions made feasible evaluate fitness game playing strategies using tournamentstyle fitness evaluation the game elements strategy chance our approach learns strategies enable computer play consistently reasonably skillful level
this paper introduces new algorithm q optimizing expected output multiinput noisy continuous function q designed need experiments avoids strong assumptions form function autonomous requires little problemspecific tweaking four existing approaches problem response surface methods numerical optimization supervised learning evolutionary methods inadequacies requirement black box behavior combined need experiments q uses instancebased determination convex region interest performing experiments in conventional instancebased approaches learning neighborhood defined proximity query point in contrast q defines neighborhood new geometric procedure captures size shape zone possible optimum locations q also optimizes weighted combinations outputs finds inputs produce target outputs we compare q optimizers noisy functions several problems including simulated noisy process nonlinear continuous dynamics discreteevent queueing components results encouraging terms speed autonomy
in paper propose multiclassification approach constructive induction the idea improvement classification accuracy based iterative modification input data space this process independently repeated pair n classes finally gives n n input data subspaces attributes dedicated optimal discrimination appropriate pairs classes we use genetic algorithms constructive induction engine a final classification obtained weighted majority voting rule according n classifier approach the computational experiment performed medical data set the obtained results point advantage using multiclassification model n classifier constructive induction relation analogous singleclassifier approach
this highly interdisciplinary project extends previous work combat modeling controltheoretic descriptions decisionmaking human factors complex activities a previous paper established first theory statistical mechanics combat smc developed using modern methods statistical mechanics baselined empirical data gleaned national training center ntc this previous project also established janustntc computer simulationwargame ntc providing statistical whatif capability ntc scenarios this mathematical formulation ripe controltheoretic extension include human factors methodology previously developed context teleoperated vehicles similar ntc scenarios differing crucial decision points used data model inuence decision making combat the results may used improve present human factors c algorithms computer simulationswargames our approach subordinate smc nonlinear stochastic equations fitted ntc scenarios establish zeroth order description combat in practice equivalent mathematicalphysics representation used suitable numerical formal work ie lagrangian representation theoretically equations nested within larger set nonlinear stochastic operatorequations include c human factors eg supervisory decisions in study propose perturb operator theory smc zeroth order set equations then subsets scenarios fit zeroth order originally considered similarly degenerate split perturbatively distinguish c decisionmaking inuences new methods very fast simulated reannealing vfsr developed previous project used fitting models empirical data
the work progress reported wright liley shows great promise primarily experimental simulation paradigms however tentative conclusion macroscopic neocortex may considered approximately linear nearequilibrium system premature correspond tentative conclusions drawn studies neocortex at time exists interdisciplinary multidimensional gradation published studies neocortex one primary dimension mathematical physics represented two extremes at one extreme much scientifically unsupported talk chaos quantum physics responsible many important macroscopic neocortical processes involving many thousands millions neurons wilczek at another extreme many nonmathematically trained neuroscientists uncritically lump neocortical mathematical theory one file consider statistical averages citations opinions quality research nunez in context important appreciate wright liley wl report scientifically sound studies macroscopic neocortical function based simulation blend sound theory reproducible experiments however pioneering work given absence much knowledge neocortex time open criticism especially respect present inferences conclusions their conclusion eeg data exhibit linear nearequilibrium dynamics may well true sense focusing one local minima possibly individualspecific physiologicalstate dependent
traditional evolutionary optimization algorithms assume static evaluation function according solutions evolved incremental evolution approach dynamic evaluation function scaled time order improve performance evolutionary optimization in paper present empirical results demonstrate effectiveness approach genetic programming using two domains twoagent pursuitevasion game tracker trailfollowing task demonstrate incremental evolution successful applied near beginning evolutionary run we also show incremental evolution successful intermediate evaluation functions difficult target evaluation function well easier target function
nklandscapes offer ability assess performance evolutionary algorithms problems different degrees epistasis in paper study performance six algorithms nklandscapes low high dimension keeping amount epistatic interactions constant the results show compared genetic local search algorithms performance standard genetic algorithms employing crossover mutation significantly decreases increasing problem size furthermore increasing k crossover based algorithms cases outperformed mutation based algorithms however relative performance differences algorithms grow significantly dimension search space indicating important consider highdimensional landscapes evaluating performance evolutionary algorithms
theories rational belief revision recently proposed gardenfors nebel illuminate many important issues impose unnecessarily strong standards correct revisions make strong assumptions information available guide revisions we reconstruct theories according economic standard rationality preferences used select among alternative possible revisions by permitting multiple partial specifications preferences ways closely related preferencebased nonmonotonic logics reconstructed theory employs information closer available practice offers flexible ways selecting revisions we formally compare notion rational belief revision gardenfors nebel adapt results universal default theories prove universal method rational belief revision examine formally different limitations rationality affect belief revision
independent component analysis ica statistical signal processing technique whose main applications blind source separation blind deconvolution feature extraction estimation ica usually performed optimizing contrast function based higherorder cumulants in paper shown almost error function used construct contrast function perform ica estimation in particular means one use contrast functions robust outliers as practical method tnding relevant extrema contrast functions txedpoint iteration scheme introduced the resulting algorithms quite simple converge fast reliably these algorithms also enable estimation independent components onebyone using simple deation scheme
we present methodology representing probabilistic relationships generalequilibrium economic model specifically define precise mapping bayesian network binary nodes market price system consumers producers trade uncertain propositions we demonstrate correspondence equilibrium prices goods economy probabilities represented bayesian network a computational market model may provide useful framework investigations belief aggregation distributed probabilistic inference resource allocation uncertainty problems de centralized uncertainty
this article appear volume journal applied statistical science adrian e raftery professor statistics sociology department statistics gn university washington seattle wa this research supported onr contract nj nih grant rhd ministere de la recherche et de lespace paris universite de paris vi inria rocquencourt france raftery thanks latter two institutions paul deheuvels gilles celeux hearty hospitality paris sabbatical article written this article prepared presentation conference applied change point analysis university marylandbaltimore march parts article review collaborative research others i would like express appreciation namely volkan akman jeff banfield nhu le steven lewis doug martin fionn murtagh ross taplin simon tavare
this paper describe functional architecture charade software platform devoted development new generation intelligent environmental decision support systems the charade platform based taskoriented approach system design exploitation new architecture problem solving integrates casebased reasoning constraint reasoning the platform developed objectoriented environment upon demonstrator developed managing first intervention attack forest fires
dimensions complexity raised definition system aimed supporting planning initial attack forest fires presented discussed the complexity deriving highly dynamic unpredictable domain forest fire one realated individuation integration planning techniques suitable domain complexity addressing problem taking account role user supported system finally complexity architecture able integrate different subsystems in particular focus severe constraints definition planning approach posed fire fighting domain constraints satisfied completely current planning paradigms we propose approach based integratation skeletal planning case based reasoning techniques constraint reasoning more specifically temporal constraints used two steps planning process plan fitting adaptation resource scheduling work development system software architecture ood methodology progress
we examine efficient implementation back prop type algorithms t vector processor fixed point engine designed neural network simulation a matrix formulation back prop matrix back prop shown efficient riscs using matrix back prop achieve asymptotically optimal performance t gops forward backward phases possible standard online method since high efficiency futile convergence poor due use fixed point arithmetic use mixture fixed floating point operations the key observation precision fixed point sufficient good convergence range appropriately chosen though expensive computations implemented fixed point achieve rate convergence comparable floating point version the time taken conversion fixed floating point also shown reasonable
we describe preliminary version investigative software gge generative genetic explorer genetic operations interact autocad generate novel d forms architect gge allows us asess evolutionary algorithms tailored suit architecture cad tasks
in paper discuss approach learning classification rules data we sketch two modules architecture namely linneo gar linneo knowledge acquisition tool illstructured domains automatically generating classes examples incrementally works unsupervised strategy linneo output representation conceptual structure domain terms classes input gar used generate set classification rules original training set gar generate conjunctive disjunctive rules herein present application techniques data obtained real wastewater treatment plant order help construction rule base this rule used knowledgebased system aims supervise whole process
when reading sentence the diplomat threw ball ballpark princess interpretation changes dance event baseball back dance such online disambiguation happens automatically appears based dynamically combining strengths association keywords two senses subsymbolic neural networks good modeling behavior they learn word meanings soft constraints interpretation dynamically combine constraints form likely interpretation on hand difficult show systematic language structures relative clauses could processed system the network would learn associate specific contexts would able process new combinations a closer look understanding embedded clauses shows humans systematic processing grammatical structures either for example the girl boy girl lived next door blamed hit cried difficult understand whereas the car man dog rabies bit drives garage this difference emerges semantic constraints work disambiguation task in chapter show subsymbolic parser combined highlevel control allows system process novel combinations relative clauses systematically still sensitive semantic constraints
we investigated generalization capabilities backpropagation learning feedforward recurrent feedforward connectionist networks assignment syllable boundaries orthographic representations dutch hyphenation this difficult task phonological morphological constraints interact leading ambiguity input patterns we compared results different symbolic pattern matching approaches exemplarbased generalization scheme related knearest neighbour approach using similarity metric weighed relative information entropy positions training patterns our results indicate generalization performance backpropagation learning task better best symbolic pattern matching approaches exemplarbased generalization
we present framework incorporating pruning strategies mtiling constructive neural network learning algorithm pruning involves elimination redundant elements connection weights neurons network considerable practical interest we describe three elementary sensitivity based strategies pruning neurons experimental results demonstrate moderate significant reduction network size without compromising networks generalization performance
a number neural learning rules recently proposed independent component analysis ica the rules usually derived informationtheoretic criteria maximum entropy minimum mutual information in paper show fact ica performed simple hebbian antihebbian learning rules may weak relations informationtheoretical quantities rather suprisingly practically nonlinear function used learning rule provided sign hebbianantihebbian term chosen correctly in addition hebbianlike mechanism weight vector constrained unit norm data preprocessed prewhitening sphering these results imply one choose nonlinearity optimize desired statistical numerical criteria
there currently several types constructive growth algorithms available training feedforward neural network this paper describes explains main ones using fundamental approach multilayer perceptron problemsolving mechanisms the claimed convergence properties algorithms verified using two mapping theorems consequently enables algorithms unified basic mechanism the algorithms compared contrasted deficiencies highlighted the fundamental reasons actual success algorithms extracted used suggest might fruitfully applied a suspicion panacea current neural network difficulties one must somewhere along line pay learning efficiency promise developed argument generalization abilities lie average backpropagation
prioritized sweeping modelbased reinforcement learning method attempts focus agents limited computational resources achieve good estimate value environment states to choose effectively spend costly planning step classic prioritized sweeping uses simple heuristic focus computation states likely largest errors in paper introduce generalized prioritized sweeping principled method generating estimates representationspecific manner this allows us extend prioritized sweeping beyond explicit statebased representation deal compact representations necessary dealing large state spaces we apply method generalized model approximators bayesian networks describe preliminary experiments compare approach classical prioritized sweeping
constructive learning algorithms offer approach incremental construction potentially nearminimal neural network architectures pattern classification tasks such algorithms help overcome need adhoc often inappropriate choice network topology use algorithms search suitable weight setting otherwise apriori fixed network architecture several algorithms proposed literature shown converge zero classification errors certain assumptions finite noncontradictory training set category classification problem this paper presents mtiling multicategory extension tiling algorithm mezard nadal we establish convergence mtiling zero classification error multicategory pattern classification task results experiments non linearly separable multicategory data sets demonstrate feasibility approach multicategory pattern classification also suggest several interesting directions future research
as real logic programmers normally use cut effective learning procedure logic programs able deal because cut predicate procedural meaning clauses containing cut learned using extensional evaluation method done learning systems on hand searching space possible programs instead space independent clauses unfeasible an alternative solution generate first candidate base program covers positive examples make consistent inserting cut appropriate the problem learning programs cut investigated seems natural reasonable approach we generalize scheme investigate difficulties arise some major shortcomings actually caused general need intensional evaluation as conclusion analysis paper suggests precise technical grounds learning cut difficult current induction techniques probably restricted purely declarative logic languages
we define gamma multilayer perceptron mlp mlp usual synaptic weights replaced gamma filters proposed de vries principe de vries principe associated gain terms throughout layers we derive gradient descent update equations apply model recognition speech phonemes we find inclusion gamma filters layers inclusion synaptic gains improves performance gamma mlp we compare gamma mlp tdnn backtsoi fir mlp backtsoi iir mlp architectures local approximation scheme we find gamma mlp results substantial reduction error rates
neural oneunit learning rules problem independent component analysis ica blind source separation introduced in new algorithms every ica neuron develops separator tnds one independent components the learning rules use simple constrained hebbianantihebbian learning decorrelating feedback may added to speed convergence stochastic gradient descent rules novel com putationally ecient txedpoint algorithm introduced
connectionist models collection forty papers representing wide variety research topics connectionism the book distinguished single feature papers almost exclusively contributions graduate students active field the students selected rigorous review process participated two week long summer school devoted connectionism as ambitious editors state foreword these bold claims true reader presented exciting opportunity sample frontiers connectionism their words imply two ways approach book the book must read random collection scientific papers also challenge evaluate controversial field this summer school actually third series previous ones held the proceedings summer school i priviledge participating reviewed nigel goddard continuing pattern fourth school scheduled held boulder co
a knowledgebased system uses database aka theory produce answers queries receives unfortunately answers may incorrect underlying theory faulty standard theory revision systems use given set labeled queries query paired correct answer transform given theory adding andor deleting either rules andor antecedents related theory accurate possible after formally defining theory revision task paper provides sample computational complexity bounds process it first specifies number labeled queries necessary identify revised theory whose error close minimal high probability it considers computational complexity finding best theory proves unless p n p polynomial time algorithm identify nearoptimal revision even given exact distribution queries except trivial situations it also shows except trivial situations polynomialtime algorithm produce theory whose error even close ie within particular polynomial factor optimal these results suggest reasons theory revision effective learning scratch also justify many aspects standard theory revision systems including practice hillclimbing locallyoptimal theory based given set labeled queries fl this paper extends short article appeared proceedings fourteenth international joint conference artificial intelligence ijcai montreal august i gratefully acknowledge receiving helpful comments edoardo amaldi mukesh dalal george drastal adam grove tom hancock sheila mcilraith roni khardon dan roth especially thorough comments anonymous referees
this paper investigates dynamic pathbased method constructing conjunctions new attributes decision tree learning it searches conditions attributevalue pairs paths form new attributes compared hypothesisdriven new attribute construction methods new idea method carries systematic search pruning path tree select conditions generating conjunction therefore conditions constructing new attributes dynamically decided search empirically evaluation set artificial realworld domains shows dynamic pathbased method improve performance selective decision tree learning terms higher prediction accuracy lower theory complexity in addition shows performance advantages fixed pathbased method fixed rulebased method learning decision trees
numerous recent papers focus standard recurrent nets problems long time lags relevant signals some propose rather sophisticated alternative methods we show many problems used test previous methods solved quickly random weight guessing
this paper contributes study nonlinear dynamical systems computational perspective these systems inherently powerful linear counterparts markov chains wide impact computer science seem likely play increasing role future however yet general techniques available handling computational aspects discrete nonlinear systems even simplest examples seem hard analyze we focus paper class quadratic systems widely used model population genetics also genetic algorithms these systems describe process random matings occur parental chromosomes via mechanism known crossover ie children inherit pieces genetic material different parents according random rule our results concern two fundamental quantitative properties crossover systems we develop general technique computing
evolution stochastic process operates dna species the evolutionary process leaves telltale signs dna used construct phylogenies evolutionary trees set species maximum likelihood estimations mle methods seek evolutionary tree likely produced dna consideration while methods widely accepted intellectually satisfying computationally intractable in paper address intractability mle methods follows we introduce metric evolutionary stochastic process show metric meaningful giving lowerbound learnability true phylogeny terms metric measure we complement result simple efficient algorithm inverting stochastic process evolution building tree observations dna species put another way show paclearn phylogenies though many heuristics suggested problem algorithm first algorithm guaranteed convergence rate rate within polynomial lowerbound rate establish our algorithm also first polynomial time algorithm guaranteed converge correct tree
much done develop learning techniques delayed reward problems worlds actions andor states approximated discrete representations although acceptable applications many situations approximation difficult unnatural for instance applications robotics real machines interact real world learning techniques use real valued continuous quantities required presented paper extension qlearning uses real valued states actions this achieved introducing activation strengths actuator system robot this allow actuators active continuous amount simultaneously learning occurs incrementally adapting expected future reward goal evaluation function gradients function respect actuator system
connections stochastic smoothingfiltering estimation incomplete data investigated it shown right censoring scheme kaplanmeier estimator characterized moment estimate based stochastic filtersmoothera pseudofiltersmoother motivated result potentially useful martingale approach estimation convergence incomplete data proposed estimators characterized pseudostochastic smoothers sometimes reduce filters described system stochastic integral equations recent results convergence stochastic integrals stochastic differential equations applied address convergence issues as illustration double censoring problem revisited framework closed form estimator proposed convergence properties studied martingale theory plays vital role entire analysis this approach essence selfconsistency method
over past years telecommunications paradigm shifting rapidly hardware middleware in particular traditional issues service characteristics network control replaced modern customerdriven issues network service management eg electronic commerce onestop shops an area service management extremely high visibility negative impact managed badly problem handling problem handling knowledge intensive activity particularly nowadays increase number complexity services becoming available trials several bt support centres already demonstrated potential casebased reasoning technology improving current practice problem detection diagnosis a major cost involved implementing casebased system manual building initial case base subsequent maintenance case base time this paper shows inductive machine learning combined casebased reasoning produce intelligent system capable extracting knowledge raw data automatically reasoning knowledge in addition discovering knowledge existing data repositories integrated system may used acquire revise knowledge continually experiments suggested integrated approach demonstrate promise justify next step
when using genetic programming gp algorithm difficult problem large set training cases large population size needed large number functiontree evaluations must carried this paper describes reduce number evaluations selecting small subset training data set actually carry gp algorithm three subset selection methods described paper dynamic subset selection dss using current gp run select difficult andor disused cases historical subset selection hss using previous gp runs random subset selection rss gp gpdss gphss gprss compared large classification problem gpdss produce better results less time taken gp gphss nearly match results gp perhaps surprisingly gprss occasionally approach results gp gp gpdss compared smaller problem hybrid dynamic fitness function dff based dss proposed
this paper presents limited error fitness lef modification standard supervised learning approach genetic programming gp individuals fitness score based many cases remain uncovered ordered training set individual exceeds error limit the training set order error limit altered dynamically response performance fittest individual previous generation
we describe experimental study pruning methods decision tree classifiers goal minimizing loss rather error in addition two common methods error minimization carts costcomplexity pruning cs errorbased pruning study extension costcomplexity pruning loss one pruning variant based laplace correction we perform empirical comparison methods evaluate respect loss we found applying laplace correction estimate probability distributions leaves beneficial pruning methods unlike error minimization somewhat surprisingly performing pruning led results par methods terms evaluation criteria the main advantage pruning reduction decision tree size sometimes factor ten while method dominated others datasets even domain different pruning mechanisms better different loss matrices
the fourier transform boolean functions come play important role proving many important learnability results we aim demonstrate fourier transform techniques also useful practical algorithm addition powerful theoretical tool we describe prominent changes introduced algorithm ones crucial without performance algorithm would severely deteriorate one benefits present confidence level prediction measures likelihood prediction correct
this paper looks use small populations genetic programming gp trend literature appears towards using large population possible requires memory resources cpuusage less efficient dynamic subset selection dss limited error fitness lef two different adaptive variations standard supervised learning method used gp this paper compares performance gp gpdss gplef case classification problem using small population size a similar comparison gp gpdss done larger messier case classification problem for problems gpdss small population size consistently produces better answer using fewer tree evaluations runs using much larger populations even standard gp seen perform well much smaller population size indicating certainly worth exploratory run three small population size assuming large population size necessary it interesting notion smaller mean faster better
one method detecting fraud check suspicious changes user behavior this paper describes automatic design user profiling methods purpose fraud detection using series data mining techniques specifically use rulelearning program uncover indicators fraudulent behavior large database customer transactions then indicators used create set monitors profile legitimate customer behavior indicate anomalies finally outputs monitors used features system learns combine evidence generate highconfidence alarms the system applied problem detecting cellular cloning fraud based database call records experiments indicate automatic approach performs better handcrafted methods detecting fraud furthermore approach adapt changing conditions typical fraud detection environments
given set samples probability distribution set discrete random variables study problem constructing good approximative neural network model underlying probability distribution our approach based unsupervised learning scheme samples first divided separate clusters cluster coded single vector these bayesian prototype vectors consist conditional probabilities representing attributevalue distribution inside corresponding cluster using prototype vectors possible model underlying joint probability distribution simple bayesian network tree realized feedforward neural network capable probabilistic reasoning in framework learning means choosing size prototype set partitioning samples corresponding clusters constructing cluster prototypes we describe prototypes determined given partition samples present method evaluating likelihood corresponding bayesian tree we also present greedy heuristic searching space different partition schemes different numbers clusters aiming optimal approximation probability distribution
this paper introduces two new crossover operators genetic programming gp contrary regular gp crossover operators presented attempt preserve context subtrees appeared parent trees a simple coordinate scheme nodes sexpression tree proposed crossovers allowed nodes exactly partially matching coordinates
hierarchical genetic programming hgp approaches rely discovery modification use new functions accelerate evolution this paper provides qualitative explanation improved behavior hgp based analysis evolution process dual perspective diversity causality from static point view use hgp approach enables manipulation population higher diversity programs higher diversity increases exploratory ability genetic search process demonstrated theoretical experimental fitness distributions expanded structural complexity individuals from dynamic point view report analyzes causality crossover operator causality relates changes structure object effect changes ie changes properties behavior object the analyses crossover causality suggests hgp discovers exploits useful structures bottomup hierarchical manner diversity causality complementary affecting exploration exploitation genetic search unlike machine learning techniques need extra machinery control tradeoff hgp automatically trades exploration exploitation
reinforcement learning rl algorithms provide sound theoretical basis building learning control architectures embedded agents unfortunately theory much practice see barto et al exception rl limited markovian decision processes mdps many realworld decision tasks however inherently nonmarkovian ie state environment incompletely known learning agent in paper consider partially observable mdps pomdps useful class nonmarkovian decision processes most previous approaches problems combined computationally expensive stateestimation techniques learning control this paper investigates learning pomdps without resorting form state estimation we present results td qlearning applied pomdps it shown conventional discounted rl framework inadequate deal pomdps finally develop new framework learning without stateestimation pomdps including stochastic policies search space defining value utility dis tribution states
the task monitor walking patterns give early warning falls using foot switch mercury trigger sensors we describe dynamic belief network model fall diagnosis given evidence sensor observations outputs beliefs current walking status makes predictions regarding future falls the model represents possible sensor error parametrised allow customisation individual monitored
i would like thank dissertation advisor roger schank valuable guidance research thank cognitive science reviewers helpful comments draft paper the research described conducted primarily yale university supported part defense advanced research projects agency monitored office naval research contract nk air force office scientific research contract fc
this paper describes two methods hierarchically organizing temporal behaviors the first intuitive grouping together common sequences events single units may treated individual behaviors this system immediately encounters problems however units binary meaning behaviors must execute completely hinders construction good training algorithms the system also runs difficulty one unit active time the second system hierarchy transition values this hierarchy dynamically modifies values specify degree one unit follow another these values continuous allowing use gradient descent learning furthermore many units active time part systems normal functionings
this paper introduces incremental selfimprovement paradigm unlike previous methods incremental selfimprovement encourages reinforcement learning system improve way learns improve way improves way learns without significant theoretical limitations system able shift inductive bias universal way its major features there explicit difference learning metalearning kinds information processing using turing machine equivalent programming language system occasionally executes selfdelimiting initially highly random selfmodification programs modify contextdependent probabilities future action sequences including future selfmodification programs the system keeps probability modifications computed useful selfmodification programs bring payoff reward reinforcement per time previous selfmodification programs the computation payoff per time takes account computation time required learning entire system life considered boundaries learning trials ignored a particular implementation based novel paradigm presented it designed exploit conventional digital machines good fast storage addressing arithmetic operations etc experiments illustrate systems mode operation keywords selfimprovement selfreference introspection machinelearning reinforcement learning note this revised extended version earlier report november
artificial neural networks ann due inherent parallelism potential fault tolerance offer attractive paradigm robust efficient implementations large modern database knowledge base systems this paper explores neural network model efficient implementation database query system the application proposed model highspeed library query system retrieval multiple items based partial match specified query criteria stored records the performance ann realization database query module analyzed compared techniques commonly current computer systems the results analysis suggest proposed ann design offers attractive approach realization query modules large database knowledge base systems especially retrieval based partial matches
in angluin showed class exhibiting combinatorial property called approximate fingerprints identified exactly using polynomially many equivalence queries polynomial size here show necessary condition every class without approximate fingerprints identification strategy makes polynomial number equivalence queries furthermore class honest technical sense computational power required strategy within polynomialtime hierarchy proving non learnability least hard showing p np
code scheduling exploit instruction level parallelism ilp critical problem compiler optimization research light increased use longinstructionword machines unfortunately optimum scheduling computationally intractable one must resort carefully crafted heuristics practice if scope application scheduling heuristic limited basic blocks considerable performance loss may incurred block boundaries to overcome obstacle basic blocks coalesced across branches form larger regions super blocks in literature regions typically scheduled using algorithms either oblivious profile information assumption process forming region fully utilized profile information use profile information addendum classical scheduling techniques we believe even simple case linear code regions super blocks additional performance improvement gained utilizing profile information scheduling well we propose general paradigm converting profileinsensitive list scheduler profilesensitive scheduler our technique developed via theoretical analysis simplified abstract model general problem profiledriven scheduling acyclic code region yielding scoring measure ranking branch instructions the ranking digests profile information useful property scheduling respect rank provably good minimizing expected completion time region within limits abstraction while ranking scheme computationally intractable general case practicable super blocks suggests heuristic present paper profiledriven scheduling super blocks experiments show heuristic offers substantial performance improvement prior methods range integer benchmarks several machine models
we propose extension genetic programming paradigm allows users traditional genetic algorithms evolve computer programs to end introduce mechanisms like transscription editing repairing genetic programming we demonstrate feasibility approach using develop programs prediction sequences integer numbers
generalized delta rule popularly known backpropagation bp probably one widely used procedures training multilayer feedforward networks sigmoid units despite reports success number interesting problems bp excruciatingly slow converging set weights meet desired error criterion several modifications improving learning speed proposed literature bp known suffer phenomenon flat spots the slowness bp direct consequence flatspots together formulation bp learning rule this paper proposes new approach minimizing error suggested mathematical properties conventional error function effectively handles flatspots occurring output layer the robustness proposed technique demonstrated number datasets widely studied machine learning community
an efficient retrieval relatively small number relevant cases huge case base crucial subtask casebased reasoning in article present case retrieval nets crns memory model recently developed task the main idea apply spreading activation process netlike case memory order retrieve cases similar posed query case we summarize basic ideas crns suggest useful extensions present initial experimental results suggest crns successfully handle case bases larger considered usually cbr community
this paper presents objectdirected case retrieval nets memory model developed application casebased reasoning task technical diagnosis the key idea store cases ie observed symptoms diagnoses network enhance network object model encoding knowledge devices application domain
alan e gelfand professor department statistics university connecticut storrs ct sujit k sahu lecturer school mathematics university wales cardiff cf yh uk the research first author supported part nsf grant dms second author supported part epsrc grant uk the authors thank brad carlin kate cowles gareth roberts anonymous referee valuable comments
technical report no department statistics university toronto abstract gaussian processes natural way defining prior distributions functions one input variables in simple nonparametric regression problem function gives mean gaussian distribution observed response gaussian process model easily implemented using matrix computations feasible datasets thousand cases hyperparameters define covariance function gaussian process sampled using markov chain methods regression models noise distribution logistic probit models classification applications implemented sampling well latent values underlying observations software available implements methods using covariance functions hierarchical parameterizations models defined way discover highlevel properties data inputs relevant predicting response
rule induction systems seek generate rule sets optimal complexity rule set this paper develops formal proof npcompleteness problem generating simplest rule set min rs accurately predicts examples training set particular type generalization algorithm algorithm complexity measure the proof informally extended cover broader spectrum complexity measures learning algorithms
many factory optimization problems inventory control scheduling reliability formulated continuoustime markov decision processes a primary goal problems find gainoptimal policy minimizes longrun average cost this paper describes new averagereward algorithm called smart finding gainoptimal policies continuous time semimarkov decision processes the paper presents detailed experimental study smart large unreliable production inventory problem smart outperforms two wellknown reliability heuristics industrial engineering a key feature study integration reinforcement learning algorithm directly two commercial discreteevent simulation packages arena csim paving way approach applied many factory optimization problems already exist simulation models
locally weighted polynomial regression lwpr popular instancebased algorithm learning continuous nonlinear mappings for two three inputs thousand datapoints computational expense predictions daunting we discuss drawbacks previous approaches dealing problem present new algorithm based multiresolution search quicklyconstructible augmented kdtree without needing rebuild tree make fast predictions arbitrary local weighting functions arbitrary kernel widths arbitrary queries the paper begins new faster algorithm exact lwpr predictions next introduce approximation achieves twoordersofmagnitude speedup negligible accuracy losses increasing certain approximation parameter achieves greater speedups still correspondingly larger accuracy degradation this nevertheless useful operations early stages model selection locating optima fitted surface we also show approximations permit realtime queryspecific optimization kernel width we conclude brief discussion potential extensions tractable instancebased learning datasets large fit com puters main memory
ordinal assertions evolutionary context form species similar species x species deduced distance matrix m interspecies dissimilarities m x lt m given species x ordinal binary character c xy m defined c xy m x lt ms species in paper present several results concerning inference evolutionary trees phylogenies ordinal assertions in particular present a sixpoint condition characterizes distance matrices whose ordinal binary characters pairwise compatible this characterization analogous fourpoint condition additive matrices an optimal on algorithm n number species recovering phylogeny realizes ordinal binary characters distance matrix satisfies sixpoint condition an npcompleteness result determining phylogeny realizes k ordinal binary characters given distance matrix
an xofn set containing one attributevalue pairs for given instance value corresponds number attributevalue pairs true in paper explore characteristics performance continuousvalued xofn attributes versus nominal xofn attributes constructive induction nominal xofns representationally powerful continuousvalued xofns former suffer fragmentation problem although mechanisms subsetting help solve problem two approaches constructive induction using continuousvalued xofns described continuousvalued xofns perform better nominal ones domains need xofns one cut point on domains need xofn representations one cut point nominal xofns perform better continuousvalued ones experimental results set artificial realworld domains support statements
this paper studies effects decision tree learning constructing four types attribute conjunctive disjunctive mofn xofn representations to reduce effects factors tree learning methods new attribute search strategies search starting points evaluation functions stopping criteria single tree learning algorithm developed with different option settings construct four different types new attribute factors fixed the study reveals conjunctive disjunctive representations similar performance terms prediction accuracy theory complexity variety concepts even dnf cnf concepts usually thought suited one two kinds representation in addition study demonstrates stronger representation power mofn conjunction disjunction stronger representation power xofn three types new attribute reflected performance decision tree learning terms higher prediction accuracy lower theory complexity
if analogy casebased reasoning systems scale large case bases important analyze various methods used retrieving analogues identify features problem appropriate this paper reports one analysis comparison retrieval marker passing spreading activation semantic network knowledgedirected spreading activation method developed wellsuited retrieving semantically distant analogues large knowledge base the analysis two complementary components theoretical model retrieval time based number problem characteristics experiments showing retrieval time approaches varies knowledge base size these two components taken together suggest kdsa likely sa able scale retrieval large knowledge bases
quence identification problems forms models depend absolute locations nucleotides assume independence consecutive nucleotide locations this paper describes new class learning methods called compressionbased induction cbi geared towards sequence learning problems arise learning dna sequences the central idea use text compression techniques dna sequences means generalizing sample sequences the resulting methods form models based important relative locations nucleotides dependence consecutive locations they also provide suitable framework biological domain knowledge injected learning process we present initial explorations range cbi methods demonstrate potential methods dna sequence identification tasks
our ability remember events situations daily life demonstrates ability rapidly acquire new memories there broad consensus hippocampal system hs plays critical role formation retrieval memories a computational model described demonstrates hs may rapidly transform transient pattern activity representing event situation persistent structural encoding via longterm potentiation longterm depression
we compare two techniques lighting control actual room equipped seven banks lights photoresistors detect lighting level four sensing points each bank lights independently set one sixteen intensity levels the task determine device intensity levels achieve particular configuration sensor readings one technique explored uses neural network approximate mapping sensor readings device intensity levels the technique examined uses conventional feedback control loop the neural network approach appears superior require experimentation fly hence fluctuating light intensity levels settling lengthy settling times deal complex interactions conventional control techniques handle well this comparison performed part adaptive house project described briefly further directions control
we provide sufficient condition convergence general class alternating estimationmaximization em type continuousparameter estimation algorithms respect given norm this class includes em penalized em greens oslem approximate em algorithms the convergence analysis extended include alternating coordinatemaximization em algorithms meng rubins ecm fessler heros sage the condition monotone convergence used establish norms distance successive iterates limit point emtype algorithm approaches zero monotonically for illustration apply results estimation poisson rate parameters emission tomography establish final iterations logarithm em iterates converge monotonically weighted euclidean norm
the kbann approach uses neural networks refine knowledge written form simple propositional rules we extend idea presenting manncon algorithm mathematical equations governing pid controller determine topology initial weights network trained using backpropagation we apply method task controlling outflow temperature water tank producing statisticallysignificant gains accuracy standard neural network approach nonlearning pid controller furthermore using pid knowledge initialize weights network produces statistically less variation testset accuracy compared networks initialized small random numbers
random field models image analysis spatial statistics usually local interactions they simulated markov chains update single site time the updating rules typically condition neighboring sites if want approximate expectation bounded function make better use simulations empirical estimator we describe symmetrizations empirical estimator computationally feasible lead considerable variance reduction the method reminiscent idea behind generalized von mises statistics to simplify exposition consider mainly nearest neighbor random fields gibbs sampler
intrator proposed feature extraction method related recent statistical theory huber friedman based biologically motivated model neuronal plasticity bienenstock et al this method recently applied feature extraction context recognizing d objects single d views intrator gold here describe experiments designed analyze nature extracted features relevance theory psychophysics object recognition
the buildingblock hypothesis appeals notion problem decomposition assembly solutions subsolutions accordingly many varieties ga test problems structure based buildingblocks many problems use deceptive fitness functions model interdependency bits within block however model interdependency buildingblocks consistent type interaction used intrablock interblock this paper discusses inadequacies various test problems literature clarifies concept buildingblock interdependency we formulate principled model hierarchical interdependency applied many levels consistent manner introduce hierarchical ifandonlyif hiff canonical example we present empirical results gas hiff showing population diversity maintained linkage tight ga able identify manipulate buildingblocks many levels assembly buildingblock hypothesis suggests
the music system multi signal processor system intelligent communication parallel distributed memory architecture based digital signal processors dsp a system processor elements operational it peak performance gflops electrical power consumption less w including forced air cooling fits rack two applications backpropagation algorithm neural net learning molecular dynamics simulations run times faster cray ymp times faster nec sx a sustained performance gflops reached the selling price system would range us
in paper report monte carlo study dynamics large untrained feedforward neural networks randomly chosen weights feedback the analysis consists looking percent systems exhibit chaos distrubution largest lyapunov exponents distrubution correlation dimensions as systems become complex increasing inputs neurons probability chaos approaches unity the correlation dimension typically much smaller system dimension
we introduce model analog computation discrete time presence analog noise flexible enough cover important concrete cases noisy analog neural nets networks spiking neurons this model subsumes classical model digital computation presence noise we show presence arbitrarily small amounts analog noise reduces power analog computational models finite automata also prove new type upper bound
in paper extend basic autologistic model include covariates indication sampling effort the model applied sampled data instead traditional use image analysis complete data available we adopt bayesian setup develop hybrid gibbs sampling estimation procedure using simulated examples show autologistic model covariates sample data improves predictions compared simple logistic regression model standard autologistic model without covariates
specifying constructing simulating structured connectionist networks requires significant programming effort system tools greatly reduce effort required providing conceptual structure within work make large complex network simulations possible the rochester connectionist simulator system tool designed aid specification construction simulation connectionist networks this report describes tool detail facilities provided use well details implementation through hope make designing verifying connectionist networks easier also encourage development refinement connectionist research tools
many stateoftheart ilp systems require large numbers negative examples avoid overgeneralization this considerable disadvantage many ilp applications namely indu ctive program synthesis relativelly small sparse example sets realistic scenario integrity constraints first order clauses play role negative examples inductive process one integrity constraint replace long list ground negative examples however checking consistency program set integrity constraints usually involves heavy oremproving we propose efficient constraint satisfaction algorithm applies wide variety useful integrity constraints uses monte carlo strategy it looks inconsistencies ra ndom generation queries program this method allows use integrity constraints instead together negative examples as consequence programs induce specified rapidly user ilp system tends obtain accurate definitions average running times greatly affected use integrity constraints compared ground negative examples
this paper develops evolutionary trade network game tng combines evolutionary game play endogenous partner selection successive generations resourceconstrained buyers sellers choose refuse trade partners basis continually updated expected payoffs trade partner selection takes place accordance modified galeshapley matching mechanism trades implemented using trade strategies evolved via standardly specified genetic algorithm the trade partnerships resulting matching mechanism shown core stable pareto optimal successive trade cycle nevertheless computer experiments suggest static optimality properties may inadequate measures optimality evolutionary perspective
a neural network method identifying ancestor hadron jet presented the idea find efficient mapping certain observed hadronic kinematical variables quarkgluon identity this done neuronic expansion terms network sigmoidal functions using gradient descent procedure errors backpropagated network with method able separate gluon quark jets originating monte carlo generated e e events accuracy the result independent mc model used this approach isolating gluon jet used study socalled string effect in addition heavy quarks b c e e reactions identified level observing hadrons in particular able separate bquarks efficiency purity comparable expected vertex detectors we also speculate neural network method used disentangle different hadronization schemes compressing dimensionality state space hadrons
selforganizing neural networks briefly reviewed compared supervised learning algorithms like backpropagation the power selforganization networks capability displaying typical features transparent manner this successfully demonstrated two applications hadronic jet physics hadronization model discrimination separation bc light quarks
the langevin updating rule noise added weights learning presented shown improve learning problems initially illconditioned hessians this particularly important multilayer perceptrons many hidden layers often illconditioned hessians in addition manhattan updating shown similar effect
the pac learning rectangles studied found experimentally yield excellent hypotheses several applied learning problems also pseudorandom sets rectangles actively studied recently subproblem common derandomization depth dnf circuits derandomizing randomized logspace ii approximate distribution n independent multivalued random variables we present improved upper bounds class problems approximating highdimensional rectangles arise pac learning pseudorandomness
a distinction two forms task knowledge transfer representational functional reviewed followed discussion mtl modified version multiple task learning mtl neural network method functional transfer the mtl method employs separate learning rate k task output node k k varies function measure relatedness r k kth task primary task interest an mtl network applied diagnostic domain four levels coronary artery disease results experiments demonstrate ability mtl develop predictive model one level disease superior diagnostic ability models produced either single task learning standard multiple task learning
this paper proposes genetic algorithms gas path planning trajectory planning autonomous mobile robot our gabased approach advantage adaptivity gas work even environment timevarying unknown therefore suitable offline online motion planning we first presents ga path planning d terrain simulation results performance adaptivity ga randomly generated terrains shown then discuss extensions ga solving path planning trajectory planning simultaneously
most work vapnikchervonenkis dimension neural networks focused feedforward networks however recurrent networks also widely used learning applications particular time relevant parameter this paper provides lower upper bounds vc dimension networks several types activation functions discussed including threshold polynomial piecewisepolynomial sigmoidal functions the bounds depend two independent parameters number w weights network length k input sequence in contrast feedforward networks vc dimension bounds expressed function w an important difference recurrent feedforward nets fixed recurrent net receive inputs arbitrary length therefore particularly interested case k w ignoring multiplicative constants main results say roughly following for architectures activation fixed nonlinear polynomial vc dimension wk for architectures activation fixed piecewise polynomial vc dimension wk w k for architectures activation h threshold nets vc dimension w logkw minfwk log wk w w log wkg for standard sigmoid x e x vc dimension wk w k an earlier version paper appeared proc rd european workshop computational learning theory lncs pages springer
the performance hillclimbing design optimization improved abstraction decomposition design space methods automatically finding exploiting abstractions decompositions presented paper a technique called operator importance analysis finds useful abstractions it determining given set operators important given class design problems hillclimbing search runs faster performed using smaller set operators a technique called operator interaction analysis finds useful decompositions it measuring pairwise interaction operators it uses measurements form ordered partition operator set this partition used hierarchic hillclimbing algorithm runs faster ordinary hillclimbing unstructured operator set we implemented techniques tested domain racing yacht hull design our experimental results show two methods produce substantial speedups little loss quality resulting designs
this paper investigates algorithm construction decisions trees comprised linear threshold units also presents novel algorithm learning nonlinearly separable boolean functions using madalinestyle networks isomorphic decision trees the construction networks discussed performance learning compared standard backpropagation sample problem many irrelevant attributes introduced littlestones winnow algorithm also explored within architecture means learning presence many irrelevant attributes the learning ability madalinestyle architecture nonoptimal larger necessary networks also explored
lipid research clinic program lipid research clinic program the lipid research clinics coronary primary prevention trial results parts i ii journal american medical association january pearl judea pearl aspects graphical models connected causality technical report rll cognitive systems laboratory ucla june submitted biometrika june short version proceedings th session international statistical institute invited papers flo rence italy august tome lv book pp
this paper investigates generation neural networks induction binary trees threshold logic units tlus initially describe framework tree construction algorithm show helps bridge gap pure connectionist neural network symbolic decision tree paradigms we also show trees threshold units induce transformed isomorphic neural network topology several methods learning linear discriminant functions node tree structure examined shown produce accuracy results comparable classical information theoretic methods constructing decision trees use single feature tests node produce trees smaller thus easier understand moreover results also show possible simultaneously learn topology weight settings neural network simply using training data set initially given
we consider problem learning dnf formulae mistakebound pac models we develop new approach called polynomial explainability shown useful learning new subclasses dnf cnf formulae known learnable unlike previous learnability results dnf cnf formulae subclasses limited number terms number variables per term yet contain subclasses kdnf ktermdnf corresponding classes cnf special cases we apply dnf results problem learning visual concepts obtain learning algorithms several natural subclasses visual concepts appear natural boolean counterpart on hand show learning natural subclasses visual concepts hard learning class dnf formulae we also consider robustness results various types noise
typical approaches plan recognition start representation agents possible plans reason evidentially observations agents actions assess plausibility various candidates a expansive view task consistent prior work accounts context plan generated mental state planning process agent consequences agents actions world we present general bayesian framework encompassing view focus context exploited plan recognition we demonstrate approach problem traffic monitoring objective induce plan driver observation vehicle movements starting model driver generates plans show highway context appropriately influence recognizers interpretation observed driver havior
i present parallel algorithm exact probabilistic inference bayesian networks for polytree networks n variables worstcase time complexity olog n crew pram concurrentread exclusivewrite parallel randomaccess machine n processors constant number evidence variables for arbitrary networks time complexity or w log n n processors ow log n r w n processors r maximum range variable w induced width maximum clique size moralizing trian gulating network
several theories inference decision employ sets probability distributions fundamental representation subjective belief this paper investigates frequentist connection empirical data convex sets probability distributions building earlier work walley fine framework advanced sequence random outcomes described drawn convex set distributions rather single distribution the extra generality detected observable characteristics outcome sequence the paper presents new asymptotic convergence results paralleling laws large numbers probability theory concludes comparison approach approaches based prior subjective constraints c fl carnegie mellon university
this reproduces report submitted rome laboratory october c flcopyright jon doyle all rights reserved freely available via httpwwwmedglcsmitedudoyle final report rational distributed reason maintenance abstract efficiency dictates plans largescale distributed activities revised incrementally parts plans revised expected utility identifying revising subplans improve expected utility using original plan the problems identifying reconsidering subplans affected changed circumstances goals closely related problems revising beliefs new changed information gained but traditional techniques reason maintenancethe standard method belief revisionchoose revisions arbitrarily enforce global notions consistency groundedness may mean reconsidering beliefs plan elements step we develop revision methods aiming revise beliefs plans worth revising tolerate incoherence ungroundedness judged less detrimental costly revision effort we use artificial market economy planning revision tasks arrive overall judgments worth present representation qualitative preferences permits capture common forms dominance information
a feedforward neural network method developed reconstructing invariant mass hadronic jets appearing calorimeter the approach illustrated w q q w bosons produced pp reactions sps collider energies the neural network method yields results superior conventional methods this neural network application differs classification ones sense analog number mass computed network rather binary decision made as byproduct application clearly demonstrates need using intelligent variables instances amount training instances limited
a new class highspeed selfadaptive massively parallel computing models called asocs adaptive selforganizing concurrent systems proposed current analysis suggests may problems implementing asocs models vlsi using hierarchical network structures originally proposed the problems inherent models rather technology used implement this led development new asocs model called dna discriminantnode asocs depend hierarchical node structure success three areas dna model briefly discussed paper dnas flexible nodes dna overcomes problems models allocating unused nodes dna operates processing learning
this paper presents approach mobile robot path planning using casebased reasoning together mapbased path planning the mapbased path planner used seed casebase innovative solutions the casebase stores paths information traversability while planning route paths preferred according former experience least risky
to successful open multiagent environments autonomous agents must capable adapting negotiation strategies tactics prevailing circumstances to end present empirical study showing relative success different strategies different types opponent different environments in particular adopt evolutionary approach strategies tactics correspond genetic material genetic algorithm we conduct series experiments determine successful strategies see strategies evolve depending context negotiation stance agents opponent
efficiency dictates plans largescale distributed activities revised incrementally parts plans revised expected utility identifying revising subplans improves expected utility using original plan the problems identifying reconsidering subplans affected changed circumstances goals closely related problems revising beliefs new changed information gained but traditional techniques reason maintenancethe standard method belief revisionchoose revisions arbitrarily enforce global notions consistency groundedness may mean reconsidering beliefs plan elements step to address problems developed revision methods aimed revising beliefs plans worth revising tolerating incoherence ungroundedness judged less detrimental costly revision effort artificial market economy planning revision tasks arriving overall judgments worth representation qualitative preferences permits capture common forms dominance information we view activities intelligent agents stemming interleaved simultaneous planning replanning execution observation subactivities in model plan construction process agents continually evaluate revise plans light happens world planning necessary organization largescale activities decisions actions taken future direct impact done shorter term but even wellconstructed value plan decays changing circumstances resources information objectives render original course action inappropriate when changes occur execution plan may necessary construct new plan starting scratch revising previous plan portions plan actually affected changes given information accrued plan execution remaining parts original plan salvaged ways parts changed incremental replanning first involves localizing potential changes conflicts identifying subset extant beliefs plans occur it involves choosing identified beliefs plans keep change for greatest efficiency choices portion plan revise revise based coherent expectations preferences among consequences different alternatives rational sense decision theory savage our work toward mechanizing rational planning replanning focussed four main issues this paper focusses latter three issues approach first see doyle replanning incremental local manner requires planning procedures routinely identify assumptions made planning connect plan elements assumptions replanning may seek change portions plan dependent upon assumptions brought question new information consequently problem revising plans account changed conditions much
in paper examine previous work naive bayesian classifier review limitations include sensitivity correlated features we respond problem embedding naive bayesian induction scheme within algorithm carries greedy search space features we hypothesize approach improve asymptotic accuracy domains involve correlated features without reducing rate learning ones we report experimental results six natural domains including comparisons decisiontree induction support hypotheses in closing discuss approaches extending naive bayesian classifiers outline directions future research
in paper present novel induction algorithm bayesian networks this selective bayesian network classifier selects subset attributes maximizes predictive accuracy prior network learning phase thereby learning bayesian networks bias small highpredictiveaccuracy networks we compare performance classifier selective nonselective naive bayesian classifiers we show selective bayesian network classifier performs significantly better versions naive bayesian classifier almost databases analyzed hence enhancement naive bayesian classifier relative nonselective bayesian network classifier selective bayesian network classifier generates networks computationally simpler evaluate display predictive accuracy comparable bayesian networks model features
we attempt recover unknown function noisy sampled data using orthonormal bases compactly supported wavelets develop nonlinear method works wavelet domain simple nonlinear shrinkage empirical wavelet coefficients the shrinkage tuned nearly minimax member wide range triebel besovtype smoothness constraints asymptotically minimax besov bodies p q linear estimates achieve even minimax rates triebel besov classes p lt method significantly outperform every linear method kernel smoothing spline sieve minimax sense variants method based simple threshold nonlinearities nearly minimax our method possesses interpretation spatial adaptivity reconstructs using kernel may vary shape bandwidth point point depending data least favorable distributions certain triebel besov scales generate objects sparse wavelet transforms many real objects similarly sparse transforms suggests minimax results relevant practical problems sequels paper discuss practical implementation spatial adaptation properties applications inverse problems acknowledgements this work completed first author leave uc berkeley research supported nsf dms nasa contract nca grant att foundation the second author supported part nsf grants dms nih phs grant gm supersedes earlier version titled wavelets optimal function estimation dated november issued technical reports departments statistics stanford uc berkeley
it established good software engineering practice ensure programs use memory via abstract data structures stacks queues lists these provide interface program memory freeing program memory management details left data structures implement the main result presented herein gp automatically generate stacks queues typically abstract data structures support multiple operations put get we show gp simultaneously evolve operations data structure implementing operation independent program tree that chromosome consists fixed number independent program trees moreover crossover mixes genetic material program trees implement operation program trees interact via shared memory shared automatically defined functions adfs
one main experimental tools probing interactions neurons measurement correlations activity in general however interpretation observed correlations difficult since correlation pair neurons influenced direct interaction also dynamic state entire network belong thus comparison observed correlations predictions specific model networks needed in paper develop theory neuronal correlation functions large networks comprising several highly connected subpopulations obeying stochastic dynamic rules when networks asynchronous states crosscorrelations relatively weak ie amplitude relative autocorrelations order n n size interacting populations using weakness crosscorrelations general equations express matrix crosscorrelations terms mean neuronal activities effective interaction matrix presented the effective interactions synaptic efficacies multiplied gain postsynaptic neurons the timedelayed crosscorrelation matrix expressed sum exponentially decaying modes correspond nonorthogonal eigenvectors effective interaction matrix the theory extended networks random connectivity randomly dilute networks this allows comparison contribution internal common input direct
technical report department statistics university washington sonia petrone assistant professor universita di pavia dipartimento di economia politica e metodi quantitativi i pavia italy adrian e raftery professor statistics sociology department statistics university washington box seattle wa this research supported onr grant nj grants murst rome
interference neural networks occurs learning one area input space causes unlearning another area networks less susceptible interference called spatially local networks these networks often used neurocontrol online applications real time nature task interference often problem although heuristics makes network local theoretical framework measuring localization this paper provides formal definition interference localization allow us measure networks local properties these definitions useful developing learning algorithms make networks local this may lead faster learning entire input domain
a considerable body evidence prosopagnosia deficit face recognition dissociable nonface object recognition indicates visual system devotes specialized functional area mechanisms appropriate face processing we present modular neural network composed two expert networks one mediating gate network task learning recognize faces individuals classifying nonface objects members one three classes while learning task network tends divide labor two expert modules one expert specializing face processing specializing nonface object processing after training observe networks performance test set one experts progressively damaged the results roughly agree data reported prosopagnosic patients damage face expert increases networks face recognition performance decreases dramatically object classification performance drops slowly we conclude datadriven competitive learning two unbiased functional units give rise localized face processing selective damage system could underlie prosopagnosia
a neural network model called lissom cooperative selforganization afferent lateral connections cortical maps applied modeling cortical plasticity after selforganization lissom maps dynamic equilibrium input reorganize like cortex response simulated cortical lesions intracortical microstimulation the model predicts adapting lateral interactions fundamental cortical reorganization suggests techniques hasten recovery following sensory cortical surgery
one way represent machine learning algorithms bias hypothesis instance space pair probability distributions this approach taken within bayesian learning schemes framework ulearnability however obvious inductive logic programming ilp system best provided probability distribution this paper extends results previous paper author introduced stochastic logic programs means providing structured definition probability distribution stochastic logic programs generalisation stochastic grammars a stochastic logic program consists set labelled clauses p c p interval c rangerestricted definite clause a stochastic logic program p distributional semantics one assigns probability distribution atoms predicate herbrand base clauses p these probabilities assigned atoms according sldresolution strategy employs stochastic selection rule it shown probabilities computed directly failfree logic programs normalisation arbitrary logic programs the stochastic proof strategy used provide three distinct functions method sampling herbrand base used provide selected targets example sets ilp experiments measure information content examples hypotheses used guide search ilp system simple method conditioning given stochastic logic program samples data functions used measure generality hypotheses ilp system progol this supports implementation bayesian technique learning positive examples fl this paper extension paper title appeared
a novel approach learning first order logic formulae positive negative examples incorporated system named icl inductive constraint logic in icl examples viewed interpretations true false target theory whereas present inductive logic programming systems examples true false ground facts clauses furthermore icl uses clausal representation corresponds conjunctive normal form conjunct forms constraint positive examples whereas classical learning techniques concentrated concept representations disjunctive normal form we present experiments new system mutagenesis problem these experiments illustrate differences systems indicate approach work least well classical approaches
in paper report result monte carlo study probability chaos large dynamical systems we use neural networks basis functions system dynamics choose parameter values networks randomly our results show dimension system complexity network increase probability chaotic dynamics increases since neural networks dense set dynamical systems conclusion large systems chaotic
automated synthesis analog electronic circuits recognized difficult problem genetic programming used evolve b h topology sizing n u e r c l v l u e f r e c h component circuit perform source identification correctly cl assify incoming signal categories
bell sejnowski derived blind signal processing algorithm nonlinear feedforward network information maximization viewpoint this paper first shows algorithm viewed maximum likelihood algorithm optimization linear generative model third paper gives partial proof folktheorem mixture sources highkurtosis histograms separable classic ica algorithm
i present expectationmaximization em algorithm principal component analysis pca the algorithm allows eigenvectors eigenvalues extracted large collections high dimensional data it computationally efficient space time it also naturally accommodates missing information i also introduce new variant pca called sensible principal component analysis spca defines proper density model data space learning spca also done em algorithm i report results synthetic real data showing em algorithms correctly efficiently find leading eigenvectors covariance datasets iterations using hundreds thousands datapoints thousands dimensions
we present new algorithms parameter estimation hmms by adapting framework used supervised learning construct iterative algorithms maximize likelihood observations also attempting stay close current estimated parameters we use bound relative entropy two hmms distance measure the result new iterative training algorithms similar em baumwelch algorithm training hmms the proposed algorithms composed step similar expectation step baumwelch new update parameters replaces maximization reestimation step the algorithm takes negligibly time per iteration approximated version uses expectation step baumwelch we evaluate experimentally new algorithms synthetic natural speech pronunciation data for sparse models ie models relatively small number nonzero parameters proposed algorithms require significantly fewer iterations
we investigate distribution performance boolean functions boolean inputs particularly parity functions alwayson even parity functions we us enumeration uniform montecarlo random sampling sampling random full trees as expected xor dramatically changes fitness distributions in cases minimum size threshold exceeded distribution performance approximately independent program length however distribution performance full trees different asymmetric trees varies tree depth we consider reject testing no free lunch nfl theorems functions
technical report bum biometrics unit cornell university abstract we analyse convergence stationarity simple nonreversible markov chain serves model several nonreversible markov chain sampling methods used practice our theoretical numerical results show nonreversibility indeed lead improvements diffusive behavior simple markov chain sampling schemes the analysis uses probabilistic techniques explicit diagonalisation we thank david aldous martin hildebrand brad mann laurent saloffcoste help
principal component analysis pca one popular techniques processing compressing visualising data although effectiveness limited global linearity while nonlinear variants pca proposed alternative paradigm capture data complexity combination local linear pca projections however conventional pca correspond probability density unique way combine pca models previous attempts formulate mixture models pca therefore extent ad hoc in paper pca formulated within maximumlikelihood framework based specific form gaussian latent variable model this leads welldefined mixture model probabilistic principal component analysers whose parameters determined using em algorithm we discuss advantages model context clustering density modelling local dimensionality reduction demonstrate application image compression handwritten digit recognition
programs independently evolved using fixed randomlygenerated fitness cases these programs subsequently tested large representative fixed population pursuers determine relative effectiveness this paper describes implementation original modified systems summarizes results tests
institute neural computation technical report series no inc january university california san diego la jolla ca abstract computational models neural map formation considered least three different levels abstraction detailed models including neural activity dynamics weight dynamics abstract neural activity dynamics adiabatic approximation objective functions weight dynamics may derived gradient flows in paper present example objective function derived detailed nonlinear neural dynamics a systematic investigation reveals different weight dynamics introduced previously derived objective functions generated prototypical terms this includes dynamic link matching special case neural map formation we focus particular role coordinate transformations derive different weight dynamics objective function coordinate transformations also important deriving normalization rules constraints several examples illustrate objective functions help understanding generating comparing different models neural map formation the techniques used analysis may also useful investigating types neural dynamics
realvalued random hidden variables useful modelling latent structure explains correlations among observed variables i propose simple unit adds zeromean gaussian noise input passing sigmoidal squashing function such units produce variety useful behaviors ranging deterministic binary stochastic continuous stochastic i show slice sampling neal used inference learning topdown networks units demonstrate learning two simple problems
there obvious need improving performance accuracy bayesian network new data observed because errors model construction changes dynamics domains afford ignore information new data while sequential update parameters fixed structure accomplished using standard techniques sequential update network structure still open problem in paper investigate sequential update bayesian networks parameters structure expected change we introduce new approach allows flexible manipulation tradeoff quality learned networks amount information maintained past observations we formally describe approach including necessary modifications scoring functions learning bayesian networks evaluate effectiveness empirical study extend case missing data
this paper sketches several aspects hypothetical cortical architecture visual object recognition based recent computational model the scheme relies modules learning examples hyperbflike networks basic components such models intended precise theories biological circuitry rather capture class explanations call memorybased models mbm contains sparse population coding memorybased recognition codebooks prototypes unlike sigmoidal units artificial neural networks units mbms consistent usual description cortical neurons tuned multidimensional optimal stimuli we describe example mbm may realized terms cortical circuitry biophysical mechanisms consistent psychophysical physiological data a number predictions testable physiological techniques made this memo describes research done within center biological computational learning department brain cognitive sciences artificial intelligence laboratory massachusetts institute technology this research sponsored grants office naval research contracts nj n grant national science foundation contract asc award includes funds arpa provided hpcc program additional support provided north atlantic treaty organization atr audio visual perception research laboratories mitsubishi electric corporation sumitomo metal industries siemens ag support ai laboratorys artificial intelligence research provided arpa contract nj tomaso poggio supported uncas helen whitaker chair mits whitaker college
we exploit qualitative probabilistic relationships among variables computing bounds conditional probability distributions interest bayesian networks using signs qualitative relationships implement abstraction operations guaranteed bound distributions interest desired direction by evaluating incrementally improved approximate networks algorithm obtains monotonically tightening bounds converge exact distributions for supermodular utility functions tightening bounds monotonically reduce set admissible decision alternatives well
a parallel algorithm proposed fundamental problem machine learning multicategory discrimination the algorithm based minimizing error function associated set highly structured linear inequalities these inequalities characterize piecewiselinear separation k sets maximum k affine functions the error function lipschitz continuous gradient allows use fast serial parallel unconstrained minimization algorithms a serial quasinewton algorithm considerably faster previous linear programming formulations a parallel gradient distribution algorithm used parallelize errorminimization problem preliminary computational results given decstation
this paper presents large systematic body data relative effectiveness mutation crossover combinations mutation crossover genetic programming gp the literature traditional genetic algorithms contains related studies mutation crossover gp differ traditional counterparts significant ways in paper present results large experimental data set equivalent approximately typical runs gp system systematically exploring range parameter settings the resulting data may useful practitioners seeking optimize parameters gp runs also theorists exploring issues role building blocks gp
technical report no department statistics university toronto markov chain monte carlo methods gibbs sampling simple forms metropolis algorithm typically move distribution sampled via random walk for complex highdimensional distributions commonly encountered bayesian inference statistical physics distance moved iteration algorithms usually small difficult impossible transform problem eliminate dependencies variables the inefficiency inherent taking small steps greatly exacerbated algorithm operates via random walk case moving point n steps away typically take around n iterations such random walks sometimes suppressed using overrelaxed variants gibbs sampling aka heatbath algorithm methods hitherto largely restricted problems full conditional distributions gaussian i present overrelaxed markov chain monte carlo algorithm based order statistics widely applicable in particular algorithm applied whenever full conditional distributions cumulative distribution functions inverse cumulative distribution functions efficiently computed the method demonstrated inference problem simple hierarchical bayesian model
this report reviews various optimum decision rules pattern recognition namely bayes rule chows rule optimum errorreject tradeoff recently proposed classselective rejection rule the latter provides optimum tradeoff error rate average number selected classes a new general relation error rate average number classes presented the error rate directly computed classselective reject function turn estimated unlabelled patterns simply counting rejects theoretical well practical implications discussed future research directions proposed
we utilize collective memory integrate weak strong search heuristics find cliques fc family graphs we construct fc pruning partial solutions ineffective each weak heuristic maintains local cache collective memory we examine impact distributed search various characteristics distribution collective memory search algorithms family graphs we find distributed search performs better individuals even though space partial solutions combinatorially explosive
in paper investigate integration knowledge acquisition machine learning techniques we argue existing machine learning techniques made useful knowledge acquisition tools allowing expert greater control interaction learning process we describe number extensions focl multistrategy hornclause learning program greatly enhanced power knowledge acquisition tool paying particular attention utility maintaining connection rule set examples explained rule the objective research make modification domain theory analogous use spread sheet a prototype knowledge acquisition tool focl constructed order evaluate strengths weaknesses approach
starting likelihood preference order worlds extend likelihood ordering sets worlds natural way examine resulting logic lewis earlier considered notion relative likelihood context studying counterfactuals assumed total preference order worlds complications arise examining partial orders present total orders there subtleties involving exact approach lifting order worlds order sets worlds in addition axiomatization logic relative likelihood case partial orders gives insight connection relative likelihood default reasoning
we empirically investigate properties search space behavior hillclimbing search solving hard random boolean satisfiability problems in experiments frequently observed rather attempting escape plateaus extensive search better completely restart new random initial state the optimum point terminate search restart determined empirically range problem sizes complexities the growth rate optimum cutoff faster linear number features although exact growth rate determined based empirical results simple runtime heuristic proposed determine give searching plateau restart this heuristic closely approximates empirically determined optimum values range problem sizes complexities consequently allows search algorithm automatically adjust strategy particular problem without prior knowledge problems complexity
the choice input representation neural network profound impact accuracy classifying novel instances however neural networks typically computationally expensive train making difficult test large numbers alternative representations this paper introduces fast quality measures neural network representations allowing one quickly accurately estimate collection possible representations problem best we show measures ranking representations accurate previously published measure based experiments three difficult realworld pattern recognition problems
further results on controllability properties of discretetime nonlinear systems fl abstract controllability questions discretetime nonlinear systems addressed paper in particular continue search conditions grouplike notion transitivity implies stronger semigrouplike property forward accessibility we show implication holds pointwise states weak poisson stability property globally exists global attractor system
we propose algorithm solving systems monotone equations combines newton proximal point projection methodologies an important property algorithm whole sequence iterates always globally convergent solution system without additional regularity assumptions moreover standard assumptions local superlinear rate convergence achieved as opposed classical globalization strategies newton methods computing stepsize use linesearch aimed decreasing value merit function instead linesearch approximate newton direction used construct appropriate hyperplane separates current iterate solution set this step followed projecting current iterate onto hyperplane ensures global convergence algorithm computational cost iteration method order classical damped newton method the crucial advantage method truly globally convergent in particular get trapped stationary point merit function the presented algorithm motivated hybrid projectionproximal point method proposed
this paper shows performance genetic programming system improved addition mechanisms nongenetic transmission information individuals culture teller previously shown genetic programming systems enhanced addition memory mechanisms individual programs teller paper show tellers memory mechanism changed allow communication individuals within across generations we show effects indexed memory culture performance genetic programming system symbolic regression problem kozas lawnmower problem wumpus world agent problems we show culture reduce computational effort required solve problems we conclude discussion possible improvements
this paper reviews large number cbr systems determine sort adaptation currently used three taxonomies proposed adaptationrelevant taxonomy cbr systems taxonomy tasks performed cbr systems taxonomy adaptation knowledge to extent set existing systems reflects constraints feasible review shows interesting dependencies different systemtypes tasks systems achieve adaptation needed meet system goals the cbr system designer may find partition cbr systems division adaptation knowledge suggested paper useful moreover paper may help focus initial stages systems development suggesting basis existing work types adaptation knowledge supported new system in addition paper provides framework preliminary evaluation comparison systems
constructive learning algorithms offer approach incremental construction nearminimal artificial neural networks pattern classification examples algorithms include tower pyramid upstart tiling algorithms construct multilayer networks threshold logic units multilayer perceptrons these algorithms differ terms topology networks construct turn biases search decision boundary correctly classifies training set this paper presents analysis algorithms geometrical perspective this analysis helps better characterization search bias employed different algorithms relation geometrical distribution examples training set simple experiments non linearly separable training sets support results mathematical analysis algorithms this suggests possibility designing efficient constructive algorithms dynamically choose among different biases build nearminimal networks pattern classification
d aldous p shields a diffusion limit class randomly growing binary trees probability theory r breathnach c benoist k ohare f gannon p chambon ovalbumin gene evidence leader sequence mrna dna sequences exonintron boundaries proceedings national academy science s brunak j engelbrecht s knudsen prediction human mrna donor acceptor sites dna sequence journal molecular biology jack cophen ian stewart the information hand the mathematical intelligencer r g gallager information theory reliable communication john wiley sons inc ali hariri bruce weber john olmstead on validity shannoninformation calculations molecular biological sequence journal theoretical biology w b davenport jr w l root an introduction theory random signals noise mcgrawhill andrzej knopka john owens complexity charts used map functional domains dna gene anal techn sm mount a catalogue splicejunction sequences nucleic acids research hm seidel dl pompliano jr knowles exons microgenes science september c e shannon a mathematical theory communication bell system tech j peter s shenkin batu erman lucy d mastrandrea informationtheoretical entropy measure sequence variability proteins r staden measurements effects coding protein dna sequence use finding genes nucleic acids research ja steitz snurps scientific american june h van trees detection estimation modulation theory wiley j d watson n h hopkins j w roberts j argetsinger steitz a m weiner molecular biology gene benjamincummings menlo park ca fourth edition ad wyner aj wyner an improved version lempelziv algorithm transactions information theory aj wyner string matching theorems applications data compression statistics phd thesis stanford university j ziv a lempel a universal algorithm sequential data compression ieee transactions information theory it
temporaldifference td learning used predict rewards commonly done reinforcement learning also predict states ie learn model worlds dynamics we present theory algorithms intermixing td models world different levels temporal abstraction within single structure such multiscale td models used modelbased reinforcementlearning architectures dynamic programming methods place conventional markov models this enables planning higher varied levels abstraction may prove useful formulating methods hierarchical multilevel planning reinforcement learning in paper treat prediction problemthat learning model value function case fixed agent behavior within context establish theoretical foundations multiscale models derive td algorithms learning two small computational experiments presented test illustrate theory this work extension generalization work singh dayan sutton pinette
this paper scientific comparison two code generation techniques identical goals generation best possible software pipelined code computers instruction level parallelism both variants modulo scheduling framework generation software pipelines pioneered rau glaser ragl otherwise quite dissimilar one technique developed silicon graphics used mipspro compiler this production compiler sgi systems based mips r processor hsu it essentially branchandbound enumeration possible schedules extensive pruning this method heuristic way prunes also interaction register allocation scheduling the second technique aims produce optimal results formulating scheduling register allocation problem integrated integer linear programming ilp problem this idea received much recent exposure literature algoga feautrier goalgaa goalgab eichenberger knowledge previous implementations preliminary detailed measurement evaluation in particular believe first published measurement runtime performance ilp based generation software pipelines a particularly valuable result study evaluation heuristic pipelining technology sgi compiler one motivations behind mcgill research hope optimal software pipelining practical use production compilers would useful evaluation validation our comparison indeed provided quantitative validation sgi compilers pipeliner leading us increased confidence techniques
paper bibtex entry available httpwwwcomplangtuwienacatpapers this paper published compiler construction cc springer lncs pages delayed exceptions speculative execution abstract superscalar processors execute basic blocks sequentially use much instruction level parallelism speculative execution proposed execute basic blocks parallel a pure software approach suffers low performance exceptiongenerating instructions executed speculatively we propose delayed exceptions combination hardware compiler extensions provide high performance correct exception handling compilerbased speculative execution delayed exceptions exploit fact exceptions rare the compiler assumes typical case exceptions schedules code accordingly inserts runtime checks fixup code ensure correct execution exceptions happen
fuzzy rules control effectively tuned via reinforcement learning reinforcement learning weak learning method requires information success failure control application the tuning process allows people generate fuzzy rules unable accurately perform control tuned rules provide smooth control this paper explores new simplified method using reinforcement learning tuning fuzzy control rules it shown learned fuzzy rules provide smoother control pole balancing domain another approach
procedural representations control policies two advantages facing scaleup problem learning tasks first implicit potential inductive generalization large set situations second facilitate modularization in paper compare several randomized algorithms learning modular procedural representations the main algorithm called adaptive representation learning arl genetic programming extension relies discovery subroutines arl suitable learning hierarchies subroutines constructing policies complex tasks arl successfully tested typical reinforcement learning problem controlling agent dynamic nondeterministic environment discovered subroutines correspond agent behaviors
we propose modification classical proximal point algorithm finding zeroes maximal monotone operator hilbert space in particular approximate proximal point iteration used construct hyperplane strictly separates current iterate solution set problem this step followed projection current iterate onto separating hyperplane all information required projection operation readily available end approximate proximal step therefore projection entails additional computational cost the new algorithm allows significant relaxation tolerance requirements imposed solution proximal point subproblems yields practical framework weak global convergence local linear rate convergence established suitable assumptions additionally presented analysis yields alternative proof convergence exact proximal point method allows nice geometric interpretation somewhat intuitive classical proof
we present resource spackling framework integrating register allocation instruction scheduling based measure reduce paradigm the technique measures resource requirements program uses measurements distribute code better resource allocation the technique applicable allocation different types resources a programs resource requirements register functional unit resources first measured using unified representation these measurements used find areas resources either utilized called resource holes excessive sets respectively conditions determined increasing resource utilization resource holes these conditions applicable local global code motion
we introduce new model distributions generated random walks graphs this model suggests variety learning problems using definitions models distribution learning defined our framework general enough model previously studied distribution learning problems well suggest new applications we describe special cases general problem investigate relative difficulty we present algorithms solve learning problem various conditions
a decision structure acyclic graph specifies order tests applied object situation arrive decision object serves simple powerful tool organizing decision process this paper proposes methodology learning decision structures oriented toward specific decision making situations the methodology consists two phases determining storing declarative rules describing decision process deriving online decision structure rules the first step performed expert aqbased inductive learning program learns decision rules examples decisions aq aq the second step transforms decision rules decision structure suitable given decision making situation the system aqdt implementing second step applied problem construction engineering in experiments aqdt outperformed programs applied problem terms accuracy simplicity generated decision structures key words machine learning inductive learning decision structures decision rules attribute selection
most constructive induction researchers focus new boolean attributes this paper reports new constructive induction algorithm called xofn constructs new nominal attributes form xofn representations an xofn set containing one attributevalue pairs for given instance value corresponds number attributevalue pairs true the promising preliminary experimental results artificial realworld domains show constructing new nominal attributes form xofn representations significantly improve performance selective induction terms higher prediction accuracy lower theory complexity
coevolution pursuit evasion ii simulation methods results fl abstract in previous sab paper presented scientific rationale simulating coevolution pursuit evasion strategies here present overview simulation methods results our notable results follows first coevolution works produce good pursuers good evaders pure bootstrapping process types rather specially adapted opponents current counterstrategies second eyes brains also coevolve within simulated species example pursuers usually evolved eyes front bodies like cheetahs evaders usually evolved eyes pointing sideways even backwards like gazelles third kinds coevolution promoted allowing spatially distributed populations gene duplication explicitly spatial morphogenesis program eyes brains allows bilateral symmetry the paper concludes discussing possible applications simulated pursuitevasion coevolu tion biology entertainment
learning store information extended time intervals via recurrent backpropagation takes long time mostly due insufficient decaying error back flow we briefly review hochreiters analysis problem address introducing novel efficient method called long shortterm memory lstm lstm learn bridge time lags excess steps enforcing constant error flow constant error carrousels within special units multiplicative gate units learn open close access constant error flow lstms update complexity per time step ow w number weights in experimental comparisons rtrl bptt recurrent cascadecorrelation elman nets neural sequence chunking lstm leads many successful runs learns much faster lstm also solves complex long time lag tasks never solved previous recurrent network algorithms it works local distributed realvalued noisy pattern representations
geometric separability generalisation linear separability familiar many minsky paperts analysis perceptron learning method the concept forms novel dimension along conceptualise learning methods the present paper shows geometric separability defined demonstrates accurately predicts performance least one empirical learning method
inspired visual motion detection model rabbit retina computational architecture used early audition barn owl designed chip employs correlation model report onedimensional field motion scene real time using subthreshold analog vlsi techniques fabricated successfully tested transistor chip using standard mosis process
in associative reinforcement learning environment generates input vectors learning system generates possible output vectors reinforcement function computes feedback signals inputoutput pairs the task discover remember inputoutput pairs generate rewards especially difficult cases occur rewards rare since expected time algorithm grow exponentially size problem nonetheless reinforcement function possesses regularities learning algorithm exploits learning time reduced nongeneralizing algorithms this paper describes neural network algorithm called complementary reinforcement backpropagation crbp reports simulation results problems designed offer differing opportunities generalization
we prove canonical distortion measure cdm optimal distance measure use nearestneighbour nn classification show reduces squared euclidean distance feature space function classes expressed linear combinations fixed set features paclike bounds given samplecomplexity required learn cdm an experiment presented neural network cdm learnt japanese ocr environ ment used nn classification
the schema theorem states implicit parallel search behind power genetic algorithm we contend chromosomes vote proportionate fitness candidate schemata we maintain population binary strings ternary schemata the string population works solving problem domain supplies fitness schema population indirectly solve original problem
the objective statistical data analysis describe behaviour system also propose construct check model observed processes bayesian methodology offers one possible approaches estimation unknown components model parameters functional components framework chosen model type however many instances evaluation bayes posterior distribution basal bayesian solutions difficult practically intractable even help numerical approximations in cases bayesian analysis may performed help intensive simulation techniques called markov chain monte carlo the present paper reviews best known approaches mcmc generation it deals several typical situations data analysis model construction mcmc methods successfully applied special attention devoted problem selection optimal regression model constructed regression splines functional units
rk belew j mcinerney n schraudolph evolving networks using genetic algorithm connectionist learning artificial life ii sfi studies science complexity cg langton c taylor jd farmer s rasmussen eds vol addisonwesley m mcinerney ap dhawan use genetic algorithms back propagation training feedforward neural networks ieee international conference neural networks vol pp fz brill de brown wn martin fast genetic selection features neural network classifiers ieee transactions neural networks vol pp f dellaert j vandewalle automatic design cellular neural networks means genetic algorithms finding feature detector the third ieee international workshop cellular neural networks their applications ieee new jersey pp de moriarty r miikkulainen efficient reinforcement learning symbiotic evolution machine learning vol pp l davis handbook genetic algorithms van nostrand reinhold new york d whitely the genitor algorithm selective pressure proceedings third interanational conference genetic algorithms jd schaffer ed morgan kauffman san mateo ca pp van camp d t plate ge hinton the xerion neural network simulator documentation department computer science university toronto toronto
an agent must learn act world trial error faces reinforcement learning problem quite different standard concept learning although good algorithms exist problem general case often quite inefficient exhibit generalization one strategy find restricted classes action policies learned efficiently this paper pursues strategy developing algorithms efficiently learn action maps expressible kdnf the algorithms compared existing methods empirical trials shown good performance
this note considers positive recurrent markov chains probability remaining current state arbitrarily close specifically conditions given ensure nonexistence central limit theorems ergodic averages functionals chain the results motivated applications metropolishastings algorithms constructed terms rejection probability rejection involves remaining current state two examples commonly used algorithms given independence sampler metropolis adjusted langevin algorithm the examples rather specialised although cases problems arise typical problems commonly occurring particular algorithm used i would like thank kerrie mengersen jeff rosenthal richard tweedie useful conversations subject paper
a reinforcement learning system limited computational resources interacts unrestricted unknown environment its goal maximize cumulative reward obtained throughout limited unknown lifetime system policy arbitrary modifiable algorithm mapping environmental inputs internal states outputs new internal states the problem realistic unknown environments policy modification process pmp occurring system life may unpredictable influence environmental states rewards pmps later time existing reinforcement learning algorithms properly deal neither naive exhaustive search among policy candidates even case small search spaces in fact reasonable way measuring performance improvements general typical situations missing i define measure based novel reinforcement acceleration criterion rac at given time rac satisfied beginning completed pmp computed currently valid policy modification followed longterm acceleration average reinforcement intake computation time later pmps taken account i present method called environmentindependent reinforcement acceleration eira guaranteed achieve rac eira neither care whether systems policy allows changing whether multiple interacting learning systems consequences sound theoretical framework metalearning success pmp recursively depends success later pmps setting stage a sound theoretical framework multiagent learning the principles implemented single system using assemblerlike programming language modify policy system consisting multiple agents agent fact connection fully recurrent reinforcement learning neural net a byproduct research general reinforcement learning algorithm nets preliminary experiments illustrate theory
evolutionary computation uses computational models evolution ary processes key elements design implementation computerbased problem solving systems in paper provide overview evolutionary computation describe several evolutionary algorithms currently interest important similarities di fferences noted lead discussion important issues need resolved items future research
there strong evidence face processing localized brain the double dissociation prosopagnosia face recognition deficit occurring brain damage visual object agnosia difficulty recognizing kinds complex objects indicates face nonface object recognition may served partially independent mechanisms brain is neural specialization innate learned we suggest specialization could result competitive learning mechanism development devotes neural resources tasks best performing further suggest specialization arises interaction task requirements developmental constraints in paper present feedforward computational model visual processing two modules compete classify input stimuli when one module receives low spatial frequency information receives high spatial frequency information task identify faces simply classifying objects low frequency network shows strong specialization faces no combination tasks inputs shows strong specialization we take results support idea innatelyspecified face processing module unnecessary
we present general method proving rigorous priori bounds number iterations required achieve convergence markov chain monte carlo we describe bounds specific models gibbs sampler obtained general method we discuss possibilities obtaining bounds generally
much recent research decision theoretic planning adopted markov decision processes mdps model choice attempted make solution tractable exploiting problem structure one particular algorithm structured policy construction achieves means decision theoretic analog goal regression using action descriptions based bayesian networks treestructured conditional probability tables the algorithm presented able deal actions correlated effects we describe new decision theoretic regression operator corrects weakness while conceptually straightforward extension requires somewhat complicated technical approach
the problem programming artificial ant follow santa fe trail repeatedly used benchmark problem recently shown performance several techniques much better best performance obtainable using uniform random search we suggested could program fitness landscape difficult hill climbers problem also difficult genetic algorithms contains multiple levels deception here redefine problem ant obliged traverse trail approximately correct order a simple genetic programming system size depth restriction show perform approximately three times better improved training function
in general machine learning process accelerated use heuristic knowledge problem solution for example monomorphic typed genetic programming gp uses type information reduce search space improve performance unfortunately monomorphic typed gp also loses generality untyped gp generated programs suitable inputs specified type polymorphic typed gp improves monomorphic untyped gp allowing type information expressed generic manner yet still imposes constraints search space this paper describes polymorphic gp system generate polymorphic programs programs take inputs one type produces outputs one type we also demonstrate operation generation map polymorphic program
although socalled naive bayesian classification makes unrealistic assumption values attributes example independent given class example learning method remarkably successful practice uniformly better learning method known boosting general method combining multiple classifiers due yoav freund rob schapire this paper shows boosting applied naive bayesian classifiers yields combination classifiers representationally equivalent standard feedforward multilayer perceptrons an ancillary result naive bayesian classification nonparametric nonlinear generalization logistic regression as training algorithm boosted naive bayesian learning quite different backpropagation definite advantages boosting requires linear time constant space hidden nodes learned incrementally starting important on realworld datasets method tried far generalization performance good better best published result using learning method unlike standard learning algorithms naive bayesian learning without boosting done logarithmic time linear number parallel computing units accordingly learning methods highly plausible computationally models animal learning other arguments suggest plausible behaviorally also
this paper addresses problem handling skewed class distributions within casebased learning cbl framework we first present baseline informationgainweighted cbl algorithm apply three data sets natural language processing nlp skewed class distributions although overall performance baseline cbl algorithm good show algorithm exhibits poor performance minority class instances we present two cbl algorithms designed improve performance minority class predictions each variation creates testcasespecific feature weights first observing path taken test case decision tree created learning task using pathspecific information gain values create appropriate weight vector use case retrieval when applied nlp data sets algorithms shown significantly increase accuracy minority class predictions maintaining improving overall classification accuracy
we introduce class iteratively decodable trellisconstrained codes generalization turbocodes lowdensity paritycheck codes seriallyconcatenated convolutional codes product codes in trellisconstrained code multiple trellises interact define allowed set codewords as result interactions minimumcomplexity single trellis code state space grows exponentially block length however turbocodes lowdensity paritycheck codes decoder approximate bitwise maximum posteriori decoding using sumproduct algorithm factor graph describes code we present two new families codes homogenous trellisconstrained codes ringconnected trellisconstrained codes give results show codes perform regime turbocodes lowdensity paritycheck codes
in paper present research identifying modeling strategies human tutors use integrating previous explanations current explanations we used work develop computational model partially implemented explanation facility existing tutoring system known sherlock we implementing system uses casebased reasoning identify previous situations explanations could potentially affect explanation constructed we identified heuristics constructing explanations exploit information ways similar observed when human tutors engage dialogue freely exploit aspects mutually known context including previous discourse utterances draw previous discourse seem awkward unnatural even incoherent previous discourse must taken account order relate new information effectively recently conveyed material avoid repeating old material would distract student new thus strategies using dialogue history generating explanations great importance research natural language generation tutorial applications the goal work produce computational model effects discourse context explanations instructional dialogues implement model intelligent tutoring system maintains dialogue history uses planning explanations based study humanhuman instructional dialogues developed taxonomy classifies types contextual effects occur data according explanatory functions serve in paper focus one important category taxonomy situations tutor explicitly refers previous explanation order point similarities differences material currently explained material presented earlier explanations we implementing system uses casebased reasoning identify previous situations explanations could potentially affect explanation constructed we identified heuristics constructing explanations exploit information ways similar observed instructional dialogues produced human tutors by building computer system capability optional facility enabled disabled able systematically evaluate hypothesis useful tutoring strategy in order test hypotheses effects previous discourse explanations building explanation component existing intelligent training system sherlock sherlock intelligent coached practice environment training avionics technicians troubleshoot complex electronic equipment using sherlock trainees solve problems minimal tutor interaction review troubleshooting behavior postproblem reflective followup session rfu tutor instructional dialogues produced human tutors
the rtrl algorithm fully recurrent continually running networks robinson fallside williams zipser requires on computations per time step n number noninput units i describe method suited online learning computes exactly gradient requires fixedsize storage order average time complexity per time step on
in many markov chain monte carlo problems target density function known normalization constant in paper take advantage knowledge facilitate convergence diagnostic markov sampler estimating l error kernel estimator firstly propose estimator normalization constant shown asymptotically normal mixing moment conditions secondly l error kernel estimator estimated using normalization constant estimator ratio estimated l error true l error shown converge probability similar conditions thirdly propose sequential plot estimated l error tool monitor convergence markov sampler finally dimensional bimodal example given illustrate proposal fl bin yu assistant professor department statistics university california berkeley ca research supported part junior faculty research grant university california berkeley grants daalg daahg army research office grant dms national science foundation the author grateful professors peter bickel andrew gelman many helpful discussions comments draft special thanks due mr sam buttrey help simulation professor per mykland mr karl broman commenting draft two anonymous two markov samplers compared example using proposed diagnostic plot
in recent years number different semantics defaults proposed preferential structures semantics possibilistic structures rankings shown characterized set axioms known klm properties kraus lehmann magidor while viewed surprise show almost inevitable we giving yet another semantics defaults uses plausibility measures new approach modeling uncertainty generalize approaches probability measures belief functions possibility measures we show earlier approaches default reasoning embedded framework plausibility we provide necessary sufficient condition plausibilities klm properties sound additional condition necessary sufficient klm properties complete these conditions easily seen hold earlier approaches thus explaining characterized klm properties
it commonplace artificial intelligence divide agents explicit beliefs two parts beliefs explicitly represented manifest memory implicitly represented constructive beliefs repeatedly reconstructed needed rather memorized many theories knowledge view relation manifest constructive beliefs logical relation manifest beliefs representing constructive beliefs logic belief this view however limits ability theory treat incomplete inconsistent sets beliefs useful ways we argue illuminating view belief result rational representation in theory agent obtains constructive beliefs using manifest beliefs preferences rationally sense decision theory choose useful conclusions indicated manifest beliefs
the economic theory rationality promises equal mathematical logic importance mechanization reasoning we survey growing literature basic notions probability utility rational choice coupled practical limitations information resources influence design analysis reasoning representation systems
in paper propose efficient reliable shotgun sequence assembly algorithm based fingerprinting scheme robust noise repetitive sequences data our algorithm uses exact matches short patterns randomly selected fragment data identify fragment overlaps construct overlap map finally deliver consensus sequence we show statistical clues made explicit approach easily exploited correctly assemble results even presence extensive repetitive sequences our approach exceptionally fast practice eg successfully assembled whole mycoplasma genitalium genome approximately kbps roughly minutes mb mhz pentium pro cpu time real shotgun data existing algorithms expected run several hours day data moreover experiments shotgun data synthetically prepared real dna sequences wide range organisms including human dna containing extensive repeating regions demonstrate algorithms robustness noise presence repetitive sequences for example correctly assembled kbp human dna sequence less minutes mb mhz pentium pro cpu time fl support research provided part office naval research grant n
at electronics lab swiss federal institute techology eth zurich high performance parallel supercomputer music multi processor system intelligent communication beed developed as applications neural network simulation molecular dynamics show electronics lab supercomputer absolutely par conventional supercomputers electric power requirements reduced factor wight reduced factor price reduced factor software development key using parallel system this report focus programming environment music system applications
the paper concerned integration constraint logic programming systems clp systems based genetic algorithms ga the resulting framework tailored applications require first phase number constraints need generated second phase optimal solution satisfying constraints produced the first phase carried clp second one ga we present specific framework ecl ps e ecrc common logic programming system genocop genetic algorithm numerical optimization constrained problems integrated framework called coco computational intelligence plus constraint logic programming the coco system applied training problem neural networks we consider constrained networks eg neural networks shared weights constraints weights example domain constraints hardware implementation etc then ecl ps e used generate chromosome representation together constraints ensure cases network specified exactly one chromosome thus problem becomes constrained optimization problem optimization criterion optimize error network genocop used find optimal solution note the work second author partially supported sion department nwo national foundation scientific research this work carried third author visiting cwi amsterdam fourth author visiting leiden university
in paper study global optimization methods like genetic algorithms used train neural networks we introduce notion regularity studying properties error function expand search space artificial way regularities used generate constraints weights network in order find satisfiable set constraints use constraint logic programming system then training network becomes constrained optimization problem we also relate notion regularity socalled network transformations
in paper study paclearning algorithms specialized classes deterministic finite automata dfa in particular study branching programs investigate influence width branching program difficulty learning problem we first present distributionfree algorithm learning width branching programs we also give algorithm proper learning width branching programs uniform distribution labeled samples we show existence efficient algorithm learning width branching programs would imply existence efficient algorithm learning dnf known case finally show existence algorithm learning width branching programs would also yield algorithm learning restricted version parity noise
the longest common subsequence problem examined point view parameterized computational complexity there several different ways parameters enter problem number sequences analyzed length common subsequence size alphabet lower bounds complexity basic problem imply lower bounds number sequence alignment consensus problems at issue theory parameterized complexity whether problem takes input x k solved time f k n ff ff independent k termed fixedparameter tractability it argued appropriate asymptotic model feasible computability problems small range parameter values covers important applications situation certainly holds many problems biological sequence analysis our main results show the longest common subsequence lcs parameterized number sequences analyzed hard w the lcs problem problem parameterized length common subsequence belongs w p hard w the lcs problem parameterized number sequences length common subsequence complete w all results obtained unrestricted alphabet sizes for alphabets fixed size problems fixedparameter tractable we conjecture remains hard
is universe computable if may much cheaper terms information requirements compute computable universes instead i apply basic concepts kolmogorov complexity theory set possible universes chat perceived true randomness life generalization learning given universe assumptions a long time ago great programmer wrote program runs possible universes his big computer possible means computable each universe evolves discrete time scale any universes state given time describable finite number bits one many universes despite evolved claim incomputable computable universes let t m denote arbitrary universal turing machine unidirectional output tape t m input output symbols comma t m possible input programs ordered alphabetically empty program etc let a k denote t m kth program list its output finite infinite string alphabet f g this sequence bitstrings separated commas interpreted evolution e k universe u k if e k includes least one comma let u l k represents u k state lth time step e k k l f g e k represented sequence u k corresponds u k big bang different algorithms may compute universe some universes finite whose programs cease producing outputs point others i dont know tm important the choice turing machine important this due compiler theorem universal turing machine c exists constant prefix c f g fl possible programs p cs output response program c p identical t m output response p the prefix c compiler compiles programs t m equivalent programs c k denote lth possibly empty bitstring lth comma u l
the metropolishastings algorithm estimating distribution based choosing candidate markov chain accepting rejecting moves candidate produce chain known invariant measure the traditional methods use candidates essentially unconnected based diffusions invariant develop onedimensional distributions class candidate distributions selftarget towards high density areas these produce metropolishastings algorithms convergence rates appear considerably better known traditional candidate choices random walk in particular wide classes choices may effectively help reduce burnin problem we illustrate behaviour examples exponential polynomial tails logistic regression model using gibbs sampling algorithm
copyright ieee published proceedings micro december research triangle park north carolina personal use material permitted however permission reprintrepublish material advertising promotional purposes creating new collective works resale redistribution servers lists reuse copyrighted component work works must obtained ieee contact manager copyrights permissions ieee service center hoes lane po box piscataway nj usa telephone intl
the application machine learning ml solve practical problems complex only recently due increased promise ml solving real problems experienced difficulty use issue started attract attention this difficulty arises complexity learning problems large variety available techniques in order understand complexity begin overcome important construct characterization learning situations building previous work dealt practical use ml set dimensions developed contrasted another recent proposal illustrated project development decisionsupport system marine propeller design the general research opportunities emerge development dimensions discussed leading toward working systems simple model presented setting priorities research selecting learning tasks within large projects central development concepts discussed paper use future projects recording successes limitations failures
we show dnf terms size approximated function od log non zero fourier coefficients expected error squared respect uniform distribution this property used derive learning algorithm dnf uniform distribution the learning algorithm uses queries learns respect uniform distribution dnf terms size time polynomial n od log the interesting implications case constant in case algorithm learns dnf polynomial number terms time n olog log n dnf terms size olog n log log n polynomial time
coins technical report december abstract multivariate decision trees overcome representational limitation univariate decision trees univariate decision trees restricted splits instance space orthogonal features axis this paper discusses following issues constructing multivariate decision trees representing multivariate test including symbolic numeric features learning coefficients multivariate test selecting features include test pruning multivariate decision trees we present new review wellknown methods forming multivariate decision trees the methods compared across variety learning tasks assess methods ability find concise accurate decision trees the results demonstrate multivariate methods effective others in addition experiments confirm allowing multivariate tests improves accuracy resulting decision tree univariate trees
technical report no april university washington department statistics seattle washington abstract kooperberg bose stone introduced polyclass methodology uses adaptively selected linear splines tensor products model conditional class probabilities the authors attempted develop methodology would work well small moderate size problems would scale large problems however version polyclass developed large problems impractical required two months cpu time apply large data set a modification methodology involving use stochastic gradient online method fitting polyclass models given sets basis functions developed makes methodology applicable large data sets in particular successfully applied phoneme recognition problem involving phonemes features cases training sample basis functions unknown parameters comparisons neural networks made original problem threevowel subproblem
the use externally imposed hierarchical structures reduce complexity learning control common however acknowledged learning hierarchical structure important step towards general learning many things required less bounded learning single thing specified learning presented paper reinforcement learning algorithm called nested qlearning generates hierarchical control structure reinforcement learning domains the emergent structure combined learned bottomup reactive reactions results reactive hierarchical control system effectively learned hierarchy decomposes would otherwise monolithic evaluation function many smaller evaluation functions recombined without loss previously learned information
we present online investment algorithm achieves almost wealth best constantrebalanced portfolio determined hindsight actual market outcomes the algorithm employs multiplicative update rule derived using framework introduced kivinen warmuth our algorithm simple implement requires constant storage computing time per stock trading period we tested performance algorithm real stock data new york stock exchange accumulated year period on data algorithm clearly outperforms best single stock well covers universal portfolio selection algorithm we also present results situation investor access additional side information
we consider belief revision operators satisfy alchourrongardenforsmakinson postulates present epistemic logic revision operator result revision described sentence logic in logic fact agents set beliefs represented sentence o o levesques know operator intuitively o read believed the fact agent believes represented sentence b read usual way believed the connective represents update defined katsuno mendelzon the revised beliefs represented sentence o b we show every revision operator satisfies agm postulates model epistemic logic beliefs implied sentence o b model correspond exactly sentences implied theory results revising this means reasoning changes agents beliefs reduces model checking certain epistemic sentences the negative result paper type formal account revision extended situation agent able reason beliefs a fully introspective agent use construction reason results revisions pain triviality
this paper introduces novel enhancement learning bayesian networks bias small highpredictiveaccuracy networks the new approach selects subset features maximizes predictive accuracy prior network learning phase we examine explicitly effects two aspects algorithm feature selection node ordering our approach generates networks computationally simpler evaluate display predictive accuracy comparable bayesian networks model attributes
while need hierarchies within control systems apparent also clear many researchers hierarchies learned learning structure component behaviors difficult task the benefit learning hierarchical structures behaviors decomposition control structure smaller transportable chunks allows previously learned knowledge applied new related tasks presented paper improvements nested qlearning nql allow realistic learning control hierarchies reinforcement environments also presented simulation simple robot performing series related tasks used compare hierarchical nonhierarchal learning techniques
recent findings suggest classification scheme based ensemble networks effective way address overfitting we study optimal methods training ensemble networks some recent experiments postal zipcode character data suggest weight decay may optimal method controlling variance classifier
bestfirst model merging general technique dynamically choosing structure neural related architecture avoiding overfitting it applicable learning recognition tasks often generalizes significantly better fixed structures we demonstrate approach applied tasks choosing radial basis functions function learning choosing local affine models curve constraint surface modelling choosing structure balltree bumptree maximize efficiency access
we describe algorithms estimating given measure known constant proportionality based large class diffusions extending langevin model invariant we show weak conditions one choose class way diffusions converge exponential rate one even ensure convergence independent starting point algorithm when convergence less exponential show often polynomial known rates we consider methods discretizing diffusion time find methods inherit convergence rates continuous time process these contrast behaviour naive euler discretization behave badly even simple cases
linsker reported development structured receptive fields simulations using hebbtype synaptic plasticity rule feedforward linear network the synapses develop dynamics determined matrix closely related covariance matrix input cell activities we analyse dynamics learning rule terms eigenvectors matrix these eigenvectors represent independently evolving weight structures some general theorems presented regarding properties eigenvectors eigenvalues for general covariance matrix four principal parameter regimes predicted we concentrate gaussian covariances layer b c linskers network analytic numerical solutions eigenvectors layer presented three eigenvectors dominate dynamics dc eigenvector synapses sign bilobed oriented eigenvector circularly symmetric centresurround eigenvector analysis circumstances vectors dominates yields explanation emergence centresurround structures symmetrybreaking bilobed structures criteria developed estimating boundary parameter regime centresurround structures emerge the application analysis linskers higher layers covariance functions oscillatory briefly discussed
this paper considers problem scaling proposal distribution multidimensional random walk metropolis algorithm order maximize efficiency algorithm the main result weak convergence result dimension sequence target densities n converges when proposal variance appropriately scaled according n sequence stochastic processes formed first component markov chain converge appropriate limiting langevin diffusion process the limiting diffusion approximation admits straightforward efficiency maximization problem resulting asymptotically optimal policy related asymptotic acceptance rate proposed moves algorithm the asymptotically optimal acceptance rate quite general conditions the main result proved case target density symmetric product form extensions result discussed
combinating reactivity planning proposed means compensating potentially slow response times planners still making progress toward long term goals the demands rapid response complexity many environments make difficult decompose tune coordinate reactive behaviors ensuring consistency neural networks address tuning problem less useful decomposition coordination we hypothesize interacting reactions decomposed separate behaviors resident separate networks interaction coordinated tuning mechanism higher level controller to explore issues implemented neural network architecture reactive component two layer control system simulated race car by varying architecture test whether decomposing reactivity separate behaviors leads superior overall performance coordination learning convergence
we introduce formal model teaching teacher tailored particular learner yet teaching protocol designed collusion possible not surprisingly model remedies nonintuitive aspects models teacher must successfully teach consistent learner we prove class exactly identified deterministic polynomialtime algorithm access rich set examplebased queries teachable computationally unbounded teacher polynomialtime learner in addition present general results relating model teaching various previous results we also consider problem designing teacherlearner pairs teacher learner polynomialtime algorithms describe teacherlearner pairs classes decision lists horn sentences
this paper explores algorithms automatic quantization realvalued datasets using thermometer codes pattern classification applications experimental results indicate relatively simple randomized thermometer code generation technique result quantized datasets used train simple perceptrons yield generalization test data substantially better obtained unquantized counterparts
automated search space candidate designs seems attractive way improve traditional engineering design process to make approach work however automated design system must include knowledge modeling limitations method used evaluate candidate designs also effective way use knowledge influence search process we suggest productive approach include knowledge implementing set model constraint functions measure much modeling assumptions violated influence search using values model constraint functions constraint inputs standard constrained nonlinear optimization numerical method we test idea domain conceptual design supersonic transport aircraft experiments indicate model constraint communication strategy decrease cost design space search one orders magnitude
the recognition objects hence descriptions must grounded environment terms sensor data we argue concepts used classify perceived objects used perform actions objects integrate actionoriented perceptual features perceptionoriented action features we present grounded symbolic representation concepts moreover concepts learned we show logicoriented approach learning grounded concepts
the curse dimensionality one severest problems concerning application rbf networks the number rbf nodes therefore number training examples needed grows exponentially intrinsic dimensionality input space one way address problem application feature selection data preprocessing step in paper propose twostep approach determination optimal feature subset first possible featuresubsets reduced best discrimination properties application fast robust filter technique eubafes secondly use wrapper approach judge preselected feature subsets leads rbf networks least complexity best classification accuracy experiments undertaken show improvement rbf networks feature selection approach
this paper reexamines problem parameter estimation bayesian networks missing values hidden variables perspective recent work online learning we provide unified framework parameter estimation encompasses online learning model continuously adapted new data cases arrive traditional batch learning preaccumulated set samples used onetime model selection process in batch case framework encompasses gradient projection algorithm em algorithm bayesian networks the framework also leads new online batch parameter update schemes including parameterized version em we provide empirical theoretical results indicating parameterized em allows faster convergence maximum likelihood parame ters standard em
many techniques speedup learning knowledge compilation focus learning optimization macrooperators control rules task domains characterized using problemspace search paradigm however characterization fit well class task domains problem solver required perform continuous manner for example many robotic domains problem solver required monitor realvalued perceptual inputs vary motor control parameters continuous online manner successfully accomplish task in domains discrete symbolic states operators difficult define to improve performance continuous problem domains problem solver must learn modify use continuous operators continuously map input sensory information appropriate control outputs additionally problem solver must learn contexts continuous operators applicable we propose learning method compile sensorimotor experiences continuous operators used improve performance problem solver the method speeds task performance well results improvements quality resulting solutions the method implemented robotic navigation system evaluated extensive experimen tation
discussions casebased reasoning often reflect implicit assumption case memory system become better informed ie increase knowledge cases added casebase this paper considers formalisations knowledge content necessary preliminary rigourous analysis performance casebased reasoning systems in particular interested modelling learning aspects casebased reasoning order study performance casebased reasoning system changes accumlates problemsolving experience the current paper presents casebase semantics generalises recent formalisations casebased classification within framework paper explores various issues assuring sematics welldefined illustrates knowledge content case memory system seen reside chosen similarity measure cases casebase
this paper proposes four performance measures genetic algorithm ga enable us compare different gas op timization problem different choices parameters values the performance measures defined terms observations simulation frequency optimal solutions fitness values frequency evolution leaps number generations needed reach optimal solution we present case study parameters ga robot path planning tuned performance optimized performance evaluation using measures especially one performance measures used demonstrate adaptivity ga robot path planning we also propose process systematic tuning based techniques design experiments
we propose analyze distribution learning algorithm subclass acyclic probabilistic finite automata apfa this subclass characterized certain distinguishability property automatas states though hardness results known learning distributions generated general apfas prove algorithm indeed efficiently learn distributions generated subclass apfas consider in particular show kldivergence distribution generated target source distribution generated hypothesis made small high confidence polynomial time we present two applications algorithm in first show model cursively written letters the resulting models part complete cursive handwriting recognition system in second application demonstrate apfas used build multiplepronunciation models spoken words we evaluate apfa based pronunciation models labeled speech data the good performance terms loglikelihood obtained test data achieved apfas incredibly small amount time needed learning suggests learning algorithm apfas might powerful alternative commonly used probabilistic models
this paper examines inductive inference complex grammar neural networks specifically task considered training network classify natural language sentences grammatical ungrammatical thereby exhibiting kind discriminatory power provided principles parameters linguistic framework governmentandbinding theory neural networks trained without division learned vs innate components assumed chomsky attempt produce judgments native speakers sharply grammaticalungrammatical data how recurrent neural network could possess linguistic capability properties various common recurrent neural network architectures discussed the problem exhibits training behavior often present smaller grammars training initially difficult however implementing several techniques aimed improving convergence gradient descent backpropagationthrough time training algorithm significant learning possible it found certain architectures better able learn appropriate grammar the operation networks training analyzed finally extraction rules form deterministic finite state automata investigated
we propose lazy neural tree lnt appropriate architecture realization smooth regression systems the lnt hybrid decision tree neural network from neural network inherits smoothness generated function incremental adaptability conceptual simplicity from decision tree inherits topology initial parameter setting well efficient sequential implementation outperforms traditional neural network simulations order magnitudes the enormous speed achieved lazy evaluation a speedup obtained application windowing scheme region interesting results restricted
to read handwritten digit string helpful segment image separate digits bottomup segmentation heuristics often fail neighboring digits overlap substantially we describe system stochastic generative model digit class show knowledge required segmentation the system uses gibbs sampling construct perceptual interpretation digit string segmentation arises naturally explaining away effects occur bayesian inference by using conditional mixtures factor analyzers possible extract explicit compact representation instantiation parameters describe pose digit these instantiation parameters used inputs higher level system models relationships digits the technique could used model individual digits redundancies instantiation parameters parts
this paper describes new method determining consensus sequences signal start translation boundaries exons introns donor acceptor sites eukaryotic mrna the method takes account dependencies adjacent bases contrast usual technique considering position independently when coupled dynamic program compute likely sequence new consensus sequences emerge the consensus sequence information summarized conditional probability matrices used locate signals uncharacterized genomic dna greater sensitivity specificity conventional matrices speciesspecific versions matrices especially effective distinguishing true false sites
in pantheon evolutionary forces optimizing apollonian powers natural selection generally assumed dominate dark dionysian dynamics sexual selection but need case particularly class selective mating mechanisms called directional mate preferences kirkpatrick in previous simulation research showed nondirectional assortative mating preferences could cause populations spontaneously split apart separate species todd miller in paper show directional mate preferences cause populations wander capriciously phenotype space strange form runaway sexual selection without influence natural selection pressures when directional mate preferences free evolve always evolve point direction naturalselective peaks sexual selection thus take life mate preferences within species become distinct important part environment species phenotypes adapt these results suggest broader conception adaptive behavior attracting potential mates becomes important finding food avoiding predators we present framework simulating wide range directional nondirectional mate preferences discuss practical scientific applications simu lating sexual selection
case based reasoning cbr uses knowledge former experiences known cases since special knowledge expert mainly subject experiences cbr techniques good base development expert systems we investigate problem technical diagnosis diagnosis considered classification task process guided computer assisted experience this corresponds flexible case completion approach flexibility also needed expert view predominant interest unexpected unpredictible cases
the paper investigates possibilities using simple recurrent networks transducers map sequential natural language input nonsequential featurebased semantics the networks perform well sentences containing single main predicate encoded transitive verbs prepositions applied multiplefeature objects encoded nounphrases adjectival modifiers shows robustness ungrammatical inputs a second set experiments deals sentences containing embedded structures here network able process multiple levels sentencefinal embeddings one level centerembedding this turns consequence networks inability retain information reflected outputs intermediate phases processing two extensions elmans original recurrent network architecture introduced
we use connectionist network trained reinforcement control autonomous robot vehicle simulated robot we show given appropriate sensory data architectural structure network learn control robot simple navigation problem we investigate complex goalbased problem examine planlike behavior emerges an autonomous agent abstractly defined mapping sequence sensory inputs appropriate action response percepts such agent autonomous extent behavior determined immediate inputs past experience rather builtin control russell wefald we interested investigating cognitive capabilities autonomous agents we believe cognitive behavior emerge reactive situated activity autonomous agents some consensus exists design autonomous agents there relatively direct coupling perception action control distributed decentralized importantly dynamic interaction environment agent maes however consensus exists best method implement design features connectionist networks easily accommodate design features believe effective mechanisms controlling autonomous agents this paper focuses exploring connectionist designs controlling simple navigation autonomous vehicle we conclude applying successful design features difficult problem examine planlike behavior emerges we examine implementation questions testing network controllers real simulated robot simple environment using experimental variables type sensory data type training subtasks amount memory to train network controllers use reinforcement learning algorithm converts abstract measures goodness reward punishment specific teacher signals the environment called playpen rectangular box fi feet light one corner the reinforcement training problems investigate involve coordination motor activity information supplied sensors the navigation problem refer avoid move reinforces robot moving playpen avoiding walls the difficult problem light food adds goal state order simulate hunger reinforces robot periodically seeking avoiding light still avoiding walls moving by holding problem constant varying sensory ability determine extent addition sensors helps robot succeed problem similarly evaluate utility contextual memory different training subtasks autoassociation prediction our robot called carbot modified toy car fi inches controlled programmable miniboard designed martin carbot inexpensive build primarily makes use primitive sensorsno lasers video sonar it two servomotors one controls forward backward motion steering the robot two types physical sensors digital touch sensors front back bumpers analog light sensors stalks near back the light sensors directed degrees side carbot carbot controlled remote connectionist network communicates miniboard the network gathers input data sensors determines set motors next time step figure shows standard network used experiments there four discrete sets input units three sensors one context memory the first set represents previous state two motors two units per motor the first motor unit represents spin direction rear motor determines direction motionforward backward the second unit designates state motor the third unit represents spin direction front motor determines direction turningleft right note order turn carbot must motors
in paper consider problem tracking subset domain called target changes gradually time a single unknown probability distribution domain used generate random examples learning algorithm measure speed target changes clearly rapidly target moves harder algorithm maintain good approximation target therefore evaluate algorithms based much movement target tolerated examples predicting accuracy furthermore complexity class h possible targets measured vcdimension also effects difficulty tracking target concept we show problem minimizing number disagreements sample among concepts class h approximated within factor k simple tracking algorithm h achieve probability making mistake target movement rate constant times kd k ln vapnikchervonenkis dimension h also show h properly paclearnable efficient randomized algorithm high probability approximately minimizes disagreements within factor yielding efficient tracking algorithm h tolerates drift rates constant times ln in addition prove complementary results classes halfspaces axisaligned hy perrectangles showing maximum rate drift algorithm even unlimited computational power tolerate constant times
practical pattern classification knowledge discovery problems require selection subset attributes features much larger set represent patterns classified this due fact performance classifier usually induced learning algorithm cost classification sensitive choice features used construct classifier exhaustive evaluation possible feature subsets usually infeasible practice large amount computational effort required genetic algorithms belong class randomized heuristic search techniques offer attractive approach find nearoptimal solutions optimization problems this paper presents approach feature subset selection using genetic algorithm some advantages approach include ability accommodate multiple criteria accuracy cost classification feature selection process find feature subsets perform well particular choices inductive learning algorithm used construct pattern classifier our experiments several benchmark realworld pattern classification problems demonstrate feasibility approach feature subset selection automated many practical pattern classification tasks eg medical diagnosis require learning appropriate classification function assigns given input pattern typically represented using vector attribute feature values one finite set classes the choice features attributes measurements used represent patterns presented classifier affect among things the accuracy classification function learned using inductive learning algorithm eg decision tree induction algorithm neural network learning algorithm the features used describe patterns implicitly define pattern language if language expressive enough would fail capture information necessary classification hence regardless learning algorithm used accuracy classification function learned would limited lack information design neural networks pattern classification knowledge discovery
the main aim paper provide tutorial regression gaussian processes we start bayesian linear regression show change viewpoint one see method gaussian process predictor based priors functions rather priors parameters this leads general discussion gaussian processes section section deals issues including hierarchical modelling setting parameters control gaussian process covariance functions neural network models use gaussian processes classification problems
general convergence results linear discriminant updates abstract the problem learning linear discriminant concepts solved various mistakedriven update procedures including winnow family algorithms wellknown perceptron algorithm in paper define general class quasiadditive algorithms includes perceptron winnow special cases we give single proof convergence covers much class including perceptron winnow also many novel algorithms our proof introduces generic measure progress seems capture much algorithms converge using measure develop simple general technique proving mistake bounds apply new algorithms well existing algorithms when applied known algorithms technique automatically produces close variants existing proofs generally obtain known bounds within constants thus showing certain sense seem ingly diverse results fundamentally isomorphic
in paper present prototype flexible similaritybased retrieval system its flexibility supported allowing imprecisely specified query moreover algorithm allows assessing retrieved items relevant initial context specified query the presented system used supporting tool software repository we also discuss system evaluation concerns usefulness scalability applicability comparability evaluation t a system three domains gives us encouraging results integration ta real software repository retrieval tool ongoing
this paper describes application inductive learning techniques casebased reasoning we introduce two main forms induction define casebased reasoning present combination the evaluation proposed system called ta carried classification task namely character recognition we show inductive knowledge improves knowledge representation turn flexibility system performance terms classification accuracy scalability
the aaai fall symposium flexible computation intelligent systems results issues opportunities nov cambridge ma abstract this paper presents casebased reasoning system ta we address flexibility casebased reasoning process namely flexible retrieval relevant experiences using novel similarity assessment theory to exemplify advantages approach experimentally evaluated system compared performance performance nonflexible version ta machine learning algorithms several domains
diagnosis disease treatment separate oneshot activities instead often dependent interleaved time mostly due uncertainty underlying disease uncertainty associated response patient treatment varying cost different treatment diagnostic investigative procedures the framework particularly suitable modeling complex therapy decision process partially observable markov decision process pomdp unfortunately problem finding optimal therapy within standard pomdp framework also computationally costly in paper investigate various structural extensions standard pomdp framework approximation methods allow us simplify model construction process larger therapy problems solve faster a therapy problem target specifically management patients ischemic heart disease
consider group bayesians subjective probability distribution set uncertain events an opinion pool derives single consensus distribution events representative group whole several pooling functions proposed sensible particular assumptions measures many researchers many years failed form consensus method best we propose marketbased pooling procedure analyze properties participants bet securities paying contingent uncertain event maximize expected utilities the consensus probability event defined corresponding securitys equilibrium price the market framework provides explicit monetary incentives participation honesty allows agents maintain individual rationality limited privacy no arbitrage arguments ensure equilibrium prices form legal probabilities we show events disjoint participants exponential utility money market derives result logarithmic opinion pool similarly logarithmic utility money yields linear opinion pool in cases prove groups behavior outside observer indistinguishable rational individual whose beliefs equal equilibrium prices
genetic programming increasing popularity basis wide range learning algorithms however technique date successfully applied modest tasks performance overheads evolving large number data structures many correspond valid program we address problem directly demonstrate evolutionary process achieved much greater efficiency use formallybased representation strong typing we report initial experimental results demonstrate technique exhibits significantly better performance previous work
the dna promoter sequences domain theory database become popular testing systems integrate empirical analytical learning this note reports simple change reinterpretation domain theory terms mofn concepts involving learning results accuracy items database moreover exhaustive search space mofn domain theory interpretations indicates expected accuracy randomly chosen interpretation maximum accuracy achieved cases this demonstrates informativeness domain theory without complications understanding interactions various learning algorithms theory in addition results help characterize difficulty learning using dna promoters theory
we discuss strategy polychotomous classification involves estimating class probabilities pair classes coupling estimates together the coupling model similar bradleyterry method paired comparisons we study nature class probability estimates arise examine performance procedure real simulated datasets classifiers used include linear discriminants nearest neighbors support vector machine
intracortical microstimulation icms single site somatosensory cortex rats monkeys hours produces large increase number neurons responsive skin region corresponding icmssite receptive field rf little effect position size icmssite rf response evoked icms site tactile stimulation recanzone et al b large changes rf topography observed following several weeks repetitive stimulation restricted skin region monkeys jenkins et al recanzone et al acde repetitive stimulation localized skin region monkeys produced training monkeys tactile frequency discrimination task improves performance recanzone et al it suggested changes rf topography caused competitive learning excitatory pathways grajski merzenich jenkins et al recanzone et al abcde icms almost simultaneously excites excitatory inhibitory terminals excitatory inhibitory cortical neurons within microns stimulating electrode thus paper investigates implications possibility lateral inhibitory pathways may undergo synaptic plasticity icms lateral inhibitory pathways may also undergo synaptic plasticity adult animals peripheral conditioning the exin afferent excitatory lateral inhibitory synaptic plasticity rules
in work present classification methodology linneo discover concepts illstructured domains organize hierarchies in order achieve aim linneo uses conceptual learning techniques classification the final target build knowledge bases expert validation some techniques improvement results classification step used like biasing using partial expert knowledge classification rules causal structural dependencies attributes delayed cluster assignation objects also comparisons wellknown systems shown
technical report january abstract the new classification algorithm clef combines version linear machine known machine nonlinear function approximator constructs features the algorithm finds nonlinear decision boundaries constructing features needed learn necessary discriminant functions the clef algorithm proven separate consistently labelled training instances even linearly separable input variables the algorithm illustrated variety tasks
in paper review five heuristic strategies handling contextsensitive features supervised machine learning examples we discuss two methods recovering lost implicit contextual information we mention evidence hybrid strategies synergetic effect we show work several machine learning researchers fits framework while claim strategies exhaust possibilities appears framework includes techniques found published literature contextsensitive learning
in paper describe new adaptive penalty approach handling constraints genetic algorithm optimization problems the idea start relatively small penalty coefficient increase decrease demand optimization progresses empirical results several engineering design domains demonstrate merit proposed approach
recent research decision theoretic planning focussed making solution markov decision processes mdps feasible we develop family algorithms structured reachability analysis mdps suitable initial state set states known using compact structured representations mdps eg bayesian networks methods vary tradeoff complexity accuracy produce structured descriptions estimated reachable states used eliminate variables variable values problem description reducing size mdp making easier solve one contribution work extension ideas graphplan deal distributed nature action representations typically embodied within bayes nets problem correlated action effects we also demonstrate algorithm made complete using kary constraints instead binary constraints another contribution illustration compact representation reachability constraints exploited several existing exact approximate abstraction algorithms mdps
many ilp systems golem foil mis take advantage user supplied metaknowledge restrict hypothesis space this metaknowledge form type information arguments predicate learned information whether certain argument predicate functionally dependent arguments supplied mode information this meta knowledge explicitly supplied ilp system addition data the present paper argues many cases meta knowledge extracted directly raw data three algorithms presented learn type mode symmetric metaknowledge data these algorithms incorporated existing ilp systems form preprocessor obviates need user explicitly provide information in many cases algorithms extract meta knowledge user either unaware information used ilp system restrict hypothesis space
gold showed even regular grammars exactly identified positive examples alone since known children learn natural grammars almost exclusively positives examples golds result used theoretical support chomskys theory innate human linguistic abilities in paper new results presented show within bayesian framework grammars also logic programs learnable arbitrarily low expected error positive examples in addition show upper bound expected error learner maximises bayes posterior probability learning positive examples within small additive term one mixture positive negative examples an inductive logic programming implementation described avoids pitfalls greedy search global optimisation function local construction individual clauses hypothesis results testing implementation artificiallygenerated datasets reported these results agreement theoretical predictions
p n we prove two results estimator smooth with high probability f fl n least smooth f wide variety smoothness measures adapt the estimator comes nearly close mean square f measurable estimator come uniformly balls two broad scales smoothness classes these two properties unprecedented several ways our proof results develops new facts abstract statistical inference connection acknowledgements these results described symposium wavelet theory held connection shanks lectures vanderbilt university april the author would like thank professor ll schumaker hospitality conference ra devore iain johnstone gerard kerkyacharian bradley lucier as nemirovskii ingram olkin dominique picard interesting discussions correspondence related topics the author also university california berkeley
in paper study sequence evolution general markov model incorporates practically every stochastic model found literature in particular study error estimation evolutionary distances dependence sample sequence lengths by deriving large deviation results applicable distancebased evolutionary tree building algorithm show harmonic greedy triplets short quartet algorithms recover evolutionary tree high probability sequences polynomial length number nodes
results reported application tools synthesizing optimizing analyzing neural networks ecg patient monitoring task a neural network synthesized rulebased classifier optimized set normal abnormal heartbeats the classification error rate separate larger test set reduced factor sensitivity analysis synthesized optimized networks revealed informative differences analysis weights unit activations optimized network enabled reduction size network factor without loss accuracy
intracortical microstimulation icms localized site somatosensory cortex rats monkeys hours produces large increase cortical representation skin region represented icmssite neurons icms little effect icmssite neurons rf location rf size responsiveness recanzone et al the exin afferent excitatory lateral inhibitory learning rules marshall used model rf changes icms the exin model produces reorganization rf topography similar observed experimentally the possible role inhibitory learning producing effects icms studied simulating exin model lateral inhibitory learning the model also produces increase cortical representation skin region represented icmssite rf icms compared artificial scotoma conditioning pettet gilbert retinal lesions dariansmith gilbert suggested lateral inhibitory learning may general principle cortical plasticity
we present detailed analysis evolution gp populations using problem finding program returns maximum possible value given terminal function set depth limit program tree known max problem we confirm basic message gathercole ross crossover together program size restrictions responsible premature convergence suboptimal solution we show happen even population retains high level variety show many cases evolution suboptimal solution solution possible sufficient time allowed in cases theoretical models presented compared actual runs experimental evidence presented prices covariance selection theorem applied gp populations practical effect program size restrictions noted finally show covariance gene frequency fitness first generations used predict course gp runs
we present symbolic machinery admits probabilistic causal information given domain produces probabilistic statements effect actions impact observations the calculus admits two types conditioning operators ordinary bayes conditioning p yjx x represents observation x x causal conditioning p yjdox x read probability y conditioned holding x constant x deliberate action given mixture observational causal sentences together topology causal graph calculus derives new conditional probabilities types thus enabling one quantify effects actions observations
a general model coevolution cooperating species presented this model instantiated tested domain function optimization compared traditional gabased function optimizer the results encouraging two respects they suggest ways performance ga eabased optimizers improved suggest new approach evolving complex structures neural networks rule sets
this paper investigates learning lifelong context lifelong learning addresses situations learner faces whole stream learning tasks such scenarios provide opportunity transfer knowledge across multiple learning tasks order generalize accurately less training data in paper several different approaches lifelong learning described applied object recognition domain it shown across board lifelong learning approaches generalize consistently accurately less training data ability transfer knowledge across learning tasks
a constant rebalanced portfolio investment strategy keeps distribution wealth among set stocks period period recently work online investment strategies competitive best constant rebalanced portfolio determined hindsight cover helmbold et al cover ordentlich cover ordentlich b ordentlich cover cover for universal algorithm cover cover provide simple analysis naturally extends case fixed percentage transaction cost commission answering question raised cover helmbold et al cover ordentlich cover ordentlich b ordentlich cover cover in addition present simple randomized implementation significantly faster practice we conclude explaining algorithms applied problems combining predictions statistical language models resulting guarantees striking
laiwan chan evan fungyu young computer science department the chinese university hong kong new territories hong kong email lwchancscuhkhk technical report cstr abstract the fully connected recurrent network frn using online training method real time recurrent learning rtrl computationally expensive it computational complexity on storage complexity on n number noninput units we devised locally connected recurrent model much lower complexity computational time storage space the ringstructure recurrent network rrn simplest kind locally connected corresponding complexity omnnp onp respectively p n number input hidden output units respectively we compare performance rrn frn sequence recognition time series prediction we tested networks ability temporal memorizing power time warpping ability sequence recognition task in time series prediction task used networks train predict three series periodic series white noise deterministic chaotic series sunspots data both tasks show rrn needs much shorter training time performance rrn comparable frn
in object recognition systems interactions objects scene ignored best interpretation considered set hypothesized objects matches greatest number image features we show image interpretation cast problem finding probable explanation mpe bayesian network models visual physical object interactions the problem determine exact conditional probabilities network shown unimportant since goal find probable configuration objects calculate absolute probabilities we furthermore show evaluating configurations feature counting equivalent calculating joint probability configuration using restricted bayesian network derive assumptions probabilities necessary make bayesian formulation reasonable
research nonmonotonic default reasoning identified several important criteria preferring alternative default inferences the theories reasoning based criteria may uniformly viewed theories rational inference reasoner selects maximally preferred states belief though researchers noted cases apparent conflict preferences supported different theories hoped special theories reasoning may combined universal logic nonmonotonic reasoning we show different categories preferences conflict realized adapt formal results social choice theory prove every universal theory default reasoning violate least one reasonable principle rational reasoning our results interpreted demonstrating within preferential framework expect much improvement rigid lexicographic priority mechanisms proposed conflict resolution
we apply exponential weight algorithm introduced littlestone warmuth vovk problem predicting binary sequence almost well best biased coin we first show case logarithmic loss derived algorithm equivalent bayes algorithm jeffreys prior studied xie barron probabilistic assumptions we derive uniform bound regret holds sequence we also show empirical distribution sequence bounded away length sequence increases infinity difference bound corresponding bound average case regret algorithm asymptotically optimal case we show gap necessary calculating regret minmax optimal algorithm problem showing asymptotic upper bound tight we also study application algorithm square loss show algorithm derived case different bayes algorithm better prediction worstcase
we study close connections game theory online prediction boosting after brief review game theory describe algorithm learning play repeated games based online prediction methods littlestone warmuth the analysis algorithm yields simple proof von neumanns famous minmax theorem well provable method approximately solving game we show online prediction model obtained applying gameplaying algorithm appropriate choice game boosting obtained applying algorithm dual game
when compiling instruction level parallelism ilp integration optimization phases lead improvement quality code generated however since several different representations program used various phases partial integration achieved date we present program representation combines resource requirements availability information control data dependence information the representation enables integration several optimizing phases including transformations register allocation instruction scheduling the basis integration simultaneous allocation different types resources we define representation show constructed we formulate several optimization phases use representation achieve better integration
many problems impede design multiagent systems least passing information agents while others hand implement communication routes semantics explore method communication evolve in experiments described model agents connectionist networks we supply agent number communications channels implemented addition input output units channel the output units initiate environmental signals whose amplitude decay distance perturbed environmental noise an agent receive input individuals rather agents input reects summation agents output signals along channel because use realvalued activations agents communicate using realvalued vectors under evolutionary program gnarl agents coevolve communication scheme continuous channels conveys taskspe cific information
as field genetic programming gp matures breadth application increases need parallel implementations becomes absolutely necessary the transputerbased system presented koza andre one rare parallel implementations until today implementation proposed parallel gp using simd architecture except dataparallel approach tufts although others exploited workstation farms pipelined supercomputers one reason certainly apparent difficulty dealing parallel evaluation different sexpressions single instruction executed time every processor the aim chapter present implementation parallel gp simd system processor efficiently evaluate different sexpression we implemented approach maspar mp computer present timing results to extent simd machines like maspar available offer costeffective cycles scientific experimentation useful approach the idea simulating mimd machine using simd architecture new hillis steele littman metcalf dietz cohen one original ideas connection machine hillis steele could simulate parallel architectures indeed extreme processor simd architecture simulate universal turing machine tm with different turing machine specifications stored local memory processor would simply tape tape head state table state pointer simulation would performed repeating basic tm operations simultaneously of course simulation would inefficient difficult program would advantage really mimd simd processor would idle state simulated machine halts now let us consider alternative idea simd processor would simulate individual stored program computer using simple instruction set for step simulation simd system would sequentially execute possible instruction subset processors whose next instruction match for typical assembly language even reduced instruction set processors would idle time however set instructions implemented virtual processor small approach fruitful in case genetic programming instruction set composed specified set functions designed task we show precompilation step simply adding push conditional unconditional branching stop instruction get effective mimd simulation running
the current tracedriven simulation approach determine superscalar processor performance widely used shortcomings modern benchmarks generate extremely long traces resulting problems data storage well long simulation run times more fundamentally simulation generally provide significant insight factors determine performance characterization interactions this paper proposes theoretical model superscalar processor performance addresses shortcomings performance viewed interaction program parallelism machine parallelism both program machine parallelisms decomposed multiple component functions methods measuring computing functions described the functions combined provide model interaction program machine parallelisms accurate estimate performance the computed performance based model compared simulated performance six benchmarks spec suite several configurations ibm rs instruction set architecture
artificial neural networks applied prediction splice site location human premrna a joint prediction scheme prediction transition regions introns exons regulates cutoff level splice site assignment able predict splice site locations confidence levels far better previously reported literature the problem predicting donor acceptor sites human genes hampered presence numerous amounts false positives paper distribution false splice sites examined linked possible scenario splicing mechanism vivo when presented method detects true donor acceptor sites makes less false donor site assignments less false acceptor site assignments for large data set used study means average one half false donor sites per true donor site six false acceptor sites per true acceptor site with joint assignment method fifth true donor sites around one fourth true acceptor sites could detected without accompaniment false positive predictions highly confident splice sites could isolated widely used weight matrix method separate splice site networks a complementary relation confidence levels codingnoncoding separate splice site networks observed many weak splice sites sharp transitions codingnoncoding signal many stronger splice sites illdefined transitions coding noncoding
to coordinate agents environment agent needs models agents trying when communication impossible expensive information must acquired indirectly via plan recognition typical approaches plan recognition start specification possible plans agents may following develop special techniques discriminating among possibilities perhaps desirable would uniform procedure mapping plans general structures supporting inference based uncertain incomplete observations in paper describe set methods converting plans represented flexible procedural language observation models represented proba bilistic belief networks
dimacs technical report dimacs partnership rutgers university princeton university att research bellcore bell laboratories dimacs nsf science technology center funded contract stc also receives support new jersey commission science technology
the construction evolutionary trees fundamental problem biology yet methods reconstructing evolutionary trees reliable comes inferring accurate topologies large divergent evolutionary trees realistic length sequences we address problem present new polynomial time algorithm reconstructing evolutionary trees called short quartets method consistent greater statistical power polynomial time methods neighborjoining approximation algorithm agarwala et al double pivot variant agarwala et al algorithm cohen farach l nearest tree problem our study indicates method produce correct topology shorter sequences guaranteed using methods
artificial life alife research offers among things new style computer simulation understanding biological systems processes but current alife work show enough methodological sophistication count good theoretical biology as first step towards developing stronger methodology alife paper identifies methodological pitfalls arising computer science inuence alife suggests methodological heuristics alife theoretical biology notes strengths alife methods versus previous research methods biology examines open questions theoretical biology may benefit alife simulation argues debate strong alife relevant alifes utility theoretical biology introduction simulating way dark continent
a complete characterization given closed shiftinvariant subspaces l ir provide specified approximation order when space principal ie generated single function characterization terms fourier transform generator as special case obtain classical strangfix conditions without requiring generating function decay infinity the approximation order general closed shiftinvariant space shown already realized specifiable principal subspace
in paper problem learning appropriate domainspecific bias addressed it shown achieved learning many related tasks domain theorem given bounding number tasks must learnt a corollary theorem tasks known possess common internal representation preprocessing number examples required per task good generalisation learning n tasks simultaneously scales like oa b tive support theoretical results reported
principal component analysis pca ubiquitous technique data analysis processing one based upon probability model in paper demonstrate principal axes set observed data vectors may determined maximumlikelihood estimation parameters latent variable model closely related factor analysis we consider properties associated likelihood function giving em algorithm estimating principal subspace iteratively discuss advantages conveyed definition probability density function pca
the study belief change active area philosophy ai in recent years two special cases belief change belief revision belief update studied detail in companion paper friedman halpern introduce new framework model belief change this framework combines temporal epistemic modalities notion plausibility allowing us examine change beliefs time in paper show belief revision belief update captured framework this allows us compare assumptions made method better understand principles underlying in particular shows katsuno mendelzons notion belief update katsuno mendelzon depends several strong assumptions may limit applicability artificial intelligence finally analysis allow us identify notion minimal change underlies broad range belief change operations including revision update fl some work done authors ibm almaden research center the first author also stanford much work done ibm stanfords support gratefully acknowledged the work also supported part air force office scientific research afsc contract fc grant f nsf grants iri iri the first author also supported part ibm graduate fellowship rockwell science center a preliminary version paper appears j doyle e sandewall p torasso eds principles knowledge representation reasoning proc fourth international conference kr pp title a knowledgebased framework belief change part ii revision update
a new heuristic approach minimizing possibly nonlinear non differentiable continuous space functions presented by means extensive testbed includes de jong functions demonstrated new method converges faster certainty adaptive simulated annealing well annealed neldermead approach reputation powerful the new method requires control variables robust easy use lends well parallel computation
we present results three new algorithms setting stepsize parameters ff temporaldifference learning methods td the overall task learning predict outcome unknown markov chain based repeated observations state trajectories the new algorithms select stepsize parameters online way eliminate bias normally inherent temporaldifference methods we compare algorithms conventional monte carlo methods monte carlo methods natural way setting step size state use step size n n number times state visited we seek come close achieving comparable stepsize algorithms td one new algorithm uses n schedule achieve effect processing state backwards td remains completely incremental another algorithm uses time equal estimated transition probability current transition we present empirical results showing improvement convergence rate monte carlo methods conventional td a limitation results present apply tasks whose state trajectories contain cycles
higher order spectra signal contain information non gaussian non linear properties system created since non linearity musical signal usually originate excitation signal linear spectral characteristics attributed resonant chambers discard spectral information looking higher order statistical properties residual signal ie estimated input signal obtained inverse filtering sound in current paper show skewness kurtosis values residual could used characterization important sound properties belonging families strings woodwind brass instrumental timbres the skewness parameter shown closely related bicoherence function calculated original signal succinct interpretation statistical test signal conforming linear non gaussian model the results compared hinich bispectral tests gaussianity non linearity time series exhibit similar classification results finally regarding higher order statistics signal feature vector statistical distance measure cumulant space suggested
in paper discuss two approaches applying memory model case retrieval nets applications distributed processing information required for distinguish two types applications namely case distributed case libraries b case distributed cases while solution former straightforward latter requires extension case retrieval nets provides kind partitioning entire net structure this extended model even allows concurrent implementation retrieval process use collaborative agents retrieval keywords casebased reasoning case retrieval memory structures distributed processing
document draftingan important problemsolving task professionals wide variety fieldstypifies design task requiring complex adaptation case reuse this paper proposes framework document reuse based explicit representation illocutionary rhetorical structure underlying documents explicit representation structure facilitates interpretation previous documents enabling explain construction documents enabling document drafters issue goalbased specifications rapidly retrieve documents similar intentional structure mainte nance multigeneration documents
visualization proven powerful widelyapplicable tool analysis interpretation multivariate data most visualization algorithms aim find projection data space twodimensional visualization space however complex data sets living highdimensional space unlikely single twodimensional projection reveal interesting structure we therefore introduce hierarchical visualization algorithm allows complete data set visualized top level clusters subclusters data points visualized deeper levels the algorithm based hierarchical mixture latent variable models whose parameters estimated using expectationmaximization algorithm we demonstrate principle approach toy data set apply algorithm visualization synthetic data set dimensions obtained simulation multiphase flows oil pipelines data dimensions derived satellite images a matlab software implementation algorithm publicly available worldwide web
assumed unless otherwise stated basically de generates new parameter vectors adding weighted difference two population vectors third vector if resulting vector yields lower objective function value predetermined population member newly generated vector replaces vector compared next generation otherwise old vector retained this basic principle however extended comes practical variants de for example existing vector perturbed adding one weighted difference vector in cases also worthwhile mix parameters old vector perturbed one comparing objective function values several variants de proven useful described
we present novel application ilp problem diterpene structure elucidation c nmr spectra diterpenes organic compounds low molecular weight based skeleton carbon atoms they significant chemical commercial interest use lead compounds search new pharmaceutical effectors the structure elucidation diterpenes based c nmr spectra usually done manually human experts specialized background knowledge peak patterns chemical structures in process skeletal atoms assigned atom number corresponds proper place skeleton diterpene classified one possible skeleton types we address problem learning classification rules database peak patterns diterpenes known structure recently propositional learning successfully applied learn classification rules spectra assigned atom numbers as assignment atom numbers difficult process possibly indistinguishable classification process apply ilp ie relational learning problem classifying spectra without assigned atom numbers
models physical systems differ according computational cost accuracy precision among things depending problem solving task hand different models appropriate several investigators recently developed methods automatically selecting among multiple models physical systems our research novel developing model selection techniques specifically suited computeraided design our approach based idea artifact performance models computeraided design chosen light design decisions required support we developed technique called gradient magnitude model selection gmms embodies principle gmms operates context hillclimbing search process it selects simplest model meets needs hillclimbing algorithm operates we using domain sailing yacht design testbed research we implemented gmms used hillclimbing search decide computationally expensive potentialflow program algebraic approximation analyze performance sailing yachts experimental tests show gmms makes design process faster would expensive model used design evaluations gmms achieves performance improvement little sacrifice quality resulting design
we present new algorithm eliminating excess parameters improving network generalization supervised training the method principal components pruning pcp based principal component analysis node activations successive layers network it simple cheap implement effective it requires network retraining involve calculating full hessian cost function only weight node activity correlation matrices layer nodes required we demonstrate efficacy method regression problem using polynomial basis functions economic time series prediction problem using twolayer feedforward network
gradientbased numerical optimization complex engineering designs offers promise rapidly producing better designs however methods generally assume objective function constraint functions continuous smooth defined everywhere unfortunately realistic simulators tend violate assumptions we present rulebased technique intelligently computing gradients presence pathologies simulators show gradient computation method used part gradientbased numerical optimization system we tested resulting system domain conceptual design supersonic transport aircraft found using rulebased gradients decrease cost design space search one orders magnitude
the first step casebased design systems select initial prototype database previous designs the retrieved prototype modified tailor given goals for particular design goal selection starting point design process dramatic effect quality eventual design overall design time we present technique automatically constructing effective prototypeselection rules our technique applies standard inductivelearning algorithm c set training data describing particular prototype would best choice goal encountered previous design session we tested technique domain racingyachthull design comparing inductively learned selection rules several competing prototypeselection methods our results show inductive prototypeselection method leads better final designs design process guided noisy evaluation function inductively learned rules often efficient competing methods many automated design systems begin retrieving initial prototype library previous designs using given design goal index guide retrieval process the retrieved prototype modified set design modification operators tailor selected design given goals in many cases quality competing designs assessed using domainspecific evaluation functions cases designmodification process often this research benefited numerous discussions members rutgers cap project we thank andrew gelsey helping crossvalidation code john keane helping ruvpp andrew gelsey tim weinrich comments previous draft paper this research supported arpafunded nasa grant nag in context casebased design systems choice initial prototype affect quality final design computational cost obtaining design three reasons first prototype selection may impact quality prototypes lie disjoint search spaces in particular systems design modification operators convert prototype prototype choice initial prototype restrict set possible designs obtained search process a poor choice initial prototype may therefore lead suboptimal final design second prototype selection may impact quality design process guided nonlinear evaluation function unknown global properties since known method guaranteed find global optimum arbitrary nonlinear function design systems rely iterative local search methods whose results sensitive initial starting point finally choice prototype may impact time needed carry design modification processtwo different starting points may yield final design take different amounts time get in design problems evaluating even single design take tremendous amounts time selecting appropriate initial prototype determining factor success failure design process this paper describes application inductive learning form rules selecting appropriate prototype designs the paper structured follows in section describe inductive method learning prototypeselection rules in section describe domain racingyachthull design tested prototypeselection methods in sections describe experiments
this paper describes automatic design methods detecting fraudulent behavior much design accomplished using series machine learning methods in particular combine data mining constructive induction standard machine learning techniques design methods detecting fraudulent usage cellular telephones based profiling customer behavior specifically use rule learning program uncover indicators fraudulent behavior large database cellular calls these indicators used create profilers serve features system combines evidence multiple profilers generate highconfidence alarms experiments indicate automatic approach performs nearly well best handtuned methods detecting fraud
in many cases programs lengths increase known bloat fluff increasing structural complexity artificial evolution we show bloat specific genetic programming suggest inherent search techniques discrete variable length representations using simple static evaluation functions we investigate bloating characteristics three nonpopulation one population based search techniques using novel mutation operator an artificial ant following santa fe trail problem solved simulated annealing hill climbing strict hill climbing population based search using two variants new subtree based mutation operator as predicted bloat observed using unbiased mutation absent simulated annealing hill climbers using length neutral mutation however bloat occurs mutations using population we conclude two causes bloat
we present method learning higherorder polynomial functions examples using linear regression feature construction regression used set training instances produce weight vector linear function feature set if hypothesis imperfect new feature constructed forming product two features effectively predict squared error current hypothesis the algorithm repeated in extension method specific pair features combine selected measuring joint ability predict hypothesis error
non bayesian experimental design linear models reviewed steinberg hunter recent book pukelsheim ford kitsos titterington reviewed non bayesian design nonlinear models bayesian design linear nonlinear models reviewed we argue design problem best considered decision problem best solved maximizing expected utility experiment this paper considers marginal way appropriate theory non bayesian design
we present methodology enables use classification algorithms regression tasks we implement method system recla transforms regression problem classification one uses existent classification system solve new problem the transformation consists mapping continuous variable ordinal variable grouping values appropriate set intervals we use misclassification costs means reflect implicit ordering among ordinal values new variable we describe set alternative discretization methods based experimental results justify need searchbased approach choose best method our experimental results confirm validity searchbased approach class discretization reveal accuracy benefits adding misclassification costs
the bayesian multivariate adaptive regression spline bmars methodology denison et al extended cope nonlinear time series financial datasets the nonlinear time series model closely related adaptive spline threshold autoregressive astar method lewis stevens financial models thought bayesian versions generalised simple autoregressive conditional heteroscadastic garch arch models
some problems solved multiagent teams in using genetic programming produce teams one faces several design decisions first questions team diversity breeding strategy in one commonly used scheme teams consist clones single individuals individuals breed normal way cloned form teams fitness evaluation in contrast teams could also consist distinct individuals in case one either allow free interbreeding members different teams one restrict interbreeding various ways a second design decision concerns types coordinationfacilitating mechanisms provided individual team members range sensors various sorts complex communication systems this paper examines three breeding strategies clones free restricted three coordination mechanisms none deictic sensing namebased sensing evolving teams agents serengeti world simple predatorprey environment among conclusions fact simple form restricted interbreeding outperforms free interbreeding teams distinct individuals fact namebased sensing consistently outperforms deictic sensing
this paper presents or nm rnm algorithm determining whether set n species perfect phylogeny number characters used describe species r maximum number states character the perfect phylogeny algorithm leads oek k e k algorithm triangulating kcolored graph e edges
the risc revolution spurred development processors increasing degrees instruction level parallelism ilp in order realize full potential processors multiple instructions must continuouslybe issued executed single cycle consequently instruction scheduling plays crucial role optimization context while early attempts instruction scheduling limited compiletime approaches current trends aimed providing dynamic support hardware in paper present results detailed comparative study performance advantages derived spectrum instruction scheduling approaches limited basicblock schedulers compiler novel aggressive schedulers hardware a significant portion experimental study via simulations devoted understanding performance advantages runtime scheduling our results indicate effective extracting ilp inherent program trace scheduled wide range machine program parameters furthermore also show effectiveness enhanced simple basicblock scheduler compiler optimizes presence runtime scheduler target current basicblock schedulers designed take advantage feature we demonstrate fact presenting novel basicblock scheduling algorithm sensitive lookahead hardware target processor fl in proceedings third international conference high performance computing dec
posner raichles images mind excellent educational book well written some aws scientific publication accuracy linear subtraction method used pet subject scrutiny research finer spatialtemporal resolutions b lack accuracy experimental paradigm used eeg complementary studies images posner raichle excellent introduction interdisciplinary research cognitive imaging science well written illustrated presents concepts manner well suited laymanundergraduate technical nonexpertgraduate student postdoctoral researcher many people involved interdisciplinary neuroscience research agree p rs statements page importance recognizing emergent properties brain function assemblies neurons it clear sparse references book intended standalone review broad field there aws scientific development must expected pioneering venture p r hav e proposed many cognitive mechanisms deserving study imaging tools yet developed yield better spatialtemporal resolutions
this paper establishes formulas used bound actual treatment effect experimental study treatment assignment random subject compliance imperfect these formulas provide tightest bounds average treatment effect inferred given distribution assignments treatments responses our results reveal even high rates noncompliance experimental data yield significant sometimes accurate information effect treatment population
most researchers machine learning built learning systems assumption external entity would work furnishing learning experiences recently however investigators several subfields machine learning designed systems play active role choosing situations learn such activity generally called exploration this paper describes exploratory learning projects reported literature attempts extract general account issues involved exploration
we study learnability readksatisfyj rksj dnf formulas these boolean formulas disjunctive normal form dnf maximum number occurrences variable bounded k number terms satisfied assignment j after motivating investigation class dnf formulas present algorithm unknown rksj dnf formula learned high probability finds logically equivalent dnf formula using wellstudied protocol equivalence membership queries the algorithm runs polynomial time k j o log n log log n n number input variables
most bayesian theory optimal experimental design normal linear model developed restrictive assumption variance known in special cases insensitivity specific design criteria specific prior assumptions variance demonstrated general result show way bayesian optimal designs affected prior information variance lacking this paper stresses important distinction expected utility functions optimality criteria examines number expected utility functions possess interesting properties deserve wider use derives relevant bayesian optimality criteria normal assumptions this unifying setup useful proving main result paper clarifies issue designing normal linear model unknown variance
recently software pipelining methods based ilp integer linear programming framework successfully applied derive rateoptimal schedules architectures involving clean pipelines pipelines without structural hazards the problem architectures beyond clean pipelines remains open one challenge unified ilp framework simultaneously represent resource constraints unclean pipelines assignment mapping operations loop pipelines in paper provide framework exactly addition constructs rateoptimal software pipelined schedules
planning learning multiple levels temporal abstraction key problem artificial intelligence in paper summarize approach problem based mathematical framework markov decision processes reinforcement learning current modelbased reinforcement learning based onestep models represent commonsense higherlevel actions going lunch grasping object flying denver this paper generalizes prior work temporally abstract models sutton extends prediction setting include actions control planning we introduce general form temporally abstract model multitime model establish suitability planning learning virtue relationship bellman equations this paper summarizes theoretical framework multitime models illustrates potential advantages the need hierarchical abstract planning fundamental problem ai see eg sacerdoti laird et al korf kaelbling dayan hinton modelbased reinforcement learning offers possible solution problem integrating planning realtime learning decisionmaking peng williams moore atkeson sutton barto however current modelbased reinforcement learning based onestep models represent commonsense higherlevel actions modeling actions requires ability handle different interrelated levels temporal abstraction a new approach modeling multiple time scales introduced sutton based prior work singh dayan sutton pinette this approach enables models environment different temporal scales intermixed producing temporally abstract models however work concerned predicting environment this paper summarizes extension approach including actions control environment precup sutton in particular generalize usual notion gridworld planning task
this paper proposes generalisation capabilities casebased reasoning system evaluated comparison rotelearning algorithm uses simple generalisation strategy two algorithms defined expressions classification accuracy derived function size training sample a series experiments using artificial natural data sets described learning curve casebased learner compared apparently trivial rotelearning learning algorithms the results show number plausible situations learning curves simple casebased learner majority rotelearner barely distinguished although domain demonstrated favourable performance casebased learner observed this suggests maxim casebased reasoning similar problems similar solutions may useful basis generalisation strategy selected domains
research robotics programming divided two camps the direct hand programmming approach uses explicit model behavioral model subsumption architecture the machine learning community uses neural network andor genetic algorithm we claim hand programming learning complementary the two approaches used together orders magnitude powerful approach taken separately we propose method combine it includes three concepts syntactic constraints restrict search space handmade problem decomposition hand given fitness we use method solve complex problem eightlegged locomotion it needs less evaluations compared genetic algorithm used alone
we apply recent results markov chain theory hastings metropolis algorithms either independent symmetric candidate distributions provide necessary sufficient conditions algorithms converge geometric rate prescribed distribution in independence case ir k indicate geometric convergence essentially occurs candidate density bounded multiple symmetric case ir show geometric convergence essentially occurs geometric tails we also evaluate recently developed computable bounds rates convergence context examples show theoretical bounds inherently extremely conservative although chain stochastically monotone bounds may well effective
we consider game sequentially assigning probabilities future data based past observations logarithmic loss we making probabilistic assumptions generation data consider situation player tries minimize loss relative loss hindsight best distribution target class worst sequence data we give bounds minimax regret terms metric entropies target class respect suitable distances distributions
in introduced formal framework constructing ordinal similarity measures suggested might also applied cardinal measures in paper place approach general framework called similarity metrics in framework ordinal similarity metrics comparison returns boolean value combined cardinal metrics returning numeric value indeed metrics returning values types produce new metrics
in paper concerned problem inducing recursive horn clauses small sets training examples the method iterative bootstrap induction presented in first step system generates simple clauses regarded properties required definition properties represent generalizations positive examples simulating effect larger number examples properties used subsequently induce required recursive definitions this paper describes method together series experiments the results support thesis iterative bootstrap induction indeed effective technique could general use ilp
considerable effort directed recently develop asymptotically minimax methods problems recovering infinitedimensional objects curves densities spectral densities images noisy data a rich complex body work evolved nearly exactly minimax estimators obtained variety interesting problems unfortunately results often translated practice variety reasons sometimes similarity known methods sometimes computational intractability sometimes lack spatial adaptivity we discuss method curve estimation based n noisy data one translates empirical wavelet coefficients towards origin amount method different methods common use today computationally practical spatially adaptive thus avoids number previous objections minimax estimators at time method nearly minimax wide variety loss functions eg pointwise error global error measured l p norms pointwise global error estimation derivatives wide range smoothness classes including standard holder classes sobolev classes bounded variation this much broader nearoptimality anything previously proposed minimax literature finally theory underlying method interesting exploits correspondence statistical questions questions optimal recovery informationbased complexity acknowledgements these results described oberwolfach meeting mathematische stochastik december ams annual meeting january this work supported nsf dms the authors would like thank paullouis hennequin organized ecole ete de probabilites saint flour collaboration began universite de paris vii jussieu universite de parissud orsay supporting visits dld imj the authors would like thank ildar ibragimov arkady nemirovskii personal correspondence cited p
figure figure b show prior distribution fcr follows flat prior skewed prior respectively figure c figure show posterior distribution pf cr jd obtained system run lipid data using flat prior skewed prior respectively from bounds balke pearl follows largesample assumption f cr jd figure prior b posterior cd distributions subpopulation f cr jd specified counterfactual query would joe improved taken drug given improve without corresponds flat prior b skewed prior this paper identifies demonstrates new application area networkbased inference techniques management causal analysis clinical experimentation these techniques originally developed medical diagnosis shown capable circumventing one major problems clinical experiments assessment treatment efficacy face imperfect compliance while standard diagnosis involves purely probabilistic inference fully specified networks causal analysis involves partially specified networks links given causal interpretation domain variables unknown the system presented paper provides clinical research community believe first time assumptionfree unbiased assessment average treatment effect we offer system practical tool used whenever full compliance enforced broadly whenever data available insufficient answering queries interest clinical investigator lipid research clinic program the lipid research clinics coronary primary prevention trial results parts ii journal american medical association january
when can we give causal interpretation abstract the assumptions underlying statistical estimation fundamentally different character causal assumptions underly structural equation models sem the differences blurred years lack mathematical notation capable distinguishing causal equational relationships recent advances graphical methods provide formal explication differences destined profound impact sems practice philosophy
incremental class learning icl provides feasible framework development scalable learning systems instead learning complex problem icl focuses learning subproblems incrementally one time using results prior learning subsequent learning combining solutions appropriate manner with respect multiclass classification problems icl approach presented paper summarized follows initially system focuses one category after learns category tries identify compact subset features nodes hidden layers crucial recognition category the system freezes crucial nodes features fixing incoming weights as result features obliterated subsequent learning these frozen features available subsequent learning serve parts weight structures build recognize categories as categories learned set features gradually stabilizes learning new category requires less effort eventually learning new category may involve combining existing features appropriate manner the approach promotes sharing learned features among number categories also alleviates wellknown catastrophic interference problem we present results applying icl approach handwritten digit recognition problem based spatiotemporal representation patterns
pathoriented scheduling methods trace scheduling hyperblock scheduling use speculation extract instructionlevel parallelism controlintensive programs these methods predict important execution paths current scheduling scope using execution profiling frequency estimation aggressive speculation applied important execution paths possibly cost degraded performance along paths therefore speed output code sensitive compilers ability accurately predict important execution paths prior work area utilized speculative yield function fisher coupled dependence height distribute instruction priority among execution paths scheduling scope while technique provides stability performance paying attention needs paths directly address problem mismatch compiletime prediction runtime behavior the work presented paper extends speculative yield dependence height heuristic explicitly minimize penalty suffered paths instructions speculated along path since execution time path determined number cycles spent paths entrance exit scheduling scope heuristic attempts eliminate unnecessary speculation delays paths exit such control speculation makes performance much less sensitive actual path taken run time the proposed method strong emphasis achieving minimal delay exits thus name speculative hedge used this paper presents speculative hedge heuristic shows controls overspeculation superblockhyperblock scheduler the stability copyright ieee published proceedings th annual international symposium microarchitecture december paris france personal use material permitted however permission reprintrepublish material resale redistribution purposes creating new collective works resale redistribution servers lists reuse copyrighted component work works must obtained ieee contact manager copyrights permissions ieee service center hoes lane po box piscataway nj usa telephone intl
a number exact algorithms developed perform probabilistic inference bayesian belief networks recent years the techniques used algorithms closely related network structures easy understand implement in paper consider problem combinatorial optimization point view state efficient probabilistic inference belief network problem finding optimal factoring given set probability distributions from viewpoint previously developed algorithms seen alternate factoring strategies in paper define combinatorial optimization problem optimal factoring problem discuss application problem belief networks we show optimal factoring provides insight key elements efficient probabilistic inference demonstrate simple easily implemented algorithms excellent performance
backpropagation learning rumelhart hinton williams useful research tool number undesiderable features experimenter decide outside learned we describe number simulations neural networks internally generate teaching input the networks generate teaching input trasforming network input connection weights evolved using form genetic algorithm what results innate evolved capacity behave efficiently environment learn behave efficiently the analysis networks evolve learn shows interesting results
to appear twelfth national conference artificial intelligence aaai seattle wa july august technical report ra april abstract evaluation counterfactual queries eg if a true would c true important fault diagnosis planning determination liability we present formalism uses probabilistic causal networks evaluate ones belief counterfactual consequent c would true antecedent a true the antecedent query interpreted external action forces proposition a true consistent lewis miraculous analysis this formalism offers concrete embodiment closest world approach properly reflects common understanding causal influences deals uncertainties inherent world amenable machine representation
evaluation counterfactual queries eg if a true would c true important fault diagnosis planning determination liability policy analysis we present method evaluating counterfactuals underlying causal model represented structural models nonlinear generalization simultaneous equations models commonly used econometrics social sciences this new method provides coherent means evaluating policies involving control variables prior enacting policy influenced variables system
theory refinement task updating domain theory light new cases done automatically expert assistance the problem theory refinement uncertainty reviewed context bayesian statistics theory belief revision the problem reduced incremental learning task follows learning system initially primed partial theory supplied domain expert thereafter maintains internal representation alternative theories able interrogated domain expert able incrementally refined data algorithms refinement bayesian networks presented illustrate meant partial theory alternative theory representation etc the algorithms incremental variant batch learning algorithms literature work well batch incremental mode
the emergence generalist specialist behavior populations neural networks studied energy extracting ability included property organism in artificial life simulations organisms living environment fitness score interpreted combination organisms behavior ability organism extract energy potential food sources distributed environment the energy extracting ability viewed evolvable trait organisms particular organisms mechanisms extracting energy environment therefore fixed decided researcher simulations fixed evolvable energy extracting abilities show energy extracting mechanism sensory apparatus behavior organisms may coevolve coadapted the results suggest populations organisms evolve generalists specialists due individual energy extracting abilities
in paper consider problem theory patching given domain theory whose components indicated possibly flawed set labeled training examples domain concept the theory patching problem revise indicated components theory resulting theory correctly classifies training examples theory patching thus type theory revision revisions made individual components theory our concern paper determine classes logical domain theories theory patching problem tractable we consider propositional firstorder domain theories show theory patching problem equivalent determining information contained theory stable regardless revisions might performed theory we show determining stability tractable input theory satisfies two conditions revisions theory component monotonic effects classification examples theory components act independently classification examples theory we also show concepts introduced used determine soundness completeness particular theory patching algorithms
this paper studies balance evolutionary design human expertise order best design situated autonomous agents learn specific tasks a genetic algorithm designs control circuits learn simple behaviors given control strategies simple behaviors genetic algorithm designs combinational circuit switches simple behaviors perform navigation task keywords genetic algorithms computational design autonomous agents robotics
in paper propose threestage incremental approach development autonomous agents we discuss issues characteristics differentiate reinforcement programs rps define trainer particular kind rp we present set results obtained running experiments trainer provides guidance autonomouse mousesized autonomous robot
in paper carefully formulate schema theorem genetic programming gp using schema definition accounts variable length nonhomologous nature gps representation in manner similar early ga research use interpretations gp schema theorem obtain gp building block definition state classical building block hypothesis bbh gp searches hierarchically combining building blocks we report approach convincing several reasons difficult find support promotion combination building blocks solely rigourous interpretation gp schema theorem even support bbh empirically questionable whether building blocks always exist partial solutions consistently average fitness resilience disruption assured also bbh constitutes narrow imprecise account gp search behavior
although feedforward neural networks well suited function approximation applications networks experience problems learning desired function one problem interference occurs learning one area input space causes unlearning another area networks less susceptible interference referred spatially local networks to understand properties theoretical framework consisting measure interference measure network localization developed incorporates network weights architecture also learning algorithm using framework analyze sigmoidal multilayer perceptron mlp networks employ backprop learning algorithm address familiar misconception sigmoidal networks inherently nonlocal demonstrating given sufficiently large number adjustable parameters sigmoidal mlps made arbitrarily local retaining ability represent continuous function compact domain
university wisconsinmadison department computer sciences technical report cstr abstract the iterated prisoners dilemma choice refusal ipdcr extension iterated prisoners dilemma evolution allows players choose refuse game partners from individual behaviors behavioral population structures emerge in report examine one particular ipdcr environment document social network methods used identify population behaviors found within complex adaptive system in contrast standard homogeneous population nice cooperators also found metastable populations mixed strategies within environment in particular social networks interesting populations evolution examined
a paradigm statistical mechanics financial markets smfm using nonlinear nonequilibrium algorithms first published l ingber mathematical modelling fit multivariate financial markets using adaptive simulated annealing asa global optimization algorithm perform maximum likelihood fits lagrangians defined path integrals multivariate conditional probabilities canonical momenta thereby derived used technical indicators recursive asa optimization process tune trading rules these trading rules used outofsample data demonstrate profit smfm model illustrate markets likely efficient
the close connection reinforcement learning rl algorithms dynamic programming algorithms fueled research rl within machine learning community yet despite increased theoretical understanding rl algorithms remain applicable simple tasks in paper i use abstract framework afforded connection dynamic programming discuss scaling issues faced rl researchers i focus learning agents learn solve multiple structured rl tasks environment i propose learning abstract environment models abstract actions represent intentions achieving particular state such models variable temporal resolution models different parts state space abstract actions span different number time steps the operational definitions abstract actions learned incrementally using repeated experience solving rl tasks i prove certain conditions solutions new rl tasks found using simu lated experience abstract actions alone
we describe supervised learning algorithm eodg uses mutual information build oblivious decision tree the tree converted oblivious readonce decision graph oodg merging nodes level tree for domains appropriate decision trees oodgs performance approximately c number nodes oodg much smaller the merging phase converts oblivious decision tree oodg provides new way dealing replication problem new pruning mechanism works top starting root the pruning mechanism well suited finding symmetries aids recovering splits irrelevant features may happen tree construction
an approach explicitly formulated blend local global theory investigate oscillatory neocortical firings determine source information processing nature alpha rhythm the basis optimism founded statistical mechanical theory neocortical interactions success numerically detailing properties shorttermmemory stm capacity mesoscopic scales columnar interactions consistent theory deriving similar dispersion relations macroscopic scales electroencephalographic eeg magnetoencephalographic meg activity manuscript received march this project supported entirely personal contributions physical studies institute university california san diego physical studies institute agency account institute pure applied physical sciences
we present new results positive negative wellstudied problem learning disjunctive normal form dnf expressions we first prove algorithm due kushilevitz mansour used weakly learn dnf using membership queries polynomial time respect uniform distribution inputs this first positive result learning unrestricted dnf expressions polynomial time nontrivial formal model learning it provides sharp contrast results kharitonov proved ac efficiently learnable model given certain plausible cryptographic assumptions we also present efficient learning algorithms various models readk satk subclasses dnf for negative results turn attention recently introduced statistical query model learning this model restricted version popular probably approximately correct pac model practically every class known efficiently learnable pac model fact learnable statistical query model here give general characterization complexity statistical query learning terms number uncorrelated functions concept class this distributiondependent quantity yielding upper lower bounds number statistical queries required learning input distribution as corollary obtain dnf expressions decision trees even weakly learnable fl this research sponsored part wright laboratory aeronautical systems center air force materiel command usaf advanced research projects agency arpa grant number f support also sponsored national science foundation grant no cc blum also supported part nsf national young investigator grant ccr views conclusions contained document authors interpreted necessarily representing official policies endorsements either expressed implied wright laboratory united states government nsf respect uniform input distribution polynomial time statistical query model this result informationtheoretic therefore rely unproven assumptions it demonstrates simple modification existing algorithms computational learning theory literature learning various restricted forms dnf decision trees passive random examples also several algorithms proposed experimental machine learning communities id algorithm decision trees variants solve general problem the unifying tool results fourier analysis finite class boolean functions hypercube
planning abstract planning learning multiple levels temporal abstraction key problem artificial intelligence in paper summarize approach problem based mathematical framework markov decision processes reinforcement learning current modelbased reinforcement learning based onestep models represent commonsense higherlevel actions going lunch grasping object flying denver this paper generalizes prior work temporally abstract models sutton b extends prediction setting include actions control planning we introduce general form temporally abstract model multitime model establish suitability planning learning virtue relationship bellman equations this paper summarizes theoretical framework multitime models illustrates potential ad the need hierarchical abstract planning fundamental problem ai see eg sacerdoti laird et al korf kaelbling dayan hinton modelbased reinforcement learning offers possible solution problem integrating planning realtime learning decisionmaking peng williams moore atkeson sutton barto press however current modelbased reinforcement learning based onestep models represent commonsense higherlevel actions modeling actions requires ability handle different interrelated levels temporal abstraction a new approach modeling multiple time scales introduced sutton b based prior work singh dayan b sutton pinette this approach enables models environment different temporal scales intermixed producing temporally abstract models however work concerned predicting environment this paper summarizes vantages gridworld planning task
in complex changing environments explanation must dynamic goaldriven process this paper discusses evolving system implementing novel model explanation generation goaldriven interactive explanation models explanation goaldriven multistrategy situated process interweaving reasoning action we describe preliminary implementation model gobie system generates explanations internal use support plan generation execution
it shown recently clarke ledyaev sontag subbotin asymptotically controllable system stabilized means certain type discontinuous feedback the feedback laws constructed work robust respect actuator errors well perturbations system dynamics a drawback however may highly sensitive errors measurement state vector this paper addresses shortcoming shows design dynamic hybrid stabilizing controller preserving robustness external perturbations actuator error also robust respect measurement error this new design relies upon controller incorporates internal model system driven previously constructed feedback
report sycon abstract this note presents explicit proof theorem due artstein states existence smooth controllyapunov function implies smooth stabilizability more result extended realanalytic rational cases well the proof uses universal formula given algebraic function lie derivatives formula originates solution simple riccati equation
modulo scheduling efficient technique exploiting instruction level parallelism variety loops resulting high performance code increased register requirements we present set low computational complexity stagescheduling heuristics reduce register requirements given modulo schedule shifting operations multiples ii cycles measurements benchmark suite loops perfect club spec livermore fortran kernels shows best heuristic achieves average decrease register requirements obtained optimal stage scheduler
modulo scheduling efficient technique exploiting instruction level parallelism variety loops resulting high performance code increased register requirements we present approach schedules loop operations minimum register requirements given modulo reservation table our method determines optimal register requirements machines finite resources general dependence graphs measurements benchmark suite loops perfect club spec livermore fortran kernels show register requirements decrease average applying optimal stage scheduler mrtschedules registerinsensitive modulo scheduler
the design implementation software ring array processor rap high performance parallel computer involved development three hardware platforms sun sparc workstations heurikon mc boards running vxworks realtime operating system texas instruments tmsc dsps the rap runs sun workstations unix vme based system using vxworks a flexible set tools provided rap user programmer primary emphasis placed improving efficiency layered artificial neural network algorithms this done providing library assembly language routines use nodecustom compilation an objectoriented rap interface c provided allows programmers incorporate rap computational server unix applications for wishing program c command interpreter built provides interactive shellscript style rap manipulation
selecting set features optimal given task problem plays important role wide variety contexts including pattern recognition adaptive control machine learning our experience traditional feature selection algorithms domain machine learning lead appreciation computational efficiency concern brittleness this paper describes alternate approach feature selection uses genetic algorithms primary search component results presented suggest genetic algorithms used increase robustness feature selection algorithms without significant decrease computational efficiency
in paper address following software pipelining problem given loop machine architecture fixed number processor resources eg function units one construct softwarepipelined schedule runs given architecture maximum possible iteration rate la rateoptimal minimizing number registers the main contributions paper first demonstrate problem described simple mathematical formulation precise optimization objectives periodic linear scheduling framework the mathematical formulation provides clear picture permits one visualize overall solution space rateoptimal schedules different sets con straints secondly show precise mathematical formulation solution make significant performance difference we evaluated performance method three leading contemporary heuristic methods huff slack scheduling wang eisenbeis jourdan sus frlc gasperoni schwiegelshohns modified list scheduling experimental results show method described paper performed significantly better methods
this paper concerns issue best form learning representing using knowledge decision making the proposed answer knowledge learned represented declarative form when needed decision making efficiently transferred procedural form tailored specific decision making situation such approach combines advantages declarative representation facilitates learning incremental knowledge modification procedural representation facilitates use knowledge decision making this approach also allows one determine decision structures may avoid attributes unavailable difficult measure given situation experimental investigations system frd demonstrated decision structures obtained via declarative route often higher predictive accuracy also simpler learned directly facts
several evolutionary algorithms make use hierarchical representations variable size rather linear strings fixed length variable complexity structures provides additional representational power may widen application domain evolutionary algorithms the price however search space openended solutions may grow arbitrarily large size in paper study effects structural complexity solutions generalization performance analyzing fitness landscape sigmapi neural networks the analysis suggests smaller networks achieve average better generalization accuracy larger ones thus confirming usefulness occams razor a simple method implementing occams razor principle described shown effective improv ing generalization accuracy without limiting learning capacity
we present interactive algorithms learning regular grammars positive examples membership queries a structurally complete set strings language lg corresponding unknown regular grammar g implicitly specifies lattice version space represents space candidate grammars containing unknown grammar g this lattice searched efficiently using membership queries identify unknown grammar g using implicit representation version space form two sets s g correspond respectively set specific general grammars consistent set positive examples provided queries answered teacher given time we present provably correct incremental version algorithm structurally complete set positive samples necessarily available learner beginning learning the learner constructs lattice grammars based strings provided start performs candidate elimination posing safe membership queries when additional examples become available learner incrementally updates lattice continues candidate elimination eventually set positive samples provided teacher encompasses structurally complete set unknown grammar algorithm terminates identifying unknown grammar g
we argue based upon numbers representations given length increase representation length inherent using fixed evaluation function discrete variable length representation two examples analysed including use prices theorem both examples confirm tendency solutions grow size caused fitness based selection
environments vary time present fundamental problem adaptive systems although worst case hope effective adaptation forms environmental variability provide adaptive opportunities we consider broad class nonstationary environments combine variable result function invariant utility function demonstrate via simulation adaptive strategy employing evolution learning tolerate much higher rate environmental variation evolutiononly strategy we suggest many cases stability previously assumed constant utility nonstationary environment may fact powerful viewpoint
we recently introduced neural network reactive obstacle avoidance based model classical operant conditioning in article describe success model implemented two real autonomous robots our results show promise selforganizing neural networks domain intelligent robotics
the paper reports application genetic algorithms probabilistic search algorithms based model organic evolution npcomplete combinatorial optimization problems in particular subset sum maximum cut minimum tardy task problems considered except fitness function problemspecific changes genetic algorithm required order achieve results high quality even problem instances size used paper for constrained problems subset sum minimum tardy task constraints taken account incorporating graded penalty term fitness function even large instances highly multimodal optimization problems iterated application genetic algorithm observed find global optimum within number runs as genetic algorithm samples tiny fraction search space results quite encouraging
cupit specialpurpose programming language designed expressing dynamic neural network learning algorithms it provides flexibility generalpurpose languages c c expressive it allows writing much clearer elegant programs particular algorithms change network topology dynamically constructive algorithms pruning algorithms in contrast languages cupit programs compiled efficient code parallel machines without changes source program thus providing easy start using parallel platforms this article analyzes circumstances cupit approach useful one presents description language constructs reports performance results cupit symmetric multiprocessors smps it concludes many cases cupit good basis neural learning algorithm research smallscale parallel machines
augmenting genetic algorithms local search heuristics promising approach solution combinatorial optimization problems in paper genetic local search approach quadratic assignment problem qap presented new genetic operators realizing approach described performance tested various qap instances containing facilitieslocations the results indicate proposed algorithm able arrive high quality solutions relatively short time limit largest publicly known prob lem instance new best solution could found
the problem programming artificial ant follow santa fe trail used example program search space previously reported genetic programming simulated annealing hill climbing performance shown much better random search ant problem analysis program search space terms fixed length schema suggests highly deceptive simplest solutions large building blocks must assembled average fitness in cases show solutions assembled using fixed representation small building blocks average fitness this suggest ant problem difficult genetic algorithms
machine learning research making great progress many directions this article summarizes four directions discusses current open problems the four directions improving classification accuracy learning ensembles classifiers b methods scaling supervised learning algorithms c reinforcement learning learning complex stochastic models
fills algorithm perfect simulation attractive finite state space models unbiased user impatience presented terms stochastic recursive sequences extended two ways repulsive discrete markov random fields two coding sets like autopoisson distribution lattice neighbourhood treated monotone systems particular partial ordering quasimaximal quasiminimal states used fills algorithm applies directly combining fills rejection sampling sandwiching leads version algorithm works general discrete conditionally specified repulsive models extensions types models briefly discussed
we consider special case reinforcement learning environment described linear system the states environment actions agent perform represented real vectors system dynamic given linear equation stochastic component the problem equivalent socalled linear quadratic regulator problem studied optimal adaptive control literature we propose learning algorithm problem analyze pac learning framework unlike algorithms adaptive control literature algorithm actively explores environment learn accurate model system faster we show control law produced algorithm high probability value close optimal policy relative magnitude initial state system the time taken algorithm polynomial dimension n statespace dimension r actionspace ratio nr constant
the results reported empirically show benefit decision tree size biases function concept distribution first shown concept distribution complexity number internal nodes smallest decision tree consistent example space affects benefit minimum size maximum size decision tree biases second policy described defines learner given knowledge complexity distribution concepts third explanations distribution concepts seen practice amenable minimum size decision tree bias given evaluated empirically
in paper describe sound classification method seems applicable broad domain stationary nonmusical sounds machine noises man made non periodic sounds the method based matching higher order spectra hos acoustic signals generalizes earlier results classification sustained musical sounds higher order statistics an efficient decorrelated matched filter implemetation presented the results show good sound classification statistics comparison spectral matching methods also discussed
many todays algorithms inductive logic programming ilp put heavy burden responsibility user declarative bias defined rather lowlevel fashion to address issue developed method generating declarative language bias topdown ilp systems highlevel declarations the key feature approach distinction user level expert level language bias declarations the expert provides abstract metadeclarations user declares relationship metalevel given database obtain lowlevel declarative language bias the suggested languages allow compact abstract specifications declarative language bias topdown ilp systems using schemata we verified several properties translation algorithm generates schemata applied successfully chemical domains as consequence propose use twolevel approach generate declarative language bias
one difficult problems area explanation based learning utility problem learning many rules low utility lead swamping degradation performance this paper introduces two new techniques improving utility learned rules the first technique combine ebl inductive learning techniques learn better set control rules second technique use inductive techniques learn approximate control rules the two techniques synthesized algorithm called approximating abductive explanation based learning axaebl axaebl shown improve substantially standard ebl several domains
we address problem program discovery defined genetic programming by combining hierarchical crossover operator two traditional single point search algorithms simulated annealing stochastic iterated hill climbing solved problems processing fewer candidate solutions greater probability success genetic programming we also enhanced genetic programming hybridizing simple idea hill climbing individuals fixed interval generations
most kdd applications consider databases static objects however many databases inherently temporal ie store evolution object passage time thus regularities dynamics databases discovered current state might depend way previous states to end preprocessing data needed aimed extracting relationships intimately connected temporal nature data make available discovery algorithm the predicate logic language ilp methods together recent advances ef ficiency makes adequate task
in paper consider continous time method approximating given distribution using langevin diffusion dl dw r log l dt we find conditions diffusion converges exponentially quickly one dimension essentially distributions exponential tails form x expfljxj fi lt fi lt exponential convergence occurs fi we consider conditions discrete approximations diffusion converge we first show even diffusion converges naive discretisations need we consider metropolisadjusted version algorithm find conditions also converges exponential rate perhaps surprisingly even metropolised version need converge exponentially fast even diffusion we briefly discuss truncated form algorithm practice avoid difficulties forms
an essential component intelligent agent ability notice encode store utilize information environment traditional approaches program induction focused evolving functional reactive programs this paper presents mapmaker approach automatic generation agents discover information environment encode information later use create simple plans utilizing stored mental models in approach agents multipart computer programs communicate shared memory both programs representation scheme evolved using genetic programming an illustrative problem gold collection used demonstrate approach one part program makes map world stores memory part uses map find gold the results indicate approach evolve programs store simple representations environments use representations produce simple plans introduction
reinforcement learning used predict rewards also predict states ie learn model worlds dynamics models defined different levels temporal abstraction multitime models models focus predicting happen rather certain event take place based multitime models define abstract actions enable planning presumably efficient way various levels abstraction
previous research shown technique called errorcorrecting output coding ecoc dramatically improve classification accuracy supervised learning algorithms learn classify data points one k classes this paper presents investigation ecoc technique works particularly employed decisiontree learning algorithms it shows ecoc method like form voting committeecan reduce variance learning algorithm furthermoreunlike methods simply combine multiple runs learning algorithmecoc correct errors caused bias learning algorithm experiments show bias correction ability relies nonlocal havior c
technical report crgtr may revised feb abstract factor analysis statistical method modeling covariance structure high dimensional data using small number latent variables extended allowing different local factor models different regions input space this results model concurrently performs clustering dimensionality reduction thought reduced dimension mixture gaussians we present exact expectationmaximization algorithm fitting parameters mixture factor analyzers
the position size shape visual receptive field rf primary visual cortical neurons change dynamically response artificial scotoma conditioning cats pettet gilbert retinal lesions cats monkeys dariansmith gilbert the exin learning rules marshall used model dynamic rf changes the exin model compared adaptation model xing gerstein lissom model sirosh miikkulainen sirosh et al to emphasize role lateral inhibitory learning rules exin lissom simulations done lateral inhibitory learning during scotoma conditioning exin model without feedforward learning produces centrifugal expansion rfs initially inside scotoma region accompanied increased responsiveness without changes spontaneous activation the exin model without feedforward learning consistent neurophysiological data adaptation model lissom model the comparison exin lissom models suggests experiments determine role feedforward excitatory lateral inhibitory learning producing dynamic rf changes scotoma conditioning
in paper present bottomup algorithm called mri induce logic programs examples this method induce programs base clause one recursive clause small number examples mri based analysis saturations examples it first generates path structure expression stream values processed predicates the concept path structure originally introduced identamalmquist used tim idestamalmquist in paper introduce concepts extension difference path structure recursive clauses expressed difference path structure extension the paper presents algorithm shows experimental results obtained method
the bayesian analysis neural networks difficult simple prior weights implies complex prior distribution functions in paper investigate use gaussian process priors functions permit predictive bayesian analysis fixed values hyperparameters carried exactly using matrix operations two methods using optimization averaging via hybrid monte carlo hyperparameters tested number challenging problems produced excellent results
explanations play key role operationalizationbased anomaly detection techniques in paper show role limited anomaly detection also used guiding automated knowledge base refinement we introduce refinement procedure takes small number refinement rules rather test cases ii explanations constructed attempt reveal cause causes inconsistencies detected verification process returns rule revisions aiming recover consistency kbtheory inconsistencies caused one anomaly handled time improves efficiency refinement process
this paper sets conceptual framework openended artificial evolution complex behaviour autonomous agents if recurrent dynamical neural networks similar used phenotypes genetic algorithm employs variable length genotypes inman harveys saga capable evolving arbitrary levels behavioural complexity furthermore simple restrictions encoding scheme governs genotypes develop phenotypes may guaranteed increase fitness requires increase behavioural complexity evolve in order process practicable design alternative however time periods involved must acceptable the final part paper looks general ways encoding scheme may modified speed process experiments reported different categories scheme tested conclusions offered promising type encoding scheme vi able openended evolutionary robotics
we recently introduced neural network mobile robot controller netmorc autonomously learns forward inverse odometry differential drive robot unsupervised learningbydoing cycle after initial learning phase controller move robot arbitrary stationary moving target compensating noise forms disturbance wheel slippage changes robots plant in addition forward odometric map allows robot reach targets absence sensory feedback the controller also able adapt response longterm changes robots plant change radius wheels in article review netmorc architecture describe simplified algorithmic implementation present new quantitative results netmorcs performance adaptability noisefree noisy conditions compare netmorcs performance trajectoryfollowing task performance alternative controller describe preliminary results hardware implementation netmorc mobile robot robuter
we develop algorithm simulating perfect random samples invariant measure harris recurrent markov chain the method uses backward coupling embedded regeneration times works effectively finite chains stochastically monotone chains even continuous spaces paths may sandwiched upper lower processes examples show naive approaches constructing bounding processes may considerably biased algorithm simplified certain cases make easier run we give explicit analytic bounds backward coupling times stochastically monotone case
this reports gives review new exact simulation algorithms using markov chains the first part covers discrete case we consider two different algorithms propp wilsons coupling past cftp technique fills rejection sampler the algorithms tested ising model without external field the second part covers continuous state spaces we present several algorithms developed murdoch green based coupling past we discuss applicability methods bayesian analysis problem surgical failure rates
specialist generalist behaviors populations artificial neural networks studied a genetic algorithm used simulate evolution processes thereby develop neural network control systems exhibit specialist generalist behaviors according fitness formula with evolvable fitness formulae evaluation measure let free evolve obtain coevolution expressed behavior individual evolvable fitness formula the use evolvable fitness formulae lets us work dynamic fitness landscape opposed work traditionally applies static fitness landscapes the role competition specialization studied letting individuals live social conditions shared environment directly compete we find competition act provide population diversification populations organisms individual evolvable fitness formulae
using deliberately designed primitive sets investigate relationship contextbased expression mechanisms size height density genetic program trees evolutionary process we show contextual semantics influence composition location flows operative code program in detail analyze dynamics discuss impact findings microlevel descriptions genetic programming
most traditional prediction techniques deliver mean probability distribution single point for multimodal processes instead predicting mean probability distribution important predict full distribution this article presents new connectionist method predict conditional probability distribution response input the main idea transform problem regression classification problem the conditional probability distribution network perform direct predictions iterated predictions task specific time series problems we compare method fuzzy logic discuss important differences also demonstrate architecture two time series the first benchmark laser series used santa fe competition deterministic chaotic system the second time series markov process exhibits structure two time scales the network produces multimodal predictions series we compare predictions network nearestneighbor predictor find conditional probability network twice likely model
we present stable ilp crossdisciplinary concept straddling machine learning nonmonotonic reasoning stable models give meaning logic programs containing negative assertions in stable ilp employ stable models represent current state specified possibly negative edb idb rules the state serves background knowledge topdown ilp learner we present framework implementation system inded one realization stable ilp
in recent years considerable effort gone understanding default reasoning most effort concentrated question entailment ie conclusions warranted knowledgebase defaults surprisingly works formally examine general role defaults we argue examination role necessary order understand defaults suggest concrete role defaults defaults simplify decisionmaking process allowing us make fast approximately optimal decisions ignoring certain possible states in order formalize approach examine decision making framework decision theory we use probability utility measure impact possible states decisionmaking process we accept default ignores states small impact according measure we motivate choice measures show resulting formalization defaults satisfies desired properties defaults namely cumulative reasoning finally compare approach pooles decisiontheoretic defaults show combined form attractive framework reasoning decisions we make numerous assumptions day car start road blocked heavy traffic pm etc many assumptions defeasible willing retract given sufficient evidence humans naturally state defaults draw conclusions default information hence defaults seem play important part commonsense reasoning to use statements however need formal understanding defaults represent conclusions admit the problem default entailmentroughly conclusions draw knowledgebase defaultshas attracted great deal attention many researchers attempt find contextfree patterns default reasoning eg kraus et al as research shows much done approach we claim however utility approach limited gain better understanding defaults need understand situations willing state default our main thesis investigation defaults elaborate role behavior reasoning agent this role allow us examine default appropriate terms implications agents overall performance in paper suggest particular role defaults show role allows us provide semantics defaults of course claim role defaults play in many applications end result reasoning choice actions usually choice optimal much uncertainty state world effects actions allow examination possibilities we suggest one role defaults lies simplifying decisionmaking process stating assumptions reduce space examined possibilities more precisely suggest default license ignore situations knowledge amounts one particular suggestion understood light semantics pearl in semantics accept default given knowledge probability small this small probability states gives us license ignore although probability plays important part decisions claim also examine utility actions for example people think highly unlikely die next year also believe accept default assumption context decision whether buy life insurance in context stakes high ignore outcome even though unlikely we suggest license ignore set given based impact decision to paraphrase view accept bird fly assuming bird flies get us much trouble to formalize intuitions examine decisionmaking framework decision theory luce raiffa decision theory represents decision problem using several components set possible states probability measure sets utility function assigns action state numerical value fl to appear ijcai
density estimation commonly used test case nonparametric estimation methods we explore asymptotic properties estimators based thresholding empirical wavelet coefficients minimax rates convergence studied large range besov function classes b spq range global l p error measures p lt a single wavelet threshold estimator asymptotically minimax within logarithmic terms simultaneously range spaces error measures in particular p gt p form nonlinearity essential since minimax linear estimators suboptimal polynomial powers n a second approach using approximation gaussian white noise model mallows metric used acknowledgements we thank alexandr sakhanenko helpful discussions references work berry esseen theorems used section this work supported part nsf dms the second author would like thank universite de
used turn approximate a empirical studies show good results achieved tsl however tsl several drawbacks training set learners eg backpropagation typically slow may require many passes training set also guarantee given arbitrary training set system find enough good critical features get reasonable approximation a moreover number features searched exponential number inputs tsl becomes computationally expensive finally scarcity interesting positive theoretical results suggests difficulty learning without sufficient priori knowledge the goal learning systems generalize generalization commonly based set critical features system available training set learners typically extract critical features random set examples while approach attractive suffers exponential growth number features searched we propose extend endowing system priori knowledge form precepts advantages augmented system speedup improved generalization greater parsimony this paper presents preceptdriven learning algorithm its main features include distributed implementation bounded learning execution times ability handle correct incorrect precepts results simulations realworld data demonstrate promise this paper presents preceptdriven learning pdl pdl intended overcome tsls weaknesses in pdl training set augmented small set precepts a pair p i o called example a precept example ientries inputs set special value dontcare an input whose value dontcare said asserted if effect value output the use special value dontcare therefore shorthand a pair containing dontcare inputs represents many examples product sizes input domains dontcare inputs introduction
many inductive learning problems expressed classical attributevalue language in order learn generalize learning systems often rely measure similarity current knowledge base new information the attributevalue language defines heterogeneous multidimensional input space attributes nominal others linear defining similarity proximity two points input spaces non trivial we discuss two representative homogeneous metrics show examples limited domains we address issues raised design heterogeneous metric inductive learning systems in particular discuss need normalization impact dontcare values we propose heterogeneous metric evaluate empirically simplified version ila
we study efficient algorithms solving following problem call switching distributions learning problem a sequence s n finite alphabet generated following way the sequence concatenation k runs consecutive subsequence each run generated independent random draws distribution p p element set distributions fp p n g the learning algorithm given sequence goal find approximations distributions p p n give approximate segmentation sequence constituting runs we give efficient algorithm solving problem show conditions algorithm guaranteed work high probability
for connectionist networks adequate higher level cognitive activities natural language interpretation generalize way appropriate given regularities domain fodor pylyshyn identified important pattern regularities domains called systematicity several attempts made show connectionist networks generalize accordance regularities satisfaction critics to address challenge paper starts establishing implications systematicity connectionist solutions variable binding problem based work hadley argue network must generalize information learns one variable binding variable bindings we show temporal synchrony variable binding shastri ajjanagadde inherently generalizes way thereby show temporal synchrony variable binding connectionist architecture accounts systematicity this important step showing connectionism adequate architecture higher level cognition
i describe distance metric called edit distance quantifies syntactic difference two genetic programs in context one specific problem bit multiplexor i use metric analyze amount new material introduced different crossover operators difference among best individuals population difference among best individuals rest population the relationships data run performance imprecise sufficiently interesting encourage encourage investigation use edit distance
both control data dependencies among primitives impact behavioural consistency subprograms genetic programming solutions behavioural consistency turn impacts ability genetic programming identify promote appropriate subprograms we present results modelling dependency parameterized problem subprogram exhibits internal external dependency levels change subprogram successively incorporated larger subsolutions we find key difference nonexistent full external dependency longer time solution identification lower likelihood success shown increased difficulty identifying promoting correct subprograms
in paper compare performance serial parallel island model genetic algorithm solving multiprocessor scheduling problem we show results using fixed scaled problems using using migration we found addition providing speedup use parallel processing parallel island model ga migration finds better quality solutions serial ga
an important reason continued popularity artificial neural networks anns machine learning community gradientdescent backpropagation procedure gives anns locally optimal change procedure addition framework understanding ann learning performance genetic programming gp also successful evolutionary learning technique provides powerful parameterized primitive constructs unlike anns though gp principled procedure changing parts learned system based current performance this paper introduces neural programming connectionist representation evolving programs maintains benefits gp the connectionist model neural programming allows regression creditblame procedure evolutionary learning system we describe general method informed feedback mechanism neural programming internal reinforcement we introduce internal reinforcement procedure demon strate use illustrative experiment
topdown induction decision trees tdidt popular machine learning technique up till mainly used propositional learning seldomly relational learning inductive logic programming the main contribution paper introduction logical decision trees make possible use tdidt inductive logic programming an implementation topdown induction logical decision trees
probabilistic neural networks pnn typically learn quickly many neural network models success variety applications however basic form tend large number hidden nodes one common solution problem keep randomlyselected subset original training data building network this paper presents algorithm called reduced probabilistic neural network rpnn seeks choose betterthanrandom subset available instances use center points nodes network the algorithm tends retain nonnoisy border points removing nodes instances regions input space highly homogeneous in experiments datasets rpnn better average generalization accuracy two pnn models requiring average less onethird number nodes
in standard neuroevolution population networks evolved task network best solves task found this network fixed used solve future instances problem networks evolved way handle realtime interaction well it hard evolve solution ahead time cope effectively possible environments might arise future possible ways someone may interact this paper proposes evolving feedforward neural networks online create agents improve performance realtime interaction this approach demonstrated game world neuralnetworkcontrolled individuals play humans through evolution individuals learn react varying opponents appropriately taking account conflicting goals after initial evaluation offline population allowed evolve online performance improves considerably the population adapts novel situations brought changing strategies opponent game layout also improves performance situations already seen offline training this paper describe implementation online evolution shows practical method exceeds performance offline evolution alone
networks methods results abstract we devise feedforward artificial neural network ann procedure predicting utility loads present resulting predictions two test problems given the great energy predictor shootout the first building data analysis prediction competition key ingredients approach method ffi test determining relevant inputs multilayer perceptron these methods briefly reviewed together comments alternative schemes like fitting polynomials use recurrent networks
in paper first review main results theory schemata genetic programming gp summarise new gp schema theory based new definition schema then study creation propagation disruption new form schemata real runs standard crossover onepoint crossover selection finally discuss results light gp schema theorem
radial basis function rbfs neural networks provide attractive method high dimensional nonparametric estimation use nonlinear control they faster train conventional feedforward networks sigmoidal activation networks backpropagation nets provide model structure better suited adaptive control this article gives brief survey use rbfs introduces new statistical interpretation radial basis functions new method estimating parameters using em algorithm this new statistical interpretation allows us provide confidence limits predictions made using networks
we review main results obtained theory schemata genetic programming gp emphasising strengths weaknesses then propose new simpler definition concept schema gp closer original concept schema genetic algorithms gas along new form crossover onepoint crossover point mutation concept schema used derive improved schema theorem gp describes propagation schemata one generation next we discuss result show schema theorem natural counterpart gp schema theorem
this paper investigates intrinsic limitation worstcase identification lti systems using data corrupted bounded disturbances unknown plant known belong given model set this done analyzing optimal worstcase asymptotic error achievable performing experiments using bounded inputs estimating plant using identification algorithm first shown topological conditions model set identification algorithm asymptotically optimal input characterization optimal asymptotic error function inputs also obtained these results hold error metric disturbance norm second general results applied three specific identification problems identification stable systems norm identification stable rational systems h norm identification unstable rational systems gap metric for problems general characterization optimal asymptotic error used find nearoptimal inputs minimize error
we present connectionist architecture demonstrate learn syntactic parsing corpus parsed text the architecture represent syntactic constituents learn generalizations syntactic constituents thereby addressing sparse data problems previous connectionist architectures we apply simple synchrony networks mapping sequences word tags parse trees after training parsed samples brown corpus networks achieve precision recall constituents approaches statistical methods task
air traffic control involved realtime planning aircraft trajectories this heavily constrained optimization problem we concentrate freeroute planning aircraft required fly way points the choice proper representation realworld problem nontrivial we propose two level representation one level evolutionary operators work derived level calculations furthermore show specific choice fitness function important finding good solutions large problem instances we use hybrid approach sense use knowledge air traffic control using number heuristics we built prototype planning tool resulted flexible tool generating freeroute planning low cost number aircraft
we describe maximumlikelihood parameter estimation problem expectationmaximization em algorithm used solution we first describe abstract form em algorithm often given literature we develop em parameter estimation procedure two applications finding parameters mixture gaussian densities finding parameters hidden markov model hmm ie baumwelch algorithm discrete gaussian mixture observation models we derive update equations fairly explicit detail prove convergence properties we try emphasize intuition rather mathematical rigor
genetic algorithms used neural networks two main ways optimize network architecture train weights fixed architecture while previous work focuses one two options paper investigates alternative evolutionary approach called breeder genetic programming bgp architecture weights optimized simultaneously the genotype network represented tree whose depth width dynamically adapted particular application specifically defined genetic operators the weights trained nextascent hillclimbing search a new fitness function proposed quantifies principle occams razor it makes optimal tradeoff error fitting ability parsimony network simulation results two benchmark problems differing complexity suggest method finds minimal size networks clean data the experiments noisy data show using occams razor improves generalization performance also accel erates convergence speed evolution fl published complex systems
spert synthetic perceptron testbed fully programmable single chip microprocessor designed efficient execution artificial neural network algorithms the first implementation cmos technology mhz clock rate prototype system designed occupy double sbus slot within sun sparcstation spert sustain fi connections per second pattern classification around fi connection updates per second running popular error backpropagation training algorithm this represents speedup around two orders magnitude sparcstation algorithms interest an earlier system produced group ring array processor rap used commercial dsp chips compared rap multiprocessor similar performance spert represents order magnitude reduction cost problems fixedpoint arithmetic satisfactory fl international computer science institute center street berkeley ca
genetic programming method program discovery consisting special kind genetic algorithm capable operating nonlinear chromosomes parse trees representing programs interpreter run programs optimised this paper describes pdgp parallel distributed genetic programming new form genetic programming suitable development finegrained parallel programs pdgp based graphlike representation parallel programs manipulated crossover mutation operators guarantee syntactic correctness offspring the paper describes operators reports preliminary results obtained paradigm
we define fitness structure genetic programming mapping subprograms program respective fitness values this paper shows various fitness structures problem independent subsolutions relate acquisition subsolutions the rate subsolution acquisition found directly correlated fitness structure whether structure uniform linear exponential an understanding fitness structure provides partial insight complicated relationship fitness function outcome genetic programmings search
it argued memorization events situations episodic memory requires rapid formation neural circuits responsive binding errors binding matches while formation circuits responsive binding matches modeled associative learning mechanisms rapid formation circuits responsive binding errors difficult explain given seemingly paradoxical behavior circuit must formed response occurrence binding ie particular pattern input subsequent formation must fire anymore response occurrence binding ie pattern led formation a plausible account formation circuits offered a computational model described demonstrates transient pattern activity representing event lead rapid formation circuits detecting bindings binding errors result longterm potentiation within structures whose architecture circuitry similar hippocampal formation neural structure known critical episodic memory the model exhibits high memory capacity robust limited amounts diffuse cell loss the model also offers alternate interpretation functional role region ca formation episodic memories predicts nature memory impairment would result damage various regions hippocampal formation
specialization populations artificial neural networks studied organisms fixed evolvable fitness formulae placed isolated shared environments emerged behaviors compared an evolvable fitness formula specifies evaluation measure let free evolve obtain coevolution expressed behavior individual evolvable fitness formula in isolated environment generalist behavior emerges organisms fixed fitness formula specialist behavior emerges organisms individual evolvable fitness formulae a population diversification analysis shows almost organisms population isolated environment converge towards behavioral strategy find competition act provide population diversification populations organisms shared environment
clones objectoriented library constructing training utilizing layered connectionist networks the clones library contains object classes needed write simulator small amount added source code examples included the size experimental ann programs greatly reduced using objectoriented library time programs easier read write evolve the library includes database network behavior training procedures customized user it designed run efficiently data parallel computers rap spert well uniprocessor workstations while efficiency portability parallel computers primary goals several secondary design goals allow heterogeneous algorithms training procedures interconnected trained together within constraints attempt maximize variety artificial neural net work algorithms supported
technical report csrp august abstract genetic programming method program discovery consisting special kind genetic algorithm capable operating parse trees representing programs interpreter run programs optimised this paper describes parallel distributed genetic programming pdgp new form genetic programming suitable development parallel programs symbolic neural processing elements combined free natural way pdgp based graphlike representation parallel programs manipulated crossover mutation operators guarantee syntactic correctness offspring the paper describes operators reports results obtained exclusiveor problem
there much interest using optics implement computer interconnection networks however little discussion routing methodologies besides already used electronics in paper neural network routing methodology proposed generate control bits optical multistage interconnection network omin though present optical implementation methodology illustrate control optical interconnection network these omins may used communication media shared memory distributed computing systems the routing methodology makes use artificial neural network ann functions parallel computer generating routes the neural network routing scheme may applied electrical well optical interconnection networks however since ann implemented using optics routing approach especially appealing optical computing environment the parallel nature ann computation may make routing scheme faster conventional routing approaches especially omins irregular furthermore neural network routing scheme faulttolerant results shown generating routes fi stage omin
the multispert parallel system straightforward extension spert workstation accelerator predominantly used speech recognition research icsi in order deliver high performance artificial neural network training without requiring changes user interfaces exisiting quicknet ann library modified run multispert in report present algorithms used parallelization quicknet code analyse communication computation requirements the resulting performance model yields better understanding system speedups potential bottlenecks experimental results actual training runs validate model demonstrate achieved performance levels
in paper explore distributed database allocation problem intractable we also discuss genetic algorithms used successfully solve combinatorial problems our experimental results show ga far superior greedy heuristic obtaining optimal near optimal fragment placements allocation problem various data sets
stefanwrobelgmdde sasodzeroskigmdde proc fgml annual workshop gi special interest group machine learning gi fg ed k morik j herrmann research report univdortmund abstract the task discovering interesting regularities large sets data data mining knowledge discovery recently met increased interest machine learning general inductive logic programming ilp particular however widely accepted definition task concept learning examples ilp definitions data mining task proposed recently in paper examine socalled nonmonotonic semantics definitions show nonmonotonicity incidental property data mining learning task task makes perfect sense without assumption we therefore introduce define generalized definition data mining task called ilp description learning problem discuss properties relation traditional concept learning prediction learning problem since characterization entirely level models definition applies independently chosen hypothesis language
optoelectronic reconfigurable interconnection networks limited significant control latency used large multiprocessor systems this latency time required analyze current traffic reconfigure network establish required paths the goal latency hiding minimize effect control overhead in paper introduce technique performs latency hiding learning patterns communication traffic using information anticipate need communication paths hence network provides required communication paths request path made in study communication patterns memory accesses parallel program used input time delay neural network tdnn perform online training prediction these predicted communication patterns used interconnection network controller provides routes memory requests based experiments neural network able learn highly repetitive communication patterns thus able predict allocation communication paths resulting reduction communication latency
technical report umiacstr cstr institute advanced computer studies university maryland college park md abstract shared memory multiprocessors require reconfigurable interconnection networks ins scalability these ins reconfigured in control unit however ins often plagued undesirable reconfiguration time primarily due control latency amount time delay control unit takes decide desired new in configuration to reduce control latency trainable prediction unit pu devised added in controller the pus job anticipate reduce control configuration time major component control latency three different online prediction techniques tested learn predict repetitive memory access patterns three typical parallel processing applications d relaxation algorithm matrix multiply fast fourier transform the predictions used routing control algorithm reduce control latency configuring in provide needed memory access paths requested three prediction techniques used tested markov predictor linear predictor time delay neural network tdnn predictor as expected different predictors performed best different applications however tdnn produced best overall results
in paper explore distributed file task placement problem intractable we also discuss genetic algorithms used successfully solve combinatorial problems our experimental results show ga far superior greedy heuristic obtaining optimal near optimal file task placements problem various data sets
in paper show posterior distribution feedforward neural networks asymptotically consistent this paper extends earlier results universal approximation properties neural networks bayesian setting the proof consistency embeds problem density estimation problem uses bounds bracketing entropy show posterior consistent hellinger neighborhoods it relates result back regression setting we show consistency setting number hidden nodes growing sample size case number hidden nodes treated parameter thus provide theoretical justification using neural networks nonparametric regression bayesian framework
this paper describes interactive planning system developed inside intelligent decision support system aimed supporting operator planning initial attack forest fires the planning architecture rests integration casebased reasoning techniques constraint reasoning techniques exploited mainly performing temporal reasoning temporal metric information temporal reasoning plays central role supporting interactive functions provided user performing two basic steps planning process plan adaptation resource scheduling a first prototype integrated situation assessment resource allocation manager subsystem currently tested
prepruning postpruning two standard methods dealing noise concept learning prepruning methods efficient postpruning methods typically accurate much slower generate overly specific concept description first we experimented variety pruning methods including two new methods try combine integrate pre postpruning order achieve accuracy efficiency this verified test series chess position classification task
pruning effective method dealing noise machine learning recently pruning algorithms particular reduced error pruning also attracted interest field inductive logic programming however shown methods inefficient time wasted generating clauses explain noisy examples subsequently pruning clauses we introduce new method searches good theories topdown fashion get better starting point pruning algorithm experiments show approach significantly lower complexity task without losing predictive accuracy
traditional databases commonly support efficient query update procedures operate time sublinear size database our goal paper take first step toward dynamic reasoning probabilistic databases comparable efficiency we propose dynamic data structure supports efficient algorithms updating querying singly connected bayesian networks in conventional algorithm new evidence absorbed time o queries processed time on n size network we propose algorithm preprocessing phase allows us answer queries time olog n expense olog n time per evidence absorption the usefulness sublinear processing time manifests applications requiring near realtime response large probabilistic databases we briefly discuss potential application dynamic probabilistic reasoning computational biology
network often however application need information every node network need exact probabilities we present localized partial evaluation lpe propagation algorithm computes interval bounds marginal probability specified query node examining subset nodes entire network conceptually lpe ignores parts network far away queried node much impact value lpe anytime property able produce better solutions tighter intervals given time consider network
we describe integrated problem solving architecture named inbanca bayesian networks casebased reasoning cbr work cooperatively multiagent planning tasks this includes twoteam dynamic tasks paper concentrates simulated soccer example bayesian networks used characterize action selection whereas casebased approach used determine implement actions this paper two contributions first survey integrations casebased bayesian approaches perspective popular cbr task decomposition framework thus explaining types integrations attempted this allows us explain unique aspects proposed integration second demonstrate bayesian nets used provide environmental context thus feature selection information casebased reasoner
machine learning systems often represent concepts rules sets attributevalue pairs many learning algorithms generalize specialize concept representations removing adding pairs thus concepts created general specific relationships this paper presents algorithms connect concepts network based general specific relationships since concept access related concepts quickly resulting structure allows increased efficiency learning reasoning the time complexity one set learning models improves on log n olog n n number nodes using general specific structure
this paper analyzes convergence properties canonical genetic algorithm cga mutation crossover proportional reproduction applied static optimization problems it proved means homogeneous finite markov chain analysis cga never converge global optimum regardless initialization crossover operator objective function but variants cgas always maintain best solution population either selection shown converge global optimum due irreducibility property underlying original nonconvergent cga these results discussed respect schema theorem
many techniques developed learning rules relationships automatically diverse data sets simplify often tedious errorprone process acquiring knowledge empirical data while techniques plausible theoretically wellfounded perform well less artificial test data sets depend ability make sense realworld data this paper describes project applying range machine learning strategies problems agriculture horticulture we briefly survey techniques emerging machine learning research describe software workbench experimenting variety techniques realworld data sets describe case study dairy herd management culling rules inferred mediumsized database herd information
decisiontheoretic preferences specify relative desirability possible outcomes alternative plans in order express general patterns preference holding domain require language refer directly preferences classes outcomes well individuals we present basic concepts theory meaning generic comparatives facilitate incremental capture exploitation automated reasoning systems our semantics lifts comparisons individuals comparisons classes things equal means contextual equivalences equivalence relations among individuals vary context application we discuss implications theory represent ing preference information
the baldwin effect first proposed late nineteenth century suggests course evolutionary change influenced individually learned behavior the existence effect still hotly debated topic in paper clear evidence presented learningbased plasticity phenotypic level produce directed changes genotypic level this research confirms earlier experimental work done others notably hinton nowlan further amount plasticity learned behavior shown crucial size baldwin effect either little much effect disappears significantly reduced finally learnable traits case made many generations become easier population whole learn traits ie phenotypic plasticity traits increase in gradual transition genetically driven population one driven learning importance baldwin effect decreases
this article presents new line research investigating online learning mechanisms autonomous intelligent agents we discuss casebased method dynamic selection modification behavior assemblages navigational system the casebased reasoning module designed addition traditional reactive control system provides flexible performance novel environments without extensive highlevel reasoning would otherwise slow system the method implemented acbarr a casebased reactive robotic system evaluated empirical simulation system several different environments including box canyon environments known problematic reactive control systems general fl technical report gitcc college computing georgia institute technology atlanta geor gia
sg specific general network supervised inductive learning examples uses ideas neural networks symbolic inductive learning gain benefits methods the network built many simple nodes learn important features input space monitor ability features predict output values the network avoids exponential nature number features creating specific features example expanding features making general expansion feature terminates encounters another feature contradicting outputs empirical evaluation model realworld data shown network provides good generalization performance convergence accomplished within small number training passes the network provides benefits automatically allocating deleting nodes without requiring user adjustment parameters the network learns incrementally operates parallel fashion this paper describes network architecture supervised learning combines techniques used neural networks symbolic machine learning gain advantages approaches in supervised learning network given training set containing examples each example gives input pattern along corresponding output network produce presented input the task network converge representation contains information given training set generalize information network respond well inputs trained one approach generalization look important features input space a feature subset network inputs along associated values a feature matched values network inputs part feature equal values inputs given feature inputs part feature value a feature predicts output high probability important feature the number inputs contained feature order feature determines generality feature a feature inputs general feature feature many inputs specific feature it impractical monitor possible input features number features exponential number inputs this paper proposes sg specific general network creates specific input features generalizes features one way sg generalizes combining similar specific features if two features similar close input space combining two features dropping inputs common features creates new feature encompasses original features the new feature general matches points input space defined example this section presents overview model later sections provide detail system the network made many simple nodes each node contains input feature monitors during training node gathers statistics giving discrete conditional probability possible output value given input feature
we provide analytical expressions governing changes bias variance lookup table estimators provided various monte carlo temporal difference value estimation algorithms oine updates trials absorbing markov reward processes we used expressions develop software serves analysis tool given complete description markov reward process rapidly yields exact meansquareerror curve curve one would get averaging together sample meansquareerror curves infinite number learning trials given problem we use analysis tool illustrate classes meansquareerror curve behavior variety example reward processes show although various temporal difference algorithms quite sensitive choice stepsize eligibilitytrace parameters values parameters make similarly competent generally good
we examine inductive inference complex grammar specifically consider task training model classify natural language sentences grammatical ungrammatical thereby exhibiting kind discriminatory power provided principles parameters linguistic framework governmentandbinding theory we investigate following models feedforward neural networks fransconigorisoda backtsoi locally recurrent networks elman narendra parthasarathy williams zipser recurrent networks euclidean editdistance nearestneighbors simulated annealing decision trees the feedforward neural networks nonneural network machine learning models included primarily comparison we address question how neural network distributed nature gradient descent based iterative calculations possess linguistic capability traditionally handled symbolic computation recursive processes initial simulations models partially successful using large temporal window input models trained fashion learn grammar significant degree attempts training recurrent networks small temporal input windows failed implemented several techniques aimed improving convergence gradient descent training algorithms we discuss theory present empirical study variety models learning algorithms highlights behaviour present attempting learn simpler grammar
a parallel version proposed fundamental theorem serial unconstrained optimization the parallel theorem allows k parallel processors use simultaneously different algorithm descent newton quasinewton conjugate gradient algorithm each processor perform one many steps serial algorithm portion gradient objective function assigned independently processors eventually synchronization step performed differentiable convex functions consists taking strong convex combination k points found k processors for nonconvex well convex differentiable functions best point found k processors taken better point the fundamental result establish accumulation point parallel algorithm stationary nonconvex case global solution convex case computational testing thinking machines cm multiprocessor indicate speedup order number processors employed
sensors represent crucial link evolutionary forces shaping species relationship environment individuals cognitive abilities behave learn we report experiments using new class latent energy environments lee models define environments carefully controlled complexity allow us state bounds random optimal behaviors independent strategies achieving behaviors using lees analytic basis defining environments use neural networks nnets model individuals steady state genetic algorithm model evolutionary process shaping nnets particular sensors our experiments consider two types contact ambient sensors variants nnets allowed learn learn via error correction internal prediction via reinforcement learning we find predictive learning even using larger repertoire sophisticated ambient sensors provides advantage nnets unable learn however reinforcement learning using small number crude contact sensors provide significant advantage our analysis results points tradeoff genetic robustness sensors informativeness learning system
this brief annotated bibliography i wanted make available attendees machine learning tutorial ai statistics workshop these slides available www pages slides please contact questions please also note date listed recently updated while i plan make occasional updates file bound outdated quickly also i apologize lack figures time project limited slides compensate finally bibliography definition this book date both pat langley tom mitchell process writing textbooks subject still waiting until i suggest looking readings recent ml conference proceedings international european there also introductory papers subject though i havent gotten around putting yet however pat langley dennis kibler written good paper ml empirical science pat written several editorials use ml author langley incomplete ive left many references may use
a bayesian approach multivariate adaptive regression spline mars fitting friedman proposed this takes form probability distribution space possible mars models explored using reversible jump markov chain monte carlo methods green the generated sample mars models produced shown good predictive power averaged allows easy interpretation relative importance predictors overall fit
resent allowed sequences resolution steps initial theory there however many characterizations allowed sequences resolution steps expressed set resolvents one approach problem presented system merlin based earlier technique learning finitestate automata represent allowed sequences resolution steps merlin extends previous technique three ways negative examples considered addition positive examples ii new strategy performing generalization used iii technique converting learned automaton logic program included results experiments presented merlin outperforms system using old strategy performing generalization traditional covering technique the latter result explained limited expressiveness hypotheses produced covering also fact covering needs produce correct base clauses recursive definition
we discuss ideas producing perfect simulations based coupling past finite state space models naturally extend multivariate distributions infinite uncountable state spaces autogamma autopoisson autonegativebinomial models using gibbs sampling combination sandwiching methods originally introduced perfect simulation point processes
the main result paper establishes equivalence null asymptotic controllability nonlinear finitedimensional control systems existence continuous controllyapunov functions clfs defined means generalized derivatives in manner one obtains complete characterization asymptotic controllability applying principle far wider class systems artsteins theorem relates closedloop feedback stabilization existence smooth clfs the proof relies viability theory optimal control techniques introduction in paper study systems general form
we apply recent results minimax risk density estimation related problem pattern classification the notion loss seek minimize information theoretic measure well predict classification future examples given classification previously seen examples we give asymptotic characterization minimax risk terms metric entropy properties class distributions might generating examples we use results characterize minimax risk special case noisy twovalued classification problems terms assouad density
genetic algorithms gas extensively used different domains means global optimization simple yet reliable manner they much better chance getting global optima gradient based methods usually converge local sub optima however gas tendency getting moderately close optima small number iterations to get close optima ga needs large number iterations whereas gradient based optimizers usually get close local optima relatively small number iterations in paper describe new crossover operator designed endow ga gradientlike abilities without actually computing gradients without sacrificing global optimality the operator works using guidance members ga population select direction exploration empirical results two engineering design domains across binary floating point representations demonstrate operator significantly improve steady state error ga optimizer
neural networks trained balancing poles attached cart fixed track for one variant single pole system pole angle cart position variables supplied inputs network must learn compute velocities all problems solved using fixed architecture using new version cellular encoding evolves application specific architecture realvalued weights the learning times generalization capabilities compared neural networks developed using methods after post processing simplification topologies produced cellular encoding simple could analyzed architectures hidden units produced single pole two pole problem velocity information supplied input moreover linear solutions display good generalization for control problems cellular encoding automatically generate architectures whose complexity structure reflect features problem solve
a recent result jun lius shown compute explicitly eigenvalues eigenvectors markov chain derived special case hastings sampling algorithm known indepdendence metropolis sampler in note show extend result obtain exact nstep transition probabilities n this done first chain finite state space extended general discrete continuous state space the paper concludes implications diagnostic tests convergence markov chain samplers
it well known searchspace reformulation improve speed reliability numerical optimization engineering design we argue best choice reformulation depends design goal present technique automatically constructing rules map design goal reformulation chosen space possible reformulations we tested technique domain racingyachthull design reformulation corresponds incorporating constraints search space we applied standard inductivelearning algorithm c set training data describing constraints active optimal design goal encountered previous design session we used rules choose appropriate reformulation set test cases our experimental results show using reformulations improves speed reliability design optimization outperforming competing methods approaching best performance possible
apply reduction two core children total sum matching weights becomes on comparison spine node critical node apply reduction core child spine node total sum matching weights becomes on with regards o comparisons two critical nodes sum exceed o n total weight thus since total on edges involved matchings time on reduce total sum matching weights o n theorem let m ir ir monotone function bounding time complexity uwbm moreover let m satisfy m x x fx constant f x ox f monotone constants b b x b fxy b f xf then p time on m n proof we spend on polylog n timeuwbmk n matchings so theorem compcoretrees computed time on polylog n timeuwbmk n applying theo get corollary mast computable time on log n whe day computational complexity inferring phylogenies dissimilarity matrices bulletin mathematical biology m farach s kannan t warnow a robust model finding optimal evolutionary trees algorithmica in press see also stoc
we report results vowel stop consonant recognition tokens extracted timit database our current system differs others similar tasks use specific time normalization techniques we use detailed biologically motivated input representation speech tokens lyons cochlear model implemented slaney this detailed high dimensional representation known cochleagram classified either backpropagation hybrid supervisedunsupervised neural network classifier the hybrid network composed biologically motivated unsupervised network supervised backpropagation network this approach produces results comparable obtained others without addition time normalization
this paper presents new application exclusivesumofproducts esop minimizer exorcismmv machine learning particularly pattern theory an analysis various logic synthesis programs conducted wright laboratory machine learning applications creating robust efficient boolean minimizer machine learning would minimize decomposed function cardinality dfc measure functions would help solve practical problems application areas interest pattern theory group especially problems require strongly unspecified multiplevaluedinput functions large number variables for many functions complexity minimization exorcismmv better espresso for small functions worse curtislike decomposer however exorcism much faster run problems variables significant dfc improvements also found we analyze cases exorcism worse espresso propose new improvements strongly unspecified functions
two selforganising controller networks presented study the clustered controller network ccn uses spatial clustering approach select controllers instant in gated controller network modelscontroller network mcn performance model attached controller used achieve controller selection an algorithm automaticly conctrust architecture networks described it makes two schemes selforganising different examples control nonlinear systems considered order illustrate behaviour iccn imcn it makes clear schemes performing much better single adaptive controller the two main advantages iccn imcn concern possibilities use controller building block network architecture apply iccn modelling purpose however iccn appears serious problems cope nonlinear systems single variable implying nonlinear behaviour the imcn suffer trouble this high sensitivity clustering space order main drawback limiting use iccn therefore makes imcn much suitable approach control wide range nonlinear systems
this paper offers perspective features pattern finding general this perspective based robust complexity measure called decomposed function cardinality a function decomposition algorithm minimizing complexity measure finding associated features outlined results experiments algorithm also summarized
we investigate problem estimating proportion vector maximizes likelihood given sample mixture given densities we adapt framework developed supervised learning give simple derivations many standard iterative algorithms like gradient projection em in framework distance new old proportion vectors used penalty term the square distance leads gradient projection update relative entropy new update call exponentiated gradient update eg curiously second order taylor expansion relative entropy used arrive update em gives usual em update experimentally em update eg update gt outperform em algorithm variants we also prove polynomial bound rate convergence eg algorithm
this paper compares direct reinforcement learning explicit model modelbased reinforcement learning simple task pendulum swing we find task modelbased approaches support reinforcement learning smaller amounts training data efficient handling changing goals
this article compares traditional fixed problem representation style genetic algorithm ga new floating representation building blocks problem fixed specific locations individuals population in addition effects noncoding segments representations studied noncoding segments computational model noncoding dna floating building blocks mimic location independence genes the fact structures prevalent natural genetic systems suggests may provide advantages evolutionary process our results show significant difference gas solve problem fixed floating representations gas able maintain diverse population floating representation the combination noncoding segments floating building blocks appears encourage ga take advantage parallel search recombination abilities
we extend hoeffding bounds develop superior probabilistic performance guarantees accurate classifiers the original hoeffding bounds classifier accuracy depend accuracy parameter since accuracy known priori parameter value gives weakest bounds used we present method loosely bounds accuracy using old method uses loose bound improved parameter value tighter bounds we show use bounds practice generalize bounds individual classifiers form uniform bounds multiple classifiers
evolving cooperative groups preliminary results abstract multiagent systems require coordination sources distinct expertise perform complex tasks effectively in paper use coevolutionary approach using genetic algorithms evolve multiple individuals effectively cooperate solve common problem we concurrently run ga individual group in paper experiment room painting domain requires cooperation two agents we used two mechanisms evaluating individual one population pair randomly members population b pair members population shared memory containing best pairs found far both approaches successful generating optimal behavior patterns however preliminary results exhibit slight edge shared memory approach
coevolution refers simultaneous evolution two genetically distinct populations coupled fitness landscapes in paper consider competitive coevolution fitness individual host population based direct competition individuals parasite population competitive coevolution applied three gamelearning problems tictactoe ttt nim small version go two new techniques competitive coevolution explored competitive fitness sharing changes way fitness measured shared sampling alters way parasites chosen testing hosts experiments using ttt nim show substantial improvement performance methods used preliminary results using coevolution discovery cellular automata rules playing go presented
we review use global local methods estimating function mapping r r n samples function containing noise the relationship methods examined empirical comparison performed using multilayer perceptron mlp global neural network model single nearestneighbour model linear local approximation la model following commonly used datasets mackeyglass chaotic time series sunspot time series british english vowel data timit speech phonemes building energy prediction data sonar dataset we find simple local approximation models often outperform mlp no criterion classificationprediction size training set dimensionality training set etc used distinguish whether mlp local approximation method superior however find consider histograms knn density estimates training datasets choose best performing method priori selecting local approximation spread density histogram large choosing mlp otherwise this result correlates hypothesis global mlp model less appropriate characteristics function approximated varies throughout input space we discuss results smoothness assumption often made function approximation biasvariance dilemma
we present implementation kohonen selforganizing feature maps spertii vector microprocessor system the implementation supports arbitrary neural map topologies arbitrary neighborhood functions for small networks used realworld tasks single spertii board measured run kohonen net classification million connections per second mcps on speech coding benchmark task spertii performs online kohonen net training million connection updates per second mcups this represents almost factor improvement compared previously reported implementations the asymptotic peak speed system mcps mcups
despite fact complex visual scenes contain multiple overlapping objects people perform object recognition ease accuracy one operation facilitates recognition early segmentation process features objects grouped labeled according object belong current computational systems perform operation based predefined grouping heuristics we describe system called magic learns group features based set presegmented examples in many cases magic discovers grouping heuristics similar previously proposed also capability finding nonintuitive structural regularities images grouping performed relaxation network attempts dynamically bind related features features transmit complexvalued signal amplitude phase one another binding thus represented phase locking related features magics training procedure generalization recurrent back propagation complexvalued units
the simple bayesian classifier sbc commonly thought assume attributes independent given class apparently contradicted surprisingly good performance exhibits many domains contain clear attribute dependences no explanation proposed far in paper show sbc fact assume attribute independence optimal even assumption violated wide margin the key finding lies distinction classification probability estimation correct classification achieved even probability estimates used contain large errors we show previouslyassumed region optimality sbc secondorder infinitesimal fraction actual one this followed derivation several necessary several sufficient conditions optimality sbc for example sbc optimal learning arbitrary conjunctions disjunctions even though violate independence assumption the paper also reports empirical evidence sbcs competitive performance domains containing substantial degrees attribute dependence
we propose method use inductive logic programming give heuristic functions searching goals solve problems the method takes solutions problem history search set background knowledge problem in large class problems problem described set states set operators solved finding series operators a solution series operators brings initial state final state transformed positive negative examples relation betterchoice describes operator better others state we also give way use betterchoice relation heuristic function the method use logic program background knowledge induce heuristics induced heuristics high readability the paper inspects method applying puzzle
we describe development monitoring system uses sensor observation data discrete events construct dynamically probabilistic model world this model bayesian network incorporating temporal aspects call dynamic belief network used reason uncertainty causes consequences events monitored the basic dynamic construction network datadriven however model construction process combines sensor data events externally provided information agents behaviour knowledge already contained within model control size complexity network this means network structure within time interval amount history detail maintained vary time we illustrate system example domain monitoring robot vehicles people restricted dynamic environment using lightbeam sensor data in addition presenting generic network structure monitoring domains describe use complex network structures address two specific monitoring problems sensor validation data association problem
we evaluate power decision tables hypothesis space supervised learning algorithms decision tables one simplest hypothesis spaces possible usually easy understand experimental results show artificial realworld domains containing discrete features idtm algorithm inducing decision tables sometimes outperform stateoftheart algorithms c surprisingly performance quite good datasets continuous features indicating many datasets used machine learning either require features features values we also describe incremental method performing crossvalidation applicable incremental learning algorithms including idtm using incremental crossvalidation possible crossvalidate given dataset idtm time linear number instances number features number label values the time incremental crossvalidation independent number folds chosen hence leaveoneout crossvalidation tenfold crossvalidation take time
in wrapper approach feature subset selection search optimal set features made using induction algorithm black box the estimated future performance algorithm heuristic guiding search statistical methods feature subset selection including forward selection backward elimination stepwise variants viewed simple hillclimbing techniques space feature subsets we utilize bestfirst search find good feature subset discuss overfitting problems may associated searching many feature subsets we introduce compound operators dynamically change topology search space better utilize information available evaluation feature subsets we show compound operators unify previous approaches deal relevant irrelevant features the improved feature subset selection yields significant improvements realworld datasets using id naivebayes induction algorithms
we constructed inexpensive videobased motorized tracking system learns track head it uses real time graphical user inputs auxiliary infrared detector supervisory signals train convolutional neural network the inputs neural network consist normalized luminance chrominance images motion information frame differences subsampled images also used provide scale invariance during online training phase neural network rapidly adjusts input weights depending upon reliability different channels surrounding environment this quick adaptation allows system robustly track head even objects moving within cluttered background
in paper consider complexity number combinatorial problems namely intervalizing colored graphs dna physical mapping triangulating colored graphs perfect phylogeny directed modified colored cutwidth feasible register assignment module allocation graphs bounded treewidth each problems characteristic uniform upper bound tree path width graphs yesinstances for problems exceptions feasible register assignment module allocation vertex edge coloring given part input our main results parameterized variant considered problems hard complexity classes w z we also show intervalizing colored graphs triangulating colored graphs
it wellknown certain learning methods eg perceptron learning algorithm acquire complete parity mappings but often overlooked stateoftheart learning methods c backpropagation generalise incomplete parity mappings the failure methods generalise parity mappings may sometimes dismissed grounds impossible generalise mappings parity problems mathematical constructs little realworld learning however paper argues dismissal unwarranted it shows parity mappings hard learn statistically neutral statistical neutrality property expect encounter frequently realworld contexts it also shows generalization failure parity mappings occurs even large minimally incomplete mappings used training purposes ie claims impossibility generalization particularly suspect
there several stochastic methods used solving nphard optimization problems approximatively examples algorithms include order increasing computational complexity stochastic greedy search methods simulated annealing genetic algorithms we investigate methods likely give best performance practice respect computational effort requires we study problem empirically selecting set stochastic algorithms varying computational complexity experimentally evaluating method goodness results achieved improves increasing computational time for evaluation use graph optimization problem closely related several realworld practical problems to get wider perspective goodness achieved results stochastic methods also compared specialcase greedy heuristics this investigation suggests although genetic algorithms provide good results simpler stochastic algorithms achieve similar performance quickly
there two generations gibbs sampling methods semiparametric models involving dirichlet process the first generation suffered severe drawback namely locations clusters groups parameters could essentially become fixed moving rarely two strategies proposed create second generation gibbs samplers integration appending second stage gibbs sampler wherein cluster locations moved we show strategies easily implemented sequential importance sampler first strategy dramatically improves results as case gibbs sampling strategies applicable much wider class models they shown provide uniform importance sampling weights lead additional raoblackwellization estimators steve maceachern associate professor department statistics ohio state university merlise clyde assistant professor institute statistics decision sciences duke university jun liu assistant professor department statistics stanford university the work second author supported part national science foundation grants dms dms last author national science foundation grants dms dms terman fellowship
we show uniform prior hypothesis functions training error early stopping fixed training error training error minimum results increase expected generalization error we also show regularization methods equivalent early stopping certain nonuniform prior early stopping solutions
we present unified framework convergence analysis generalized subgradienttype algorithms presence perturbations one principal novel features analysis perturbations need tend zero limit it established iterates algorithms attracted certain sense stationary set problem depends magnitude perturbations characterization attraction sets given general nonsmooth nonconvex case the results strengthened convex weakly sharp strongly convex problems our analysis extends unifies previously known results convergence stability properties gradient subgradient methods including incremental parallel heavy ball modifications fl the first author supported part cnpq grant research second author supported part international science foundation grant nby international science foundation russian goverment grant nby russian foundation fundamental research grant n instituto de matematica pura e aplicada estrada dona castorina jardim botanico rio de janeiro rj cep brazil email solodovimpabr z operations research department faculty computational mathematics cybernetics moscow state university moscow russia
dr mccalleys research partially supported grants national science foundation pacific gas electric company dr honavars research partially supported grants national science foundation john deere foundation this paper appear proceedings th annual north american power symposium oct laramie wyoming
coevolutionary learning involves embedding adaptive learning agents fitness environment dynamically responds progress potential solution many technological chicken egg problems heart several recent surprising successes sims artificial robot tesauros backgammon player we recently solved two spirals problem difficult neural network benchmark classification problem using genetic programming primitives set koza instead using absolute fitness use relative fitness angeline pollack based competition coverage data set as population reproduces fitness function driving selection changes subproblem niches opened rather crowded the solutions found method symbiotic structure suggests holding niches open crossover better able discover modular build ing blocks
we show two cooperating robots learn exactly stronglyconnected directed graph n indistinguishable nodes expected time polynomial n we introduce new type homing sequence two robots helps robots recognize certain previouslyseen nodes we present algorithm robots learn graph homing sequence simultaneously actively wandering graph unlike previous learning results using homing sequences algorithm require teacher provide counterexamples furthermore algorithm use efficiently additional information available distinguishes nodes we also present algorithm robots learn taking random walks the rate random walk graph converges stationary distribution characterized conductance graph our randomwalk algorithm learns expected time polynomial n inverse conductance efficient homingsequence algorithm highconductance graphs
we introduce new model learning membership queries queries near boundary target concept may receive incorrect dont care responses in partial compensation assume distribution examples zero probability mass boundary region the motivation behind model reason incorrect dont care response examples extremely rare practice thus matter learner classifies we present several positive results new model we show learn intersection two halfspaces membership queries near boundary may answered incorrectly our algorithm extension algorithm baum learns intersections two homogeneous halfspaces pacwithmembershipqueries model we also describe algorithms learning several subclasses monotone dnf formulas
to understand interspike interval isi variability displayed visual cortical neurons softky koch critical examine dynamics neuronal integration well variability synaptic input current most previous models focused latter factor we match simple integrateandfire model experimentally measured integrative properties cortical regular spiking cells mccormick et al after setting rc parameters postspike voltage reset set match experimental measurements neuronal gain obtained vitro plots firing frequency vs injected current examination resulting model leads intuitive picture neuronal integration unifies seemingly contradictory p n arguments hold spiking regular memory last spike becomes negligible spike threshold crossing caused input variance around steady state spiking poisson in integrateandfire neurons matched cortical cell physiology steady state behavior predominant isis highly variable physiological firing rates wide range inhibitory excitatory inputs
this paper describes argumentation system cooperative design applications web the system provides experts involved procedures means expressing weighing individual arguments preferences order argue selection certain choice it supports defeasible qualitative reasoning presence illstructured information argumentation performed set discourse acts call variety procedures propagation information corresponding discussion graph the paper also reports integration case based reasoning techniques used resolve current design issues considering previous similar situations specitcation similarity measures various argumentation items aim estimate variations among opinions designers involved cooperative design
this paper describes new efficient algorithms learning deterministic finite automata our approach primarily distinguished two features adoption averagecase setting model typical labeling finite automaton retaining worstcase model underlying graph automaton along learning model learner provided means experiment machine rather must learn solely observing automatons output behavior random input sequence the main contribution paper presenting first efficient algorithms learning nontrivial classes automata entirely passive learning model we adopt online learning model learner asked predict output next state given next symbol random input sequence goal learner make prediction mistakes possible assuming learner means resetting target machine fixed start state first present efficient algorithm makes expected polynomial number mistakes model next show first algorithm used subroutine second algorithm also makes polynomial number mistakes even absence reset along way prove number combinatorial results randomly labeled automata we also show labeling states bits input sequence need truly random merely semirandom finally discuss extension results model automata used represent distributions binary strings
this paper presents comparison genetic programminggp simulated annealing sa stochastic iterated hill climbing sihc based suite program discovery problems previously tackled gp all three search algorithms employ hierarchical variable length representation programs brought recent prominence gp paradigm we feel intuitively obvious mutationbased adaptive search handle program discovery yet date gp problem tried sa sihc also work
given markov chain sampling scheme standard empirical estimator make best use data we show construct better estimators we restrict attention nearest neighbor random fields gibbs samplers deterministic sweep approach applies sampler uses reversible variableatatime updating deterministic sweep the structure transition distribution sampler exploited construct empirical estimators combined standard empirical estimator reduce asymptotic variance the extra computational cost negligible when random field spatially homogeneous symmetrizations estimator lead variance reduction the performance estimators evaluated simulation study ising model
in order learning improve adaptiveness animals behavior thus direct evolution way baldwin suggested learning mechanism must incorporate innate evaluation animals actions influence reproductive fitness for example many circumstances damage animal otherwise reduce fitness painful tend avoided we refer mechanism animal evaluates fitness consequences actions motivation system argue system must evolve along behaviors evaluates we describe simulations evolution populations agents instantiating number different architectures generating action learning worlds differing complexity we find cases members populations evolve motivation systems accurate enough direct learning increase fitness actions agents perform furthermore motivation systems tend incorporate systematic distortions representations worlds inhabit distortions increase adaptiveness behavior generated
the relation orthography phonology language traditionally modelled handcrafted rule sets machinelearning ml approaches offer means gather knowledge automatically problems arise training material sparse generalising sparse data wellknown problem many ml algorithms we present experiments connectionist instancebased decisiontree learning algorithms applied small corpus scottish gaelic instancebased learning ibig algorithm yields best generalisation performance algorithms tested perform tolerably well given availability lexicon even sparse ml valuable efficient tool automatic phonetic transcription written text
high performance compilers increasingly rely accurate modeling machine resources efficiently exploit instruction level parallelism application in paper propose reduced machine description results faster detection resource contentions preserving scheduling constraints present original machine description the proposed approach reduces machine description automated errorfree efficient fashion moreover fully supports schedulers backtrack process operations arbitrary order reduced descriptions dec alpha mips rr cydra result times faster detection resource contentions require memory storage used original machine descriptions
we study problem estimating log spectrum stationary gaussian time series thresholding empirical wavelet coefficients we propose use thresholds jn depending sample size n wavelet basis resolution level j at fine resolution levels j propose the purpose thresholding level make reconstructed logspectrum nearly noisefree possible in addition pleasant visual point view noisefree character leads attractive theoretical properties wide range smoothness assumptions previous proposals set much smaller thresholds enjoy properties jn ff j log n
data mining algorithms including machine learning statistical analysis pattern recognition techniques greatly improve understanding data warehouses becoming widespread in paper focus classification algorithms review need multiple classification algorithms we describe system called mlc designed help choose appropriate classification algorithm given dataset making easy compare utility different algorithms specific dataset interest mlc provides workbench comparisons also provides library c classes aid development new algorithms especially hybrid algorithms multistrategy algorithms such algorithms generally hard code scratch we discuss design issues interfaces programs visualization resulting classifiers
reinforcement learning methods applied control problems objective optimizing value function time they used train single neural networks learn solutions whole tasks jacobs jordan shown set expert networks combined via gating network quickly learn tasks decomposed even decomposition learned inspired boyans work modular neural networks learning temporaldifference methods modify reinforcement learning algorithm called qlearning train modular neural network solve control problem the resulting algorithm demonstrated classical polebalancing problem the advantage method makes possible deal complex dynamic control problem effectively using task decomposition competitive learning
this report replicates extends results reported naval air warfare center nawc personnel automatic classification sonar images they used novel casebased reasoning systems empirical studies obtain comparative analyses using standard classification algorithms therefore quality nawc results unknown we replicated nawc studies also tested several classifiers ie casebased otherwise machine learning literature these comparisons ramifications detailed paper next investigated fala walkers two suggestions future work ie combining similarity functions alternative case representation finally describe several ways incorporate additional domainspecific knowledge applying casebased classifiers similar tasks
in casebased reasoning systems case adaptation process traditionally controlled static libraries handcoded adaptation rules this paper proposes method learning adaptation knowledge form adaptation strategies type developed handcoded kass adaptation strategies differ standard adaptation rules encode general memory search procedures finding information needed case adaptation paper focuses issues involved learning memory search procedures form basis new adaptation strategies it proposes method starts small library abstract adaptation rules uses introspective reasoning systems memory organization generate memory search plans needed apply rules the search plans packaged original abstract rules form new adaptation strategies future use this process allows cbr system learn domain storing results case adaptation also learn apply cases memory effectively
in artificial intelligence psychology education growing body research supports view learning goaldirected process psychological experiments show people different goals process information differently studies education show goals strong effects students learn functional arguments machine learning support necessity goalbased focusing learner effort at fourteenth annual conference cognitive science society symposium brought together researchers ai psychology education discuss goaldriven learning this article presents fundamental points illuminated symposium placing context open questions current research di rections goaldriven learning fl appears ai magazine
we present new method inspired bootstrap whose goal determine quality reliability neural network predictor our method leads robust forecasting along large amount statistical information forecast performance exploit we exhibit method context multivariate time series prediction financial data new york stock exchange it turns variation due different resamplings ie splits training crossvalidation test sets significantly larger variation due different network conditions architecture initial weights furthermore method allows us forecast probability distribution opposed traditional case single value time step we demonstrate strictly heldout test set includes stock market crash we also compare performance class neural networks identically bootstrapped linear models
we present new method obtaining local error bars ie estimates confidence predicted value depend input we approach problem nonlinear regression maximum likelihood framework we demonstrate technique first computer generated data locally varying normally distributed target noise we apply laser data santa fe time series competition finally extend technique estimate error bars iterated predictions apply exact competition task gives best performance date
pinsker gave precise asymptotic evaluation minimax mean squared error estimation signal gaussian noise signal known priori lie compact ellipsoid hilbert space this minimax bayes method applied variety global nonparametric estimation settings parameter spaces far ellipsoidal for example leads theory exact asymptotic minimax estimation norm balls besov triebel spaces using simple coordinatewise estimators wavelet bases this paper outlines features method common several applications in particular derive new results exact asymptotic minimax risk weak p balls r n n also class local estimators triebel scale by nature method reveals structure asymptotically least favorable distributions thus may simulate least favorable sample paths we illustrate estimation signal gaussian white noise norm balls certain besov spaces in wavelet bases p lt least favorable priors sparse resulting sample paths strikingly different observed pinskers ellipsoidal setting p acknowledgements i grateful many conversations david donoho carl taswell referee helpful comments this work supported part nsf grants dms nih phs grant gm
a proper choice proposal distribution mcmc methods eg metropolishastings algorithm well known crucial factor convergence algorithm in paper introduce adaptive metropolis algorithm am gaussian proposal distribution updated along process using full information cumulated far due adaptive nature process am algorithm nonmarkovian establish correct ergodic properties we also include results numerical tests indicate am algorithm competes well traditional metropolishastings algorithms demonstrate am provides easy use algorithm practical computation mathematics subject classification c u keywords adaptive mcmc comparison convergence ergodicity markov chain
we previously shown regularization principles lead approximation schemes equivalent networks one layer hidden units called regularization networks in particular discussed standard smoothness functionals lead subclass regularization networks wellknown radial basis functions approximation schemes in paper show regularization networks encompass much broader range approximation schemes including many popular general additive models neural networks in particular introduce new classes smoothness functionals lead different classes basis functions additive splines well tensor product splines obtained appropriate classes smoothness functionals furthermore extension leads radial basis functions rbf hyper basis functions hbf also leads additive models ridge approximation models containing special cases breimans hinge functions forms projection pursuit regression we propose use term generalized regularization networks broad class approximation schemes follow extension regularization in probabilistic interpretation regularization different classes basis functions correspond different classes prior probabilities approximating function spaces therefore different types smoothness assumptions in final part paper show relation activation functions gaussian sigmoidal type considering simple case kernel gx jxj in summary different multilayer networks one hidden layer collectively call generalized regularization networks correspond different classes priors associated smoothness functionals classical regularization principle three broad classes radial basis functions generalize hyper basis functions b tensor product splines c additive splines generalize schemes type ridge approximation hinge functions onehiddenlayer perceptrons this paper describes research done within center biological computational learning department brain cognitive sciences artificial intelligence laboratory this research sponsored grants office naval research contracts nj nj grant national science foundation contract asc includes funds darpa provided hpcc program grant national institutes health contract nih srr additional support provided north atlantic treaty organization atr audio visual perception research laboratories mitsubishi electric corporation sumitomo metal industries siemens ag support ai laboratorys artificial intelligence research provided onr contract nj tomaso poggio supported uncas helen whitaker chair whitaker college massachusetts institute technology c fl massachusetts institute technology
in many cases programs lengths increase known bloat fluff increasing structural complexity artificial evolution we show bloat specific genetic programming suggest inherent search techniques discrete variable length representations using simple static evaluation functions we investigate bloating characteristics three nonpopulation one population based search techniques using novel mutation operator an artificial ant following santa fe trail problem solved simulated annealing hill climbing strict hill climbing population based search using two variants new subtree based mutation operator as predicted bloat observed using unbiased mutation absent simulated annealing hill climbers using length neutral mutation however bloat occurs mutations using population we conclude two causes bloat search operators length bias tend sample bigger trees competition within populations favours longer programs usually reproduce accurately
we propose probabilistic casespace metric case matching case adaptation tasks central approach probability propagation algorithm adopted bayesian reasoning systems allows casebased reasoning system perform theoretically sound probabilistic reasoning the probability propagation mechanism actually offers uniform solution case matching case adaptation problems we also show algorithm implemented connectionist network efficient massively parallel case retrieval inherent property system we argue using kind approach difficult problem case indexing completely avoided pp topics casebased reasoning edited stefan wess klausdieter althoff michael m richter volume lecture
this paper describes simple means genetic search towards optimal neural network architectures improved convergence speed quality final result this result theoretically explained baldwin effect implemented learning process network alone also changing network architecture part learning procedure this seen combination two different techniques help ing improving simple genetic search
technical report no revised march university washington department statistics seattle washington abstract during past years several nonparametric alternatives cox proportional hazards model appeared literature these methods extend techniques well known regression analysis analysis censored survival data in paper discuss methods based partition trees polynomial splines analyze two datasets using survival trees hare compare strengths weaknesses two methods one strengths hare model fitting procedure implicit check proportionality underlying hazards model it also provides explicit model conditional hazards function makes convenient obtain graphical summaries on hand treebased methods automatically partition dataset groups cases similar survival history results obtained survival trees hare often complimentary trees splines survival analysis provide data analyst two useful tools analyzing survival data
this paper second series two problem estimating function probability distribution finite set samples distribution in first paper bayes estimator function probability distribution introduced optimal properties bayes estimator discussed bayes frequencycounts estimators shannon entropy derived graphically contrasted in current paper analysis first paper extended derivation bayes estimators several functions interest statistics information theory these functions powers mutual information chisquared tests independence variance covariance average finding bayes estimators several functions requires extensions analytical techniques developed first paper extensions form main body paper this paper extends analysis ways well example enlarging class potential priors beyond uniform prior assumed first paper in particular use entropic dirichlet priors considered
many lowerlevel areas mammalian visual system organized retinotopically maps preserve certain degree topography retina a unit part retinotopic map normally responds selectively stimulation welldelimited part visual field referred receptive field rf receptive fields probably prominent ubiquitous computational mechanism employed biological information processing systems this paper surveys possible computational reasons behind ubiquity rfs discussing examples rfbased solutions problems vision spatial acuity sensory coding object recognition fl weizmann institute cstr appear vision r j watt ed mit press
we address problem computing largest fraction missing information em algorithm worst linear function data augmentation these largest eigenvalue associated eigenvector jacobian em operator maximum likelihood estimate important assessing convergence iterative simulation an estimate largest fraction missing information available em iterates often adequate since figures accuracy needed in instances em iteration also gives estimate worst linear function we show power method eigencomputation used compute efficient accurate estimates quantities unlike eigenvalue decomposition power method computes largest eigenvalue eigenvector matrix take advantage good eigenvector estimate initial value terminated figures accuracy obtained moreover matrix products needed power method computed extrapolation obviating need form jacobian em operator we give results simultation studies multivariate normal data showing approach becomes efficient data dimension increases methods use finitedifference approximation jacobian generalpurpose alternative available fl funded national institutes health small business innovation reseach grant rca office naval research contracts n n we indebted tim hesterberg jim schimert doug clarkson anne greenbaum adrian raftery comments discussion helped advance research improve paper
we describe directed acyclic graphical model contains hierarchy linear units mechanism dynamically selecting appropriate subset units model observation the nonlinear selection mechanism hierarchy binary units gates output one linear units there connections linear units binary units generative model viewed logistic belief net neal selects skeleton linear model among available linear units we show gibbs sampling used learn parameters linear binary units even sampling brief markov chain far equilibrium
we describe simple reduction problem paclearning multipleinstance examples paclearning onesided random classification noise thus concept classes learnable onesided noise includes concepts learnable usual sided random noise model plus others parity function learnable multipleinstance examples we also describe efficient somewhat technically involved reduction statisticalquery model results polynomialtime algorithm learning axisparallel rectangles sample complexity od r saving roughly factor r results auer et al
the absence powerful control structures processes synchronize coordinate switch choose among regulate direct modulate interactions combine distinct yet interdependent modules large connectionist networks cn probably one important reasons networks yet succeeded handling difficult tasks eg complex object recognition description complex problemsolving planning in paper examine cn built large numbers relatively simple neuronlike units given ability handle problems typical multicomputer networks artificial intelligence programs along types programs always handled using extremely elaborate precisely worked central control coordination synchronization switching etc we point several mechanisms central control unbrainlike sort cn already built albeit hidden often overlooked ways we examine kinds control mechanisms found computers programs fetal development cellular function immune system evolution social organizations especially brains might use cn particularly intriguing suggestions found pacemakers oscillators local sources brains complex partial synchronies diffuse global effects slow electrical waves neurohormones developmental program guides fetal development communication coordination within among living cells working immune system evolutionary processes operate large populations organisms great variety partially competing partially cooperating controls found small groups organizations larger societies all systems rich control typically control emerges complex interactions many local diffuse sources we explore several different kinds plausible control mechanisms might incorporated cn assess potential benefits respect cost
in drug activity prediction handwritten character recognition features extracted describe training example depend pose location orientation etc example in handwritten character recognition one best techniques addressing problem tangent distance method simard lecun denker jain et al b introduce new techniquedynamic reposingthat also addresses problem dynamic reposing iteratively learns neural network reposes examples effort maximize predicted output values new models trained new poses computed models poses converge this paper compares dynamic reposing tangent distance method task predicting biological activity musk compounds in fold crossvalidation
we analyse behaviour propose revise architecture vt elevator design problem show problem solving method solve possible cases covered available domain knowledge we investigate problem show limitation caused restricted search regime employed method competence method improved acquiring additional domain knowledge we therefore propose alternative design problem solver integrates casebased reasoning heuristic search techniques overcomes competencerelated limitations exhibited propose revise architecture maintaining level efficiency we describe four algorithms casebased design exploit general properties parametric design tasks application specific heuristic knowledge
genetic algorithms related evolutionary techniques offer promising approach automatically exploring design space neural architectures artificial intelligence cognitive modeling central process evolutionary design neural architectures edna choice representation scheme used encode neural architecture form gene string genotype decode genotype corresponding neural architecture phenotype the representation scheme used constrains class neural architectures representable evolvable system also determines efficiency timespace complexity evolutionary design procedure whole this paper identifies discusses set properties used characterize different representations used edna design select representations necessary properties particular classes applications
load balancing even irregular neural networks the idea achieve goals lies programming model cupit programs objectcentered connections nodes graph neural network objects algorithms based parallel local computations nodes connections communication along connections plus broadcast reduction operations this report describes design considerations resulting language definition discusses detail tutorial example program
when reasoner explains surprising events internal use key motivation explaining perform learning facilitate achievement goals human explainers use range strategies build explanations including internal reasoning external information search goalbased considerations profound effect choices pursue explanations however standard ai models explanation rely goalneutral use single fixed strategygenerally backwards chainingto build explanations this paper argues explanation modeled goaldriven learning process gathering transforming information discusses issues involved developing active multistrategy process goaldriven explanation
rflissom selforganizing model laterally connected orientation maps primary visual cortex used study psychological phenomenon known tilt aftereffect the selforganizing processes responsible longterm development map lateral connections shown result tilt aftereffects short time scales adult the model allows observing large numbers neurons connections simultaneously making possible relate higherlevel phenomena lowlevel events difficult experimentally the results give computational support idea direct tilt aftereffects arise adaptive lateral interactions feature detectors long surmised they also suggest indirect effects could result conservation synaptic resources process the model thus provides unified computational explanation selforganization direct indirect tilt aftereffects primary visual cortex
a factor graph bipartite graph expresses global function several variables factors product local functions factor graphs subsume many graphical models including bayesian networks markov random fields tanner graphs we describe general algorithm computing marginals global function distributed messagepassing corresponding factor graph a wide variety algorithms developed artificial intelligence statistics signal processing digital communications communities derived specific instances general algorithm including pearls belief propagation belief revision algorithms fast fourier transform viterbi algorithm forwardbackward algorithm iterative turbo decoding algorithm
genetic programming automatic programming technique evolves computer programs solve approximately solve problems this paper presents two examples genetic programming creates computer program controlling robot robot moves specified destination point minimal time in first approach genetic programming evolves computer program composed ordinary r h e c p e r n n
this paper argues task matching casebased reasoning often improved comparing new cases portions precedents an example presented illustrates combining portions multiple precedents permit new cases resolved would indeterminate new cases could compared entire precedents a system uses portions precedents legal analysis domain texas workers compensation law grebe described examples grebes analysis combine reasoning steps multiple precedents presented
in designing autonomous agents deal competently issues involving time space tradeoff made guaranteed responsetime reactions one hand flexibility expressiveness we propose model action probabilistic reasoning decision analytic evaluation use layered control architecture our model well suited tasks require reasoning interaction behaviors events fixed temporal horizon decisions continuously reevaluated problem plans becoming obsolete new information becomes available in paper particularly interested tradeoffs required guarantee fixed reponse time reasoning nondeterministic causeandeffect relationships by exploiting approximate decision making processes able trade accuracy predictions speed decision making order improve expected per formance dynamic situations
we propose examine method approximate dynamic programming markov decision processes based structured problem representations we assume mdp represented using dynamic bayesian network construct value functions using decision trees function representation the size representation kept within acceptable limits pruning value trees leaves represent possible ranges values thus approximating value functions produced optimization we propose method detecting convergence prove errors bounds resulting approximately optimal value functions policies describe preliminary experi mental results
the majority commercial computers today register machines von neumann type we developed method evolve turingcomplete programs register machine the described implementation enables use program constructs arithmetic operators large indexed memory automatic decomposition subfunctions subroutines adfs conditional constructs ie ifthenelse jumps loop structures recursion protected functions string list functions any cfunction compiled linked function set system the use register machine language allows us work lowest level binary machine code without interpreting steps in von neumann machine programs data reside memory genetic operators thus directly manipulate binary machine code memory the genetic operators written clanguage modify individuals binary representation the result execution speed enhancement times compared interpreting clanguage implementation times compared lisp implementation the use binary machine code demands compact coding one byte per node individual the resulting evolved programs disassembled cmodules incorporated conventional software development environment the low memory requirements significant speed enhancement technique could use applying genetic programming new application areas platforms research domains
this paper considers importance exploration gameplaying programs learn playing opponents the central question whether learning program play move offers best chance winning present game play move best chance providing useful information future games an approach addressing question developed using probability theory implemented two different learning methods initial experiments game go suggest program takes exploration account learn better knowledgeable opponent program
technical report computer sciences department university wisconsin madison nov abstract this article describes approach combining symbolic connectionist approaches machine learning a threestage framework presented research several groups reviewed respect framework the first stage involves insertion symbolic knowledge neural networks second addresses refinement prior knowledge neural representation third concerns extraction refined symbolic knowledge experimental results open research issues discussed a shorter version paper appear machine learning
a distributed neural network model called spec processing sentences recursive relative clauses described the model based separating tasks segmenting input word sequence clauses forming caserole representations keeping track recursive embeddings different modules the system needs trained basic sentence constructs generalizes new instances familiar relative clause structures novel structures well spec exhibits plausible memory degradation depth center embeddings increases memory primed earlier constituents performance aided semantic constraints constituents the ability process structure largely due central executive network monitors controls execution entire system this way contrast earlier subsymbolic systems parsing modeled controlled highlevel process rather one based automatic reflex responses
in paper propose memorybased qlearning algorithm called predictive qrouting pqrouting adaptive traffic control we attempt address two problems encountered qrouting boyan littman namely inability finetune routing policies low network load inability learn new optimal policies decreasing load conditions unlike memorybased reinforcement learning algorithms memory used keep past experiences increase learning speed pqrouting keeps best experiences learned reuses predicting traffic trend the effectiveness pqrouting verified various network topologies traffic conditions simulation results show pqrouting superior
several researchers demonstrated neural networks trained compensate nonlinear signal distortion eg digital satellite communications systems these networks however require original signal distorted version known therefore trained offline adapt changing channel characteristics in paper novel dual reinforcement learning approach proposed adapt online system performing assuming channel characteristics directions two predistorters end communication channel coadapt using output predistorter determine reinforcement using common volterra series model simulate channel system shown successfully learn compensate distortions significantly higher might expected actual channel
most connectionist modeling assumes noisefree inputs this assumption often violated this paper introduces idea clearning simultaneously cleaning data learning underlying structure the cleaning step viewed topdown processing model modifies data learning step viewed bottomup processing data modifies model clearning used conjunction standard pruning this paper discusses statistical foundation clearning gives interpretation terms mechanical model describes obtain point predictions conditional densities output shows resulting model used discover properties data otherwise accessible signaltonoise ratio inputs this paper uses clearning predict foreign exchange rates noisy time series problem wellknown benchmark performances on outofsample test period clearning obtains annualized return investment significantly better otherwise identical network the final ultrasparse network remaining nonzero inputtohidden weights initial weights inputs hidden units robust overfitting this small network also lends interpretation
most connectionist modeling assumes noisefree inputs this assumption often violated this paper introduces idea clearning simultaneously cleaning data learning underlying structure the cleaning step viewed topdown processing model modifies data learning step viewed bottomup processing data modifies model clearning used conjunction standard pruning this paper discusses statistical foundation clearning gives interpretation terms mechanical model describes obtain point predictions conditional densities output shows resulting model used discover properties data otherwise accessible signaltonoise ratio inputs this paper uses clearning predict foreign exchange rates noisy time series problem wellknown benchmark performances on outofsample test period clearning obtains annualized return investment significantly better otherwise identical network the final ultrasparse network remaining nonzero inputtohidden weights initial weights inputs hidden units robust overfitting this small network also lends interpretation
we discuss bayesian formalism gives rise type wavelet threshold estimation nonparametric regression a prior distribution imposed wavelet coefficients unknown response function designed capture sparseness wavelet expansion common applications for prior specified posterior median yields thresholding procedure our prior model underlying function adjusted give functions falling specific besov space we establish relation hyperparameters prior model parameters besov spaces within realizations prior fall such relation gives insight meaning besov space parameters moreover established relation makes possible principle incorporate prior knowledge functions regularity properties prior model wavelet coefficients however prior knowledge functions regularity properties might hard elicit mind propose standard choise prior hyperparameters works well examples several simulated examples used illustrate method comparisons made thresholding methods we also present application data set collected anaesthesiological study
the perfect phylogeny problem classical problem computational evolutionary biology set speciestaxa described set qualitative characters in recent years problem shown npcomplete general different fixed parameter versions solved polynomial time in particular agarwala fernandezbaca developed o r nk k algorithm perfect phylogeny problem n species defined k rstate characters since commonly character data drawn alignments molecular sequences k length sequences thus large hundreds thousands thus imperative develop algorithms run efficiently large values k in paper make additional observations structure problem produce algorithm problem runs time o r k n we also show possible efficiently build structure implicitly represents set perfect phylogenies randomly sample set
belief networks probabilistic networks neural networks two forms network representations used development intelligent systems field artificial intelligence belief networks provide concise representation general probability distributions set random variables facilitate exact calculation impact evidence propositions interest neural networks represent parameterized algebraic combinations nonlinear activation functions found widespread use models real neural systems function approximators amenability simple training algorithms furthermore simple local nature neural network training algorithms provides certain biological plausibility allows massively parallel implementation in paper show similar local learning algorithms derived belief networks learning algorithms operate using information directly available normal inferential processes networks this removes main obstacle preventing belief networks competing neural networks abovementioned tasks the precise local probabilistic interpretation belief networks also allows partially wholly constructed humans allows results learning easily understood allows contribute rational decisionmaking welldefined way
we present new parallel algorithm learning bayesian inference networks data our learning algorithm exploits properties mdlbased score metric distributed asynchronous adaptive search technique called nagging nagging intrinsically fault tolerant dynamic load balancing features scales well we demonstrate viability effectiveness scalability approach empirically several experiments using order machines more specifically show distributed algorithm provide optimal solutions larger problems well good solutions bayesian networks variables
in article investigate relationship two popular algorithms em algorithm gibbs sampler we show approximate rate convergence gibbs sampler gaussian approximation equal corresponding em type algorithm this helps implementing either algorithms improvement strategies one algorithm directly transported in particular running em algorithm know approximately many iterations needed convergence gibbs sampler we also obtain result conditions em algorithm used finding maximum likelihood estimates slower converge corresponding gibbs sampler bayesian inference uses proper prior distributions we illustrate results number realistic examples based generalized linear mixed models
underwater mammal sound classification demonstrated using novel application wavelet timefrequency decomposition feature extraction using bcm unsupervised network different feature extraction methods different wavelet representations studied the system achieves outstanding classification performance even tested mammal sounds recorded different locations used training the improved results suggest nonlinear feature extraction wavelet representations outperforms different linear choices basis functions
multiclass learning problems involve finding definition unknown function fx whose range discrete set containing k gt values ie k classes the definition acquired studying large collections training examples form hx fx existing approaches problem include direct application multiclass algorithms decisiontree algorithms id cart b application binary concept learning algorithms learn individual binary functions k classes c application binary concept learning algorithms distributed output codes employed sejnowski rosenberg nettalk system this paper compares three approaches new technique bch errorcorrecting codes employed distributed output representation we show output representations improve performance id nettalk task backpropagation isolatedletter speechrecognition task these results demonstrate errorcorrecting output codes provide generalpurpose method improving performance inductive learning programs multiclass problems
in paper give completeness theorem inductive inference rule inverse entailment proposed muggleton our main result hypothesis clause h derived example e background theory b inverse entailment iff h subsumes e relative b plotkins sense the theory b clausal theory example e clause neither tautology implied b the derived hypothesis h clause always definite in order prove result give declarative semantics arbitrary consistent clausal theories show sbresolution originally introduced plotkin complete procedural semantics the completeness shown extension completeness theorem sldresolution we also show every hypothesis h derived saturant generalization proposed rouveirol must subsume e wrt b buntines sense moreover show saturant generalization obtained inverse entailment giving restriction usage
we present algorithm arc reversal bayesian networks treestructured conditional probability tables consider advantages especially simulation dynamic probabilistic networks in particular method allows one produce cpts nodes involved reversal exploit regularities conditional distributions we argue approach alleviates overhead associated arc reversal plays important role evidence integration used restrict sampling variables dpns we also provide algorithm detects dynamic irrelevance state variables forward simulation this algorithm exploits structured cpts reversed network determine timeindependent fashion conditions variable need sampled
a novel approach learning first order logic formulae positive negative examples presented whereas present inductive logic programming systems employ examples true false ground facts clauses view examples interpretations true false target theory this viewpoint allows reconcile inductive logic programming paradigm classical attribute value learning sense latter special case former because property able adapt aq cn type algorithms order enable learning full first order formulae however whereas classical learning techniques concentrated concept representations disjunctive normal form use clausal representation corresponds conjuctive normal form conjunct forms constraint positive examples this representation duality reverses also role positive negative examples heuristics algorithm the resulting theory incorporated system named icl inductive constraint logic
the multiple instance problem arises tasks training examples ambiguous single example object may many alternative feature vectors instances describe yet one feature vectors may responsible observed classification object this paper describes compares three kinds algorithms learn axisparallel rectangles solve multipleinstance problem algorithms ignore multiple instance problem perform poorly an algorithm directly confronts multiple instance problem attempting identify feature vectors responsible observed classifications performs best giving correct predictions muskodor prediction task the paper also illustrates use artificial data debug compare algorithms
a new class data structures called bumptrees described these structures useful efficiently implementing number neural network related operations an empirical comparison radial basis functions presented robot arm mapping learning task applications density estimation classification constraint representation learning also outlined
model learning combined dynamic programming shown effective learning control continuous state dynamic systems the simplest method assumes learned model correct applies dynamic programming many approximators provide uncertainty estimates fit how exploited this paper addresses case system must prevented catastrophic failures learning we propose new algorithm adapted dual control literature use bayesian locally weighted regression models stochastic dynamic programming a common reinforcement learning assumption aggressive exploration encouraged this paper addresses converse case system reign exploration the algorithm illustrated dimensional simulated control problem
handling multiclass problems real numbers important practical applications machine learning kdd problems while attributevalue learners address problems rule ilp systems the ilp systems handle real numbers mostly trying real values applicable thus running efficiency overfitting problems this paper discusses recent extensions icl address problems icl stands inductive constraint logic ilp system learns first order logic formulae positive negative examples the main charateristic icl view examples these seen interpretations true false clausal target theory cnf we first argue icl used learning theory disjunctive normal form dnf with mind possible solution handling two classes given based ideas cn finally show tackle problems continuous values adapting discretization techniques attribute value learners
the paper presents learning controller capable increasing insertion speed consecutive pegintohole operations without increasing contact force level our aim find better relationship measured forces controlled velocity without using complicated human generated model we followed connectionist approach two learning phases distinguished first learning controller trained initialised supervised way suboptimal task frame controller then reinforcement learning phase follows the controller consists two networks policy network exploration network online robotic exploration plays crucial role obtaining better policy optionally architecture extended third network reinforcement network the learning controller implemented cadbased contact force simulator in contrast related work experiments simulated d degrees freedom performance pegintohole task measured insertion time averagemaximum force level the fact better performance obtained way demonstrates importance modelfree learning techniques repetitive robotic assembly tasks the paper presents approach simulation results keywords robotic assembly pegintohole artificial neural networks reinforcement learning
indirect experiments studies randomized control replaced randomized encouragement subjects encouraged rather forced receive treatment programs the purpose paper bring attention experimental researchers simple mathematical results enable us assess indirect experiments strength causal influences operate among variables interest the results reveal despite laxity encouraging instrument indirect experimentation yield significant sometimes accurate information impact program population whole well particular individuals participated program
in paper problem system identification h investigated case given frequency response data necessarily uniformly spaced grid frequencies a large class robustly convergent identification algorithms derived a particular algorithm examined explicit worst case error bounds h norm derived discretetime continuoustime systems examples provided illustrate application algorithms
advances vlsi technology enable chips billion transistors within next decade unfortunately centralizedresource architectures modern microprocessors illsuited exploit advances achieving high level parallelism reasonable clock speed requires distributing processor resources trend already visible dualregisterfile architecture alpha a raw microprocessor takes extreme position space distributing resources instruction streams register files memory ports alus pipelined twodimensional interconnect exposing fully compiler compilation instructionlevel parallelism ilp distributedresource machines requires spatial instruction scheduling traditional temporal instruction scheduling this paper describes techniques used raw compiler handle issues preliminary results suifbased compiler sequential programs written c fortran indicate raw approach exploiting ilp achieve speedups scalable number processors applications parallelism the raw architecture attempts provide performance least comparable provided scaling existing architecture achieve orders magnitude improvement performance applications large amount parallelism this paper offers positive results direction
the use previously learned knowledge learning shown reduce number examples required good generalization increase robustness noise examples in reviewing various means using learned knowledge domain guide learning domain two underlying classes discerned methods use previous knowledge initialize learner initialization bias use previous knowledge constrain learner search bias we show methods fact exploit domain knowledge differently complement this shown presenting combined approach initializes constrains learner this combined approach seen outperform individual methods conditions accurate previously learned domain knowledge available irrelevant features domain representation
we consider recurrent analog neural nets output gate subject gaussian noise common noise distribution nonzero large set we show many regular languages recognized networks type give precise characterization languages recognized this result implies severe constraints possibilities constructing recurrent analog neural nets robust realistic types analog noise on hand present method constructing feedforward analog neural nets robust regard analog noise type
this paper describes rapture system revising probabilistic rule bases converts symbolic rules connectionist network trained via connectionist techniques it uses modified version backpropagation refine certainty factors rule base uses ids informationgain heuristic quinlan add new rules work currently way finding improved techniques modifying network architectures include adding hidden units using upstart algorithm frean a case made via comparison fully connected connectionist techniques keeping rule base close original possible adding new input units needed
this paper tackles supervised induction distance examples described horn clauses constrained clauses in opposition syntaxdriven approaches approach discriminationdriven proceeds defining small set complex discriminant hypotheses these hypotheses serve new concepts used redescribe initial examples further redescription embedded space natural integers distance examples thus naturally follows
probability theory represents manipulates uncertainties tell us behave for need utility theory assigns values usefulness different states decision theory concerns optimal rational decisions there many methods probability modeling learning utility decision models we use reinforcement learning find optimal sequence questions diagnosis situation maintaining high accuracy automated diagnosis heartdisease domain used demonstrate temporaldifference learning improve diagnosis on cleveland heartdisease database results better reported previous methods
the simple bayesian classifier sbc sometimes called naivebayes built based conditional independence model attribute given class the model previously shown surprisingly robust obvious violations independence assumption yielding accurate classification models even clear conditional dependencies the sbc serve excellent tool initial exploratory data analysis coupled visualizer makes structure comprehensible we describe visual representation sbc model successfully implemented we describe requirements visualization design decisions made satisfy
this paper describes recent work development computer architectures efficient execution artificial neural network algorithms our earlier system ring array processor rap multiprocessor based commercial dsps lowlatency ring interconnection scheme we used rap simulate variable precision arithmetic guide us design higher performance neurocomputers based custom vlsi the rap system played critical role study enabling us experiment much larger networks would otherwise possible our study shows backpropagation training algorithms require moderate precision specifically b weight values b output values sufficient achieve training classification results comparable b floating point although results gathered frame classification continuous speech expect extend many connectionist calculations we used results part design programmable single chip microprocessor spert the reduced precision arithmetic permits use multiple units per processor also reduced precision operands make efficient use valuable processormemory bandwidth for moderateprecision fixedpoint arithmetic applications spert represents order magnitude reduction cost systems based dsp chips
rk belew j mcinerney n schraudolph evolving networks using genetic algorithm connectionist learning artificial life ii sfi studies science complexity cg langton c taylor jd farmer s rasmussen eds vol addisonwesley m mcinerney ap dhawan use genetic algorithms back propagation training feedforward neural networks ieee international conference neural networks vol pp fz brill de brown wn martin fast genetic selection features neural network classifiers ieee transactions neural networks vol pp f dellaert j vandewalle automatic design cellular neural networks means genetic algorithms finding feature detector the third ieee international workshop cellular neural networks their applications ieee new jersey pp de moriarty r miikkulainen efficient reinforcement learning symbiotic evolution machine learning vol pp l davis handbook genetic algorithms van nostrand reinhold new york d whitely the genitor algorithm selective pressure proceedings third interanational conference genetic algorithms jd schaffer ed morgan kauffman san mateo ca pp van camp d t plate ge hinton the xerion neural network simulator documentation department computer science university toronto toronto
research machine learning design concentrated use development techniques solve simple welldefined problems invariably effort important early stages development field scale address real design problems since existing techniques based simplifying assumptions hold real design in particular address dependence context multiple often conflicting interests constitutive design this paper analyzes present situation criticizes number prevailing views subsequently paper offers alternative approach whose goal advance use machine learning design practice the approach partially integrated modeling system called ndim the use machine learning ndim presented open research issues outlined
kj the standard ppr algorithm friedman stuetzle estimates smooth functions f j using supersmoother nonparametric scatterplot smoother friedmans algorithm constructs model m max linear combinations prunes back simpler model size m m max m m max specified user this paper discusses alternative algorithm smooth functions estimated using smoothing splines the direction coefficients ff j amount smoothing direction number terms m m max determined optimize single generalized crossvalidation measure
in paper suggest mechanism improves significantly performance topdown inductive logic programming ilp learning system this improvement achieved cost giving system extra information difficult formulate this information appears form algorithm sketch incomplete somewhat vague representation computation related particular example we describe sketches admissible give details learning algorithm exploits information contained sketch the experiments carried implemented system skil demonstrated usefulness method potential future applications
in paper concerned problem inducing recursive horn clauses small sets training examples the method iterative bootstrap induction presented in first step system generates simple clauses regarded properties required definition properties represent generalizations positive examples simulating effect larger number examples properties used subsequently induce required recursive definitions this paper describes method together series experiments the results support thesis iterative bootstrap induction indeed effective technique could general use ilp
this paper aims examine use genetic algorithms optimize subsystems cellular neural network architectures the application hand character recognition aim evolve optimal feature detector order aid conventional classifier network generalize across different fonts to end performance function genetic encoding feature detector presented an experiment described optimal feature detector indeed found genetic algorithm we interested application cellular neural networks computer vision genetic algorithms gas serve optimize design cellular neural networks although design global architecture system could still done human insight propose specific submodules system best optimized using one optimization method gas good candidate fulfill optimization role well suited problems objective function complex function many parameters the specific problem want investigate one character recognition more specifically would like use ga find optimal feature detectors used recognition digits
this article exposes problems commonly used technique splitting available data training validation test sets held fixed warns drawing strong conclusions static splits shows potential pitfalls ignoring variability across splits using bootstrap resampling method compare uncertainty solution stemming data splitting neural network specific uncertainties parameter initialization choice number hidden units etc we present two results data new york stock exchange first variation due different resamplings significantly larger variation due different network conditions this result implies important overinterpret model ensemble models estimated one specific split data second split neural network solution early stopping close linear model significant nonlinearities extracted
validation used detect overfitting starts supervised training neural network training stopped convergence avoid overfitting early stopping the exact criterion used validationbased early stopping however usually chosen adhoc fashion training stopped interactively this trick describes select stopping criterion systematic fashion trick either speeding learning procedures improving generalization whichever important particular situation an empirical investigation multilayer perceptrons shows exists tradeoff training time generalization from given mix training runs using different problems different network architectures i conclude slower stopping criteria allow small improvements generalization average cost much training time factor longer average
we introduce new formal model learning algorithm must combine collection potentially poor statistically independent hypothesis functions order approximate unknown target function arbitrarily well our motivation includes question make optimal use multiple independent runs mediocre learning algorithm well settings many hypotheses obtained distributed population identical learning agents
markov chain monte carlo mcmc methods make possible use flexible bayesian models would otherwise computationally infeasible in recent years great variety applications described literature applied statisticians new methods may several questions concerns however how much effort expertise needed design use markov chain sampler how much confidence one answers mcmc produces how use mcmc affect rest modelbuilding process at joint statistical meetings august panel experienced mcmc users discussed issues well various tricks trade this paper edited recreation discussion its purpose offer advice guidance novice users mcmc notsonovice users well topics include building confidence simulation results methods speeding assessing convergence estimating standard errors identification models good mcmc algorithms exist current state software development
neural network machine learning laboratory computer science department brigham young university provo ut usa email martinezcsbyuedu www httpaxoncsbyuedu abstract many neural network models must trained finding set realvalued weights yield high accuracy training set other learning models require weights input attributes yield high leaveoneout classification accuracy order avoid problems associated irrelevant attributes high dimensionality in addition variety general problems set real values must found maximize evaluation function this paper presents algorithm schemata search realvalued weight space find set weights real values yield high values given evaluation function the algorithm called realvalued schemata search rvss uses brace statistical technique moore lee determine narrow search space this paper details rvss approach gives initial empirical results
a large body nonparametric statistical literature devoted density estimation overviews given silverman izenman this paper addresses problem univariate density estimation novel way our approach falls class called projection estimators introduced cencov the orthonormal basis used basis compactly supported wavelets daubechies family kerkyacharian picard donoho et al delyon juditsky among others applied wavelets density estimation the local nature wavelet functions makes wavelet estimator superior projection estimators use classical orthonormal bases fourier hermite etc instead estimating unknown density directly estimate square root density enables us control positiveness l norm density estimate however approach one needs preestimator density calculate sample wavelet coefficients we describe visustop datadriven procedure determining maximum number levels wavelet density estimator coefficients selected levels thresholded make estimator parsimonious
intermediate higher vision processes require selection subset available sensory information processing usually selection implemented form spatially circumscribed region visual field socalled focus attention scans visual scene dependent input attentional state subject we present model control focus attention primates based saliency map this mechanism expected model functionality biological vision also essential understanding complex scenes machine vision
abstract the problem hypothesis testing examined historical bayesian points view case sampling underlying joint probability distribution hypotheses tested independence dependence underlying distribution exact results bayesian method provided asymptotic bayesian results historical method quantities compared historical method quantities interpreted terms clearly defined bayesian quantities the asymptotic bayesian test relies upon statistic predominantly mutual information problems hypothesis testing arise ubiquitously situations observed data produced unknown process question asked from process observed data arise historically hypothesis testing problem approached point view sampling whereby several fixed hypotheses tested given measures test quality found directly likelihood ie amounts sampling likelihood to specific hypothesis set possible parameter vectors parameter vector completely specifying sampling distribution a simple hypothesis hypothesis set contains one parameter vector a composite hypothesis occurs nonempty hypothesis set single parameter vector generally test procedure chooses true hypothesis gives largest test value although notion procedure specific may refer method choosing hypothesis given test values since interest quantify quality test level significance generated level probability chosen hypothesis test procedure incorrect hypothesis choice made the significance generated using sampling distribution likelihood for simple hypotheses level significance found using single parameter value hypothesis when test applied case composite hypothesis size test found given supremum probability ranging parameter vectors hypothesis set chosen
this literature review discusses different methods general rubric learning bayesian networks data includes overlapping work general probabilistic networks connections drawn statistical neural network uncertainty communities different methodological communities bayesian description length classical statistics basic concepts learning bayesian networks introduced methods reviewed methods discussed learning parameters probabilistic network learning structure learning hidden variables the presentation avoids formal definitions theorems plentiful literature instead illustrates key concepts simplified examples
recent work supervised learning shown surprisingly simple bayesian classifier strong assumptions independence among features called naive bayes competitive state art classifiers c this fact raises question whether classifier less restrictive assumptions perform even better in paper examine evaluate approaches inducing classifiers data based recent results theory learning bayesian networks bayesian networks factored representations probability distributions generalize naive bayes classifier explicitly represent statements independence among approaches single method call tree augmented naive bayes tan outperforms naive bayes yet time maintains computational simplicity search involved robustness characteristic naive bayes we experimentally tested approaches using benchmark problems u c irvine repository compared c naive bayes wrapperbased feature selection methods
in recent years flurry works learning probabilistic belief networks current state art methods shown successful two learning scenarios learning network structure parameters complete data learning parameters fixed network incomplete datathat presence missing values hidden variables however method yet demonstrated effectively learn network structure incomplete data in paper propose new method learning network structure incomplete data this method based extension expectationmaximization em algorithm model selection problems performs search best structure inside em procedure we prove convergence algorithm adapt learning belief networks we describe learn networks two scenarios data contains missing values presence hidden variables we provide experimental results show effectiveness procedure scenarios
this paper defines class problems involving combinations induction cost optimisation a framework presented systematically describes problems involve construction decision trees rules optimising accuracy well measurement misclassification costs it present new algorithms shows framework used configure greedy algorithms constructing trees rules the framework covers number existing algorithms moreover framework also used define algorithm configurations new functionalities expressed evaluation functions
we introduce new framework study reasoning the learning order reason approach developed views learning integral part inference process suggests learning reasoning studied together the learning reason framework combines interfaces world used known learning models reasoning task performance criterion suitable in framework intelligent agent given access favorite learning interface also given grace period interact interface construct representation kb world w the reasoning performance measured period agent presented queries ff query language relevant world answer whether w implies ff the approach meant overcome main computational difficulties traditional treatment reasoning stem separation world since agent interacts world constructing knowledge representation choose representation useful task hand moreover make explicit dependence reasoning performance environment agent interacts we show previous results learning theory reasoning fit framework illustrate usefulness learning reason approach exhibiting new results possible traditional setting first give learning reason algorithms classes propositional languages efficient reasoning algorithms represented traditional formulabased knowledge base second exhibit learning reason algorithm class propositional languages known learnable traditional sense an earlier version paper appears proceedings national conference artificial intelligence aaai roni khardon supported aro grant daalg nsf grant ccr dan roth supported nsf grant ccr darpa afosrfj author present addresses roni khardon division applied sciences harvard university cambridge ma email ronidasharvardedu dan roth department computer science university illinois urbanachampaign w springfield ave urbana illinois email danrcsuiucedu permission make digital hard copies part work personal classroom use granted without fee provided copies made distributed profit direct commercial advantage copies show notice first page initial screen display along full citation copyrights components work owned others acm must honored abstracting credit permitted to copy otherwise republish post servers redistribute lists use component work works requires prior specific permission andor fee permissions may requested publications dept acm inc broadway new york ny usa fax permissionsacmorg
many ai problems formalized reduce evaluating probability propositional expression true in paper show problem computationally intractable even surprisingly restricted cases even settle approximation probability we consider various methods used approximate reasoning computing degree belief bayesian belief networks well reasoning techniques constraint satisfaction knowledge compilation use approximation avoid computational difficulties reduce modelcounting problems propositional domain we prove counting satisfying assignments propositional languages intractable even horn monotone formulae even size clauses number occurrences variables extremely limited this contrasted case deductive reasoning horn theories theories binary clauses distinguished existence linear time satisfiability algorithms what even surprising show even approximating number satisfying assignments ie approximating approximate reasoning intractable restricted theories we also identify restricted classes propositional formulae efficient algorithms counting satisfying assignments given fl preliminary version paper appeared proceedings th international joint conference artificial intelligence ijcai supported nsf grants ccr ccr darpa afosrfj
recently extended kalman filter ekf based training demonstrated effective neural network training however conjunction pruning methods weight decay optimal brain damage obd yet studied in paper elucidate method ekf training propose pruning method based results obtained ekf training these combined training pruning method applied time series prediction problem
we describe recent extensions framework automatic generation musicmaking programs we previously used genetic programming techniques produce musicmaking programs satisfy userprovided critical criteria in paper describe new work use connectionist techniques automatically induce musical structure corpus we show resulting neural networks used critics drive genetic programming system we argue framework potentially support induction recapitulation deep structural features music we present initial results produced using neural hybrid symbolicneural critics discuss directions future work
much effort devoted understanding learning reasoning artificial intelligence however models attempt integrate two complementary processes rather vast body research machine learning often focusing inductive learning examples quite isolated work reasoning artificial intelligence though two processes may different much interrelated the ability reason domain knowledge often based rules domain must learned somehow and ability reason often used acquire new knowledge learn this paper introduces incremental learning algorithm ila attempts combine inductive learning prior knowledge reasoning ila many important characteristics useful combination including incremental selforganizing learning nonuniform learning inherent nonmonotonicity extensional intensional capabilities low order polynomial complexity the paper describes ila gives simulation results several applications discusses characteristics detail
this paper appeared machine learning abstract this paper demonstrates nature opposition training affects learning play twoperson perfect information board games it considers different kinds competitive training impact trainer error appropriate metrics posttraining performance measurement ways metrics applied the results suggest teaching program leading repeatedly restricted paths albeit high quality ones overly narrow preparation variations appear realworld experience the results also demonstrate variety introduced training random choice unreliable preparation program directs training may overlook important situations the results argue broad variety training experience play many levels this variety may either inherent game introduced deliberately training lesson practice training blend expert guidance knowledgebased selfdirected elaboration shown particularly effective learning competition
the negative effect naturally significant complex domain the graph simple domain crosses line earlier complex domain that means learning starts useful weight greater simple domain complex domain as relax optimality requirement g n f c n l w h w macro usage complex domain becomes advantageous the purpose research described paper identify parameters effects deductive learning perform experiments systematically order understand nature effects the goal paper demonstrate methodology performing parametric experimental study deductive learning the example include study two parameters point satisficingoptimizing scale used search carried problem solving time learning time we showed a looks optimal solutions benefit macro learning strategy comes closer bestfirst satisficing search utility macros increases we also demonstrated deductive learners learn offline solving training problems sensitive type search used learning we showed general optimizing search best learning it generates macros increase quality solutions regardless search method used problem solving it also improves efficiency problem solvers require high level optimality the drawback using optimizing search increase learning resources spent we aware fact results described surprising the goal parametric study necessarily find exciting results obtain results sometimes even previously known controlled experimental environment the work described part research plan we currently process extensive experimentation parameters described also others we also intend test validity conclusions reached study repeating tests several commonly known search problems we hope systematic experimentation help research community better understand process deductive learning serve demonstration experimental methodology used machine learning research
we examine number techniques representing actions stochastic effects using bayesian networks influence diagrams we compare techniques according ease specification size representation required complete specification dynamics particular system paying particular attention role persistence relationships we precisely characterize two components frame problem bayes nets stochastic actions propose several ways deal problems compare solutions reiters solution frame problem situation calculus the result set techniques permit ease specification compact representation probabilistic system dynamics comparable size timbre reiters representation ie explicit frame axioms
given function f mapping nvariate inputs finite field f f consider task reconstructing list nvariate degree polynomials agree f tiny nonnegligible fraction ffi input space we give randomized algorithm solving task accesses f black box runs time polynomial djf j for special case solve problem def jf j gt in case running time algorithm bounded polynomial n exponential our algorithm generalizes previously known algorithm due goldreich levin solves
many problems correspond classical control task determining appropriate control action take given sequence observations one standard approach learning control rules called behavior cloning involves watching perfect operator operate plant trying emulate behavior in experimental learning approach contrast learner first guesses initial operationtoaction policy tries if policy performs suboptimally learner modify produce new policy recur this paper discusses relative effectiveness two approaches especially presence perceptual aliasing showing particular experimental learner often learn effectively cloning one
we present connectionist method representing images explicitly addresses hierarchical nature it blends data neuroscience wholeobject viewpoint sensitive cells inferotemporal cortex attentional basisfield modulation v ideas hierarchical descriptions based microfeatures the resulting model makes critical use bottomup topdown pathways analysis synthesis we illustrate model simple example representing information faces
this paper discusses role culture evolution cognitive systems we define culture information transmitted individuals generations nongenetic means experiments presented use genetic programming systems include special mechanisms cultural transmission information these systems evolve computer programs perform cognitive tasks including mathematical function mapping action selection virtual world the data show presence culturesupporting mechanisms clear beneficial impact evolvability correct programs the implications results may cognitive science briefly discussed
numerical design optimization algorithms highly sensitive particular formulation optimization problems given the formulation search space objective function constraints generally large impact duration optimization process well quality resulting design furthermore best formulation vary one application domain another one problem another within given application domain unfortunately design engineer may know best formulation advance attempting set run design optimization process in order attack problem developed software environment supports interactive formulation testing reformulation design optimization strategies our system represents optimization strategies terms secondorder dataflow graphs reformulations strategies implemented transformations dataflow graphs the system permits user interactively generate search space design optimization strategies experimentally evaluate performance test problems order find strategy suitable application domain the system implemented domain independent fashion tested domain racing yacht design
this paper presents basic results ideas dynamic programming relate directly concerns planning ai these form theoretical basis incremental planning methods used integrated architecture dyna these incremental planning methods based continually updating evaluation function situationaction mapping reactive system actions generated reactive system thus involve minimal delay incremental planning process guarantees actions evaluation function eventually optimalno matter extensive search required these methods well suited stochastic tasks tasks complete accurate model available for tasks large implement situationaction mapping table supervisedlearning methods must used capabilities remain significant limitation approach
this paper reports project document retrieval industrial setting the objective provide tool helps finding documents related given query answers frequently asked questions databases a cbr approach used develop running prototypical system currently practical evaluation
we investigate query complexity exact learning membership proper equivalence query model we give complete characterization concept classes learnable polynomial number polynomial sized queries model we give applications characterization including results learning natural subclass dnf formulas learning membership queries alone query complexity previously used prove lower bounds time complexity exact learning we show new relationship query complexity time complexity exact learning if honest class exactly properly learnable polynomial query complexity learnable polynomial time p np in particular show honest class exactly polynomialquery learnable learnable using oracle p
this paper presents case study evaluating casebased system it describes evaluation anapron system pronounces names combination rulebased casebased reasoning three sets experiments run anapron set exploratory measurements profile systems operation comparison anapron namepronunciation systems set studies modified various parts system isolate contribution lessons learned experiments cbr evaluation methodology cbr theory discussed this work may copied reproduced whole part commercial purpose permission copy whole part without payment fee granted nonprofit educational research purposes provided whole partial copies include following notice copying permission mitsubishi electric research laboratories cambridge massachusetts acknowledgment authors individual contributions work applicable portions copyright notice copying reproduction republishing purpose shall require license payment fee mitsubishi electric research laboratories all rights reserved
consider given value function states markov decision problem might result applying reinforcement learning algorithm unless value function equals corresponding optimal value function states discrepancy natural call bellman residual value function specifies state obtained onestep lookahead along seemingly best action state using given value function evaluate succeeding states this paper derives tight bound far optimal discounted return greedy policy based given value function function maximum norm magnitude bellman residual a corresponding result also obtained value functions defined stateaction pairs used qlearning one significant application results problems function approximator used learn value function training approximator based trying minimize bellman residual across states stateaction pairs when control based use resulting value function result provides link well objectives function approximator training met quality resulting control
to measure quality set vector quantization points means measuring distance random point quantization required common metrics hamming euclidean metrics mathematically simple inappropriate comparing natural signals speech images in paper shown environment functions input space x induces canonical distortion measure cdm x the depiction canonical justified shown optimizing reconstruction error x respect cdm gives rise optimal piecewise constant approximations functions environment the cdm calculated closed form several different function classes an algorithm training neural networks implement cdm presented along en couraging experimental results
theory revision systems typically use set theorytotheory transformations f k g hillclimb given initial theory new theory whose empirical accuracy given set labeled training instances fc j g local maximum at heart process evaluator compares accuracy current theory kb neighbors f k kbg goal determining neighbor highest accuracy the obvious wrapper evaluator simply evaluates individual neighbor theory kb k k kb instance c j as expensive evaluate single theory single instance great many training instances huge number neighbors approach prohibitively slow we present alternative system employs smarter evaluator quickly computes accuracy transformed theory k kb looking inside kb reasoning effects k transformation we compare performance naive wrapper system realworld theories obtained fielded expert system find runs times faster attaining accuracy this paper also discusses source power keywords theory revision efficient algorithm hillclimbing system multiple submissions we submited related version paper aaai fl we gratefully acknowledge many helpful comments report george drastal chandra mouleeswaran geoff towell
this paper describes wavelet method estimation density hazard rate functions randomly right censored data we adopt nonparametric approach assuming density hazard rate specific parametric form the method based dividing time axis dyadic number intervals counting number events within interval the number events survival function observations separately smoothed time via linear wavelet smoothers hazard rate function estimators obtained taking ratio we prove estimators possess pointwise global mean square consistency obtain best possible asymptotic mise convergence rate also asymptotically normally distributed we also describe simulation experiments show estimators reasonably reliable practice the method illustrated two real examples the first uses survival time data patients liver metastases colorectal primary tumour without distant metastases the second concerned times unemployment women wavelet estimate flexibility provides new interesting interpretation
experience plays important role development human expertise one computational model experience affects expertise provided research casebased reasoning examines stored cases encapsulating traces specific prior problemsolving episodes retrieved reapplied facilitate new problemsolving much progress made methods accessing relevant cases casebased reasoning receiving wide acceptance technology developing intelligent systems cognitive model human reasoning process however one important aspect casebased reasoning remains poorly understood process retrieved cases adapted fit new situations the difficulty encoding effective adaptation rules hand widely recognized serious impediment development fully autonomous casebased reasoning systems consequently important question casebased reasoning systems might learn improve expertise case adaptation we present framework acquiring expertise using combination general adaptation rules introspective reasoning casebased reasoning case adaptation task
there strong evidence face processing brain localized the double dissociation prosopagnosia face recognition deficit occurring brain damage visual object agnosia difficulty recognizing kinds complex objects indicates face nonface object recognition may served partially independent neural mechanisms in chapter use computational models show face processing specialization apparently underlying prosopagnosia visual object agnosia could attributed relatively simple competitive selection mechanism development devotes neural resources tasks best performing developing infants need perform subordinate classification identification faces early infants low visual acuity birth inspired de schonen mancinis arguments factors like could bias visual system develop specialized face processor jacobs kosslyns experiments mixtures experts me modeling paradigm provide preliminary computational demonstration theory accounts double dissociation face object processing we present two feedforward computational models visual processing in models selection mechanism gating network mediates competition modules attempting classify input stimuli in model i modules simple unbiased classifiers competition sufficient achieve enough specialization damaging one module impairs models face recognition object recognition damaging module impairs models object recognition face recognition in model ii however bias modules providing one low spatial frequency information high spatial frequency information in case models task subordinate classification faces superordinate classification objects low spatial frequency network shows even stronger specialization faces no combination tasks inputs shows strong specialization we take results support idea something resembling face processing module could arise natural consequence infants developmental environment without innately specified
in paper indicate possible applications ilp similar techniques knowledge discovery field discuss several methods adapting linking ilpsystems relational database systems the proposed methods range pure ilp based techniques originating ilp we show easy advantageous adapt ilpsystems way
most exact algorithms general pomdps use form dynamic programming piecewiselinear convex representation one value function transformed another we examine variations incremental pruning approach solving problem compare earlier algorithms theoretical empirical perspectives we find incremental pruning presently efficient algorithm solving pomdps
we improve error bounds based vc analysis classes sets similar classifiers we apply new error bounds separating planes artificial neural networks key words machine learning learning theory generalization vapnikchervonenkis separating planes neural networks
the higherorder structure genes features biological sequences described means formal grammars these grammars used generalpurpose parsers detect assemble structures means syntactic pattern recognition we describe grammar parser eukaryotic proteinencoding genes measures effective current connectionist combinatorial algorithms predicting gene structures sequence database entries parameters grammar rules optimized several different species mixing experiments performed determine degree species specificity relative importance compositional signalbased syntactic components gene prediction
the double dissociation prosopagnosia face recognition deficit occurring brain damage visual object agnosia difficulty recognizing kinds complex objects indicates face nonface object recognition may served partially independent mechanisms brain such dissociation could result competitive learning mechanism development devotes neural resources tasks best performing studies normal adult performance face object recognition tasks seem indicate face recognition primarily configural involving low spatial frequency information present stimulus relatively large distances whereas object recognition primarily featural involving analysis objects parts using local high spatial frequency information in feedforward computational model visual processing two modules compete classify input stimuli one module receives low spatial frequency information receives high spatial frequency information lowfrequency module shows strong specialization face recognition combined face identificationobject classification task the series experiments shows fine discrimination necessary distinguishing members visually homoge neous class faces relies heavily low spatial frequencies present stimulus
we present novel classification regression method combines exploratory projection pursuit unsupervised training projection pursuit regression supervised training yield new family costcomplexity penalty terms some improved generalization properties demonstrated real world problems
in paper present objective function formulation bcm theory visual cortical plasticity permits us demonstrate connection unsupervised bcm learning procedure various statistical methods particular projection pursuit this formulation provides general method stability analysis fixed points theory enables us analyze behavior evolution network various visual rearing conditions it also allows comparison many existing unsupervised methods this model shown successful various applications phoneme d object recognition we thus striking possibly highly significant result biological neuron performing sophisticated statistical procedure
a system automatic face recognition presented it consists several steps automatic detection eyes mouth followed spatial normalization images the classification normalized images carried hybrid supervised unsupervised neural network two methods reducing overfitting common problem high dimensional classification schemes presented superiority combination demonstrated
radial basis function rbf neural networks offer attractive equation form use modelbased control approximate highly nonlinear plants yet well suited linear adaptive control we show interpreting rbfs mixtures gaussians allows application many statistical tools including em algorithm parameter estimation the resulting emrbf models give uncertainty estimates warn extrapolating beyond region training data available
cabins framework modeling optimization task illstructured domains in domains neither systems human experts possess exact model guiding optimization and users model optimality subjective situationdependent cabins optimizes solution iterative revision using casebased reasoning in cabins task structure analysis adopted creating initial model optimization task generic vocabularies found analysis specialized case feature descriptions application problems extensive experimentation job shop scheduling problems shown cabins operationalize improve model accumulation cases
five related factors identified enable single compartment hodgkinhuxley model neurons convert random synaptic input irregular spike trains similar seen vivo cortical recordings we suggest cortical neurons may operate narrow parameter regime synaptic intrinsic conductances balanced flect spike timing detailed correlations inputs fl please send comments tonysalkedu the reference paper technical report inc february institute neural computation ucsd san diego ca
the application genetic algorithms neural network optimization gann produced active field research this paper proposes classification encoding strategies also gives critical analysis the idea evolving artificial neural networks nn genetic algorithms ga based powerful metaphor evolution human brain this mechanism developed highest form intelligence known scratch the metaphor inspired great deal research activities traced late instance an increasing amount research reports journal papers theses published topic generating continously growing field researchers devoloped variety different techniques encode neural networks ga increasing complexity this young field driven mostly small independet research groups scarcely cooperate this paper attempt analyse structure already performed work point shortcomings approaches current state development
we propose object recognition scheme based method feature extraction gray level images corresponds recent statistical theory called projection pursuit derived biologically motivated feature extracting neuron to evaluate performance method use set detailed psychophysical d object recognition experiments bulthoff edelman
wavelet shrinkagethe method proposed seminal work donohoand johnstone disarmingly simple efficient way denoising data shrinking wavelet coefficients proposed several optimality criteria the notable asymptotic minimax crossvalidation criteria in paper wavelet shrinkage imposing natural properties bayesian models data proposed the performance methods tested standard donohojohnstone test functions key words phrases wavelets discrete wavelet transform thresholding bayes model ams subject classification a g
this paper introduces idea clearning simultaneously cleaning data learning underlying structure the cleaning step viewed topdown processing model modifies data learning step viewed bottomup processing data modifies model after discussing statistical foundation proposed method maximum likelihood perspective apply clearning notoriously hard problem benchmark performances well known prediction foreign exchange rates on difficult test period clearning conjunction pruning yields annualized return outofsample significantly better otherwise identical network trained without cleaning the network started inputs hidden units ended nonzero weights inputs hidden units the resulting ultrasparse final architectures obtained clearning pruning immune overfitting even noisy problems since cleaned data allow simpler model apart competitive performance clearning gives insight data show estimate overall signaltonoise ratio input variable show error estimates pattern used detect remove outliers replace missing corrupted data cleaned values clearning used nonlinear regression classification problem
a large class machinelearning problems natural language require characterization linguistic context two characteristic properties problems feature space high dimensionality target concepts depend small subset features space under conditions multiplicative weightupdate algorithms winnow shown exceptionally good theoretical properties in work reported present algorithm combining variants winnow weightedmajority voting apply problem aforementioned class contextsensitive spelling correction this task fixing spelling errors happen result valid words substituting casual causal we evaluate algorithm winspell comparing bayspell statisticsbased method representing state art task we find when run full unpruned set features winspell achieves accuracies significantly higher bayspell able achieve either pruned unpruned condition when compared systems literature winspell exhibits highest performance while several aspects winspells architecture contribute superiority bayspell primary factor able learn better linear separator bayspell learns when run test set drawn different corpus training set drawn winspell better able bayspell adapt using strategy present combines supervised learning training set unsupervised learning noisy test set
various notions geometric ergodicity markov chains general state spaces exist in paper review certain relations implications among we apply results collection chains commonly used markov chain monte carlo simulation algorithms socalled hybrid chains we prove certain conditions hybrid chain inherit geometric ergodicity constituent parts acknowledgements we thank charlie geyer number useful comments regarding spectral theory central limit theorems we thank alison gibbs phil reiss peter rosenthal richard tweedie helpful discussions we thank referee editor many excellent suggestions
we present polynomialtime algorithm determining whether set species described characters exhibit phylogenetic tree assuming maximum number possible states character fixed this solves open problem posed kannan warnow our result contrasted proof steel bodlaender fellows warnow phylogeny problem npcomplete general
when trying forecast future behavior realworld system two key problems nonstationarity process eg regime switching overfitting model particularly serious noisy processes this articles shows gated experts point solutions problems the architecture also called society experts mixture experts consists nonlinear gating network several nonlinear competing experts each expert learns conditional mean usual expert also adaptive width the gating network learns assign probability expert depends input this article first discusses assumptions underlying architecture derives weight update rules it evaluates performance gated experts comparison single networks well networks two outputs one predicting mean one local error bar this article also investigates ability gated experts discover characterize underlying regimes the results significantly less overfitting compared single nets two reasons subsets potential inputs given experts gating network less curse dimensionality experts learn match variances local noise levels thus learning this article focuses architecture overfitting problem applications computergenerated toy problem laser data santa fe competition given mangeas weigend application realworld problem predicting electricity demand france given mangeas et al much data support
given set samples unknown probability distribution study problem constructing good approximative bayesian network model probability distribution question this task viewed search problem goal find maximal probability network model given data in work make attempt learn arbitrarily complex multiconnected bayesian network structures since resulting models unsuitable practical purposes due exponential amount time required reasoning task instead restrict special class simple treestructured bayesian networks called bayesian prototype trees polynomial time algorithm bayesian reasoning exists we show probability given bayesian prototype tree model evaluated given data evaluation criterion used stochastic simulated annealing algorithm searching model space the simulated annealing algorithm provably finds maximal probability model provided sufficient amount time used
in many applications decision support negotiation planning scheduling etc one needs express requirements partially satisfied in order express requirements propose technique called forwardtracking intuitively forwardtracking kind dual chronological backtracking program globally fails find solution new execution started program point state forward computation tree this search technique applied constraint logic programming obtaining powerful extension preserves useful properties original scheme we report successful practical application forwardtracking evolutionary training constrained neural networks
local search algorithms combinatorial search problems frequently encounter sequence states impossible improve value objective function moves regions called plateau moves dominate time spent local search we analyze characterize plateaus three different classes randomly generated boolean satisfiability problems we identify several interesting features plateaus impact performance local search algorithms we show local minima tend small occasionally may large we also show local minima escaped without unsatisfying large number clauses systematically searching escape route may computationally expensive local minimum large we show plateaus exits called benches tend much larger minima benches exit states local search use escape we show solutions ie global minima randomly generated problem instances form clusters behave similarly local minima we revisit several enhancements local search algorithms explain performance light results finally discuss strategies creating next generation local search algorithms
visual objects perceived parts correctly identified integrated a neural network theory proposed seeks explain human visual system binds together visual properties dispersed space time multiple objects problem known temporal binding problem the proposed theory based upon neural mechanisms construct update object representations interactions serial attentional mechanism location objectbased selection preattentive gestaltbased grouping mechanisms associative memory structure binds together object identity form spatial information a working model presented provides unified quantitative explanation results psychophysical experiments object review object integration multielement tracking
we consider two methods tracing genetic algorithms the first method based expected values bit products second method expected values walsh products we treat proportional selection mutation uniform onepoint crossover as applications obtain results stable points fitness schemata
routing problems important class planning problems usually many different constraints optimization criteria involved difficult find general methods solving routing problems we propose evolutionary solver planning problems an instance solver tested specific routing problem time constraints the performance evolutionary solver compared biased random solver biased hillclimber solver results show evolutionary solver performs significantly better two solvers
we investigating possible modes cooperation among homogeneous agents learning capabilities in paper focused agents learn solve problems using casebased reasoning cbr present two modes cooperation among distributed casebased reasoning distcbr collective casebased reasoning colcbr we illustrate modes application different cbr agents able recommend chromatography techniques protein purification cooperate the approach taken extend noos representation language used cbr agents noos knowledge modeling framework designed integrate learning methods based taskmethod decomposition principle the extension present plural noos allows communication cooperation among agents implemented noos means three basic constructs alien references foreign method evaluation mobile methods
bayesian network inference formulated combinatorial optimization problem concerning computation optimal factoring distribution represented net since determination optimal factoring computationally hard problem heuristic greedy strategies able find approximations optimal factoring usually adopted in present paper investigate alternative approach based combination genetic algorithms ga casebased reasoning cbr we show use genetic algorithms improve quality computed factoring case static strategy used mpe computation combination ga cbr still provide advantages case dynamic strategies some preliminary results different kinds nets reported
this paper attempts rigorously determine computation communication requirements connectionist algorithms running distributedmemory machine the strategy involves specifying key connectionist algorithms highlevel objectoriented language extracting running times polynomials analyzing polynomials determine algorithms space time complexity results presented various implementations backpropagation algorithm
we present new adaptive connectionist planning method by interaction environment world model progressively constructed using backpropagation learning algorithm the planner constructs lookahead plan iteratively using model predict future reinforcements future reinforcement maximized derive suboptimal plans thus determining good actions directly knowledge model network strategic level this done gradient descent action space
in proceedings conference uncertainty artificial intelli gence uai seattle wa july technical report rb april abstract evaluation counterfactual queries eg if a true would c true important fault diagnosis planning determination liability in paper present methods computing probabilities queries using formulation proposed balke pearl antecedent query interpreted external action forces proposition a true when prior probability available causal mechanisms governing domain counterfactual probabilities evaluated precisely however causal knowledge specified conditional probabilities observables bounds computed this paper develops techniques evaluating bounds demonstrates use two applications determination treatment efficacy studies subjects may choose treatment determination liability productsafety litigation
the foremost goal superscalar processor design increase performance exploitation instructionlevel parallelism ilp previous studies shown speculative execution required high instruction per cycle ipc rates nonnumerical applications the general trend toward supporting speculative execution complicated dynamicallyscheduled processors performance though high ipc rate also depends upon instruction count cycle time boosting architectural technique supports general speculative execution simpler staticallyscheduled processors boosting labels speculative instructions control dependence information this labelling eliminates control dependence constraints instruction scheduling still providing full dependence information hardware we incorporated boosting tracebased global scheduling algorithm exploits ilp without adversely affecting instruction count program we use algorithm estimates boosting hardware involved evaluate much speculative execution support really necessary achieve good performance we find staticallyscheduled superscalar processor using minimal implementation boosting easily reach performance much complex dynamicallyscheduled superscalar processor
this research sponsored part national science foundation award iri wright laboratory aeronautical systems center air force materiel command usaf advanced research projects agency arpa grant number f the views conclusions contained document author interpreted necessarily representing official policies endorsements either expressed implied nsf wright laboratory united states government
we investigate application classification techniques utility elicitation in decision problem two sets parameters must generally elicited probabilities utilities while prior conditional probabilities model change user user utility models thus necessary elicit utility model separately new user elicitation long tedious particularly outcome space large decomposable there two common approaches utility function elicitation the first base determination users utility function solely elicitation qualitative preferences the second makes assumptions form decomposability utility function here take different approach attempt identify new users utility function based classification relative database previously collected utility functions we identifying clusters utility functions minimize appropriate distance measure having identified clusters develop classification scheme requires many fewer simpler assessments full utility elicitation robust utility elicitation based solely preferences we tested algorithm small database utility functions prenatal diagnosis domain results quite promising
the standard method training hidden markov models optimizes point estimate model parameters this estimate viewed maximum posterior probability density model parameters may susceptible overfitting contains indication parameter uncertainty also maximum may unrepresentative posterior probability distribution in paper study method optimize ensemble approximates entire posterior probability distribution the ensemble learning algorithm requires the traditional training algorithm hidden markov models expectationmaximization em algorithm dempster et al known baumwelch algorithm it maximum likelihood method simple modification penalized maximum likelihood method viewed maximizing posterior probability density model parameters recently hinton van camp developed technique known ensemble learning see also mackay review whereas maximum posteriori methods optimize point estimate parameters ensemble learning ensemble optimized approximates entire posterior probability distribution parameters the objective function optimized variational free energy feynman measures relative entropy approximating ensemble true distribution in paper derive test ensemble learning algorithm hidden markov models building neal resources traditional baumwelch algorithm
a quantitative model provided psychophysical data tracking multiple visual elements multielement tracking the model employs objectbased attentional mechanism constructing updating object representations the model selectively enhances neural activations serially construct update internal representations objects correlationbased changes synaptic weights the correspondence problem items memory elements visual input resolved combination topdown prediction signals bottomup grouping processes simulations model image sequences used multielement tracking experiments show reported results consistent serial tracking mechanism based psychophysical neurobiological findings in addition simulations show observed effects perceptual grouping tracking accuracy may result interactions attentionguided predictions object location motion grouping processes involved solving motion correspondence problem
this paper considers design analysis adaptive wavelet control algorithms uncertain nonlinear dynamical systems the lyapunov synthesis approach used develop statefeedback adaptive control scheme based nonlinearly parametrized wavelet network models semiglobal stability results obtained key assumption system uncertainty satisfies matching condition the localization properties adaptive networks discussed formal definitions interference localization measures proposed
temporal difference td methods constitute class methods learning predictions multistep prediction problems parameterized recency factor currently important application methods temporal credit assignment reinforcement learning well known reinforcement learning algorithms ahc qlearning may viewed instances td learning this paper examines issues efficient general implementation td arbitrary use reinforcement learning algorithms optimizing discounted sum rewards the traditional approach based eligibility traces argued suffer inefficiency lack generality the ttd truncated temporal differences procedure proposed alternative indeed approximates td requires little computation per action used arbitrary function representation methods the idea derived fairly simple new probably unexplored far encouraging experimental results presented suggesting using gt ttd procedure allows one obtain significant learning speedup essentially cost usual td learning
this paper presents methodological point view first results interdisciplinary project scientific data mining we analyze data carcinogenicity chemicals derived carcinogenesis bioassay program longterm research study performed us national institute environmental health sciences the database contains detailed descriptions tests performed compounds animals different species strains sexes the chemical structures described atom bond level terms various relevant structural properties the goal paper investigate effects various levels detail amounts information resulting hypotheses quantitatively qualitatively we apply relational propositional machine learning algorithms learning problems formulated regression classification tasks in addition experiments conducted two learning problems different levels detail quantitatively experiments indicate additional information necessarily improves accuracy qualitatively number potential discoveries made algorithm relational regression forced abstract details contained relations database
neural networks bayesian inference provide useful framework within solve regression problems however parameterization means bayesian analysis neural networks difficult in paper investigate method regression using gaussian process priors allows exact bayesian analysis using matrix manipulations we discuss workings method detail we also detail range mathematical numerical techniques useful applying gaussian processes general problems including efficient approximate matrix inversion methods developed skilling
prototypes proposed representation concepts used effectively humans developing computational schemes generating prototypes examples however proved difficult problem we present novel genetic algorithm based prototype learning system please constructing appropriate prototypes classified training instances after constructing set prototypes possible classes class new input instance determined nearest prototype instance attributes assumed ordinal nature prototypes represented sets featurevalue pairs a genetic algorithm used evolve number prototypes per class positions input space we present experimental results series artificial problems varying complexity please performs competitively several nearest neighbor classification algorithms problem set an analysis strengths weaknesses initial version system motivates need additional operators the inclusion operators substantially improves performance system particularly difficult problems
in paper problem asymptotic identification fading memory systems presence bounded noise studied for experiment worstcase error characterized terms diameter worstcase uncertainty set optimal inputs minimize radius uncertainty studied characterized finally convergent algorithm require knowledge noise upper bound furnished the algorithm based interpolating data spline functions shown well suited identification presence bounded noise basis functions polynomials
this paper describes rapture system revising probabilistic knowledge bases combines connectionist symbolic learning methods rapture uses modified version backpropagation refine certainty factors probabilistic rule base uses ids informationgain heuristic add new rules results refining three actual expert knowledge bases demonstrate combined approach generally performs better previous methods
in open world applications number machinelearning techniques may potentially apply given learning situation the research presented illustrates complexity involved automatically choosing appropriate technique multistrategy learning system it also constitutes step toward general computational solution learningstrategy selection problem the approach treat learningstrategy selection separate planning problem set goals case ordinary problemsolvers therefore management pursuit learning goals becomes central issue learning similar goalmanagement problems associated traditional planning systems this paper explores issues problems possible solutions framework examples presented multistrategy learning system called metaaqua
we present empirical evidence considering volatility eurodollar futures stochastic process requiring generalization standard blackscholes bs model treats volatility constant we use previous development statistical mechanics financial markets smfm model issues
we examine new approach modeling uncertainty based plausibility measures plausibility measure associates event plausibility element partially ordered set this approach easily seen generalize approaches modeling uncertainty probability measures belief functions possibility measures the lack structure plausibility measure makes easy us add structure needed basis letting us examine required ensure plausibility measure certain properties interest this gives us insight essential features properties question allowing us prove general results apply many approaches reasoning uncertainty plausibility measures already proved useful analyzing default reasoning in paper examine algebraic properties analogues use fi probability theory an understanding properties essential plausibility measures used practice representation tool
in paper describe number intelligent data analysis techniques preprocess analyze data coming home monitoring diabetic patients in particular show combination temporal abstractions statistical probabilistic techniques may applied derive useful summaries patients behaviour certain monitoring period finally describe intelligent data analysis methods may used index past cases perform casebased trieval database past cases
multipleinstance learning variation supervised learning task learn concept given positive negative bags instances each bag may contain many instances bag labeled positive even one instances falls within concept a bag labeled negative instances negative we describe new general framework called diverse density solving multipleinstance learning problems we apply framework learn simple description person series images bags containing person stock selection problem drug activity prediction problem
restrictions number depth existential variables defined language series clint rae widely used ilp expected produce considerable reduction size hypothesis space in paper show generally case the lower bounds present lead intractable hypothesis spaces except toy domains we argue parameters chosen clint unsuitable sensible bias shift operations propose alternative approaches resulting desired reduction hypothesis space allowing natural integration shift bias
this paper discussion relationship learning forgetting an analysis economics learning carried argued knowledge sometimes negative value a series experiments involving program learns traverse state spaces described it shown knowledge acquired negative value even though correct acquired solving similar problems it shown value knowledge depends else known random forgetting sometimes lead substantial improvements performance it concluded research knowledge acquisition take seriously possibility knowledge may sometimes harmful the view taken learning forgetting complementary processes construct maintain useful representations experience
we apply general technique learning overcomplete bases problem finding efficient image codes the bases learned algorithm localized oriented bandpass consistent earlier results obtained using related methods we show learned bases gaborlike structure higher degrees overcompleteness produce greater sampling density position orientation scale the efficient coding framework provides method comparing different bases objectively calculating probability given observed data measuring entropy basis function coefficients compared complete overcomplete fourier wavelet bases learned bases much better coding efficiency we demonstrate improvement representation learned bases showing superior performance image denoising fillingin missing pixels
in paper problem kolmogorov complexity related binary strings faced we propose genetic programming approach consists evolving population lisp programs looking optimal program generates given string this evolutionary approach permited overcome intractable space time difficulties occurring methods perform approximation kolmogorov complexity function the experimental results quite significant also show interesting computational strategies proving effectiveness implemented technique
most inductive inference algorithms ie learners work effectively training data contain completely specified labeled samples in many diagnostic tasks however data include values attributes model blocking process hides values attributes learner while blockers remove values critical attributes handicap learner paper instead focuses blockers remove superfluous attribute values ie values needed classify instance given values unblocked attributes we first motivate formalize model superfluousvalue blocking demonstrate omissions useful showing certain classes seem hard learn general pac model viz decision trees trivial learn setting even learned manner robust classification noise we also discuss model extended deal theory revision ie modifying existing decision tree complex attributes correspond combinations atomic attributes blockers occasionally include superfluous values exclude required values hypothesis classes eg dnf formulae declaration this paper already accepted currently review journal another conference submitted ijcais review period fl this extended version paper appeared working notes aaai fall symposium relevance new orleans november authors listed alphabetically we gratefully acknowledge receiving helpful comments dale schuurmans george drastal
we propose casebased method selecting behavior sets addition traditional reactive robotic control systems the new system acbarr a case based reactive robotic system provides flexible performance novel environments well overcoming standard hard problem reactive systems box canyon additionally acbarr designed manner intended remain close pure reactive control possible higher level reasoning memory functions intentionally kept minimum as result new reasoning significantly slow system pure reactive speeds
when using machine learning techniques knowledge discovery output comprehensible human important predictive accuracy we introduce new algorithm setgen improves comprehensibility decision trees grown standard c without reducing accuracy it using genetic search select set input features c allowed use build tree we test setgen wide variety realworld datasets show setgen trees significantly smaller reference significantly fewer features trees grown c without using setgen statistical significance tests show accuracies setgens trees either distinguishable accurate original c trees ten datasets tested
we present method automatically determining structure connection weights boltzmann machine corresponding given bayesian network representation probability distribution set discrete variables the resulting boltzmann machine structure implemented efficiently massively parallel hardware since structure divided two separate clusters nodes one cluster updated simultaneously the updating process boltzmann machine approximates gibbs sampling process original bayesian network sense boltzmann machine converges final state gibbs sampler the mapping bayesian network boltzmann machine seen method incorporating probabilistic priori information neural network architecture trained existing learning algorithms
dori d tarsi m a simple algorithm construct consistent extension partially oriented graph computer science department telaviv university also technical report r ucla cognitive systems laboratory october pearl j wermuth n when can association graphs admit causal interpretation ucla cognitive systems laboratory technical report rl november verma ts pearl j deciding morality graphs npcomplete technical report r ucla cognitive systems laboratory october
a default theory sanction different mutually incompatible answers certain queries we identify theory set related credulous theories produces single response query imposing total ordering defaults our goal identify credulous theory optimal expected accuracy averaged natural distribution queries domain there two obvious complications first expected accuracy theory depends query distribution usually known second task identifying optimal theory even given distribution information intractable this paper presents method optacc sidesteps problems using set samples estimate unknown distribution hillclimbing local optimum in particular given error confidence parameters ffi gt optacc produces theory whose expected accuracy probability least
given problem casebased reasoning cbr system search case memory use stored cases find solution possibly modifying retrieved cases adapt required input specifications in discrete domains cbr reasoning based rigorous bayesian probability propagation algorithm such bayesian cbr system implemented probabilistic feedforward neural network one layers representing cases in paper introduce minimum description length mdl based learning algorithm obtain proper network structure associated conditional probabilities this algorithm together resulting neural network implementation provide massively parallel architecture solving efficiency bottleneck casebased reasoning
working paper is leonard n stern school business new york university in decision technologies financial engineering proceedings fourth international conference neural networks capital markets nncm pp edited asweigend ysabumostafa apnrefenes singapore world scientific httpwwwsternnyueduaweigendresearchpaperssharperatio while many trading strategies based price prediction traders financial markets typically interested riskadjusted performance sharpe ratio rather price predictions this paper introduces approach generates nonlinear strategy explicitly maximizes sharpe ratio it expressed neural network model whose output position size risky riskfree asset the iterative parameter update rules derived compared alternative approaches the resulting trading strategy evaluated analyzed computergenerated data real world data dax daily german equity index trading based sharpe ratio maximization compares favorably profit optimization probability matching crossentropy optimization the results show goal optimizing outofsample riskadjusted profit achieved nonlinear approach
randomized adaptive greedy search using evolutionary algorithms offers powerful versatile approach automated design neural network architectures variety tasks artificial intelligence robotics in paper present results evolutionary design neurocontroller robotic bulldozer this robot given task clearing arena littered boxes pushing boxes sides through careful analysis evolved networks show evolution exploits design constraints properties environment produce network structures high fitness we conclude brief summary related ongoing research examining intricate interplay environment evolutionary processes determining structure function resulting neural architectures
in paper present framework definition similarity measures using latticevalued functions we show strengths particularly combining similarity measures then investigate particular instantiation framework sets used represent objects denote degrees similarity the paper con cludes suggesting generalisations findings
we investigate solution constraintbased configuration problems preference function outcomes unknown incompletely specified the aim configure system personal computer optimal given user the goal project develop algorithms generate preferred feasible configuration posing preference queries user in order minimize number complexity preference queries posed user algorithm reasons users preferences taking account constraints set feasible configurations we assume user structure preferences particular way natural many settings exploited optimization process we also address preliminary fashion tradeoffs computational effort solution problem degree interaction user
selfselection input examples basis performance failure powerful bias learning systems the definition constitutes learning bias however typically restricted bias provided input language hypothesis language preference criteria competing concept hypotheses but bias taken broader context basis provides preference one concept change another paradigm failuredriven processing indeed provides bias bias exhibited selection examples input stream examples failure successful performance filtered we show degrees freedom less failuredriven learning successdriven learning learning facilitated constraint we also broaden definition failure provide novel taxonomy failure causes illustrate interaction multistrategy learning system called metaaqua
we previously introduced gamma mlp defined mlp usual synaptic weights replaced gamma filters associated gain terms throughout layers in paper apply gamma mlp larger scale speech phoneme recognition problem analyze operation network investigate gamma mlp perform better alternatives the gamma mlp capable employing multiple temporal resolutions temporal resolution defined per de vries principe number parameters freedom ie number tap variables per unit time gamma memory equal gamma memory parameter detailed paper multiple temporal resolutions may advantageous certain problems eg different resolutions may optimal extracting different features input data for problem paper gamma mlp observed use large range temporal resolutions in comparison tdnn networks typically use single temporal resolution further motivation gamma mlp related curse dimensionality ability gamma mlp trade temporal resolution memory depth therefore increase memory depth without increasing dimensionality network the iir mlp general version gamma mlp however iir mlp performs poorly problem paper investigation suggests error surface gamma mlp suitable gradient descent training error surface iir mlp
we present fast algorithm nonlinear dimension reduction the algorithm builds local linear model data merging pca clustering based new distortion measure experiments speech image data indicate local linear algorithm produces encodings lower distortion built five layer autoassociative networks the local linear algorithm also order magnitude faster train
wavelets wide potential use statistical contexts the basics discrete wavelet transform reviewed using filter notation useful subsequently paper a stationary wavelet transform coefficient sequences decimated stage described two different approaches construction inverse stationary wavelet transform set the application stationary wavelet transform exploratory statistical method discussed together potential use nonparametric regression a method local spectral density estimation developed this involves extensions wavelet context standard time series ideas periodogram spectrum the technique illustrated application data sets astronomy veterinary anatomy
business users analysts commonly use spreadsheets d plots analyze understand data online analytical processing olap provides users added flexibility pivoting data around different attributes drilling multidimensional cube aggregations machine learning researchers however concentrated hypothesis spaces foreign users hyperplanes perceptrons neural networks bayesian networks decision trees nearest neighbors etc in paper advocate use decision table classifiers easy lineofbusiness users understand we describe several variants algorithms learning decision tables compare performance describe visualization mechanism implemented mineset the performance decision tables comparable known algorithms cc yet resulting classifiers use fewer attributes comprehensible
the va management services department invests considerably collection assessment data inform hospital carearea specific levels quality care resulting time series quality monitors provide information relevant evaluating patterns variability hospitalspecific quality care time across care areas compare assess differences across hospitals in collaboration va management services group developed various models evaluating patterns dependencies combining data across va hospital system this paper provides brief overview resulting models summary examples three monitor time series discussion data modelling inference issues this work introduces new models multivariate nongaussian time series the framework combines crosssectional hierarchical models population hospitals time series structure allow measure timevariations associated hierarchical model parameters in va study withinyear components models describe patterns heterogeneity across population hospitals relationships among several monitors time series components describe patterns variability time hospitalspecific effects relationships across quality monitors additional model components isolate unpredictable aspects variability quality monitor outcomes hospital care areas we discuss model assessment residual analysis mcmc algorithms developed fit models interest related applications socioeconomic areas
we report development highperformance system neural network signal processing applications we designed implemented vector microprocessor packaged attached processor conventional workstation we present performance comparisons commercial workstations neural network backpropagation training the spertii system demonstrates significant speedups extensively hand optimization code running workstations
a pure rulebased program return set answers query return answer set even rules reordered however impure program includes prolog cut operators return different answers rules reordered there also many reasoning systems return first answer found query first answers depend rule order even pure rulebased systems a theory revision algorithm seeking revised rulebase whose expected accuracy distribution queries optimal therefore consider modifying order rules this paper first shows polynomial number training labeled queries query coupled correct answer provides distribution information necessary identify optimal ordering it proves however task determining ordering optimal given information intractable even trivial situations eg even query atomic literal seeking perfect theory rule base propositional we also prove task even approximable unless p n p polynomial time algorithm produce ordering nrule theory whose accuracy within n fl optimal fl gt we also prove similar hardness nonapproximatability results related tasks determining impure contexts optimal ordering antecedents optimal set rules add delete optimal priority values set defaults
identifying open research issues field necessary step progress field this paper describes four open research problems computational models precedentbased legal reasoning relating case representation precedent use modeling selection construction arguments based pairwise case comparison multipleprecedent arguments modeling process whereby purposes policies principles used case similarity assessment extending applicability precedents tasks classification
financial forecasting example signal processing problem challenging due small sample sizes high noise nonstationarity nonlinearity neural networks successful number signal processing applications we discuss fundamental limitations inherent difficulties using neural networks processing high noise small sample size signals we introduce new intelligent signal processing method addresses difficulties the method uses conversion symbolic representation selforganizing map grammatical inference recurrent neural networks we apply method prediction daily foreign exchange rates addressing difficulties nonstationarity overfitting unequal priori class probabilities find significant predictability comprehensive experiments covering different foreign exchange rates the method correctly predicts direction change next day error rate the error rate reduces around rejecting examples system low confidence prediction the symbolic representation aids extraction symbolic knowledge recurrent neural networks form deterministic finite state automata these automata explain operation system often relatively simple rules related well known behavior trend following mean reversal extracted
coins technical report february abstract the problem learn examples studied throughout history machine learning many successful learning algorithms developed a problem received less attention select algorithm use given learning task the ability chosen algorithm induce good generalization depends appropriate model class underlying algorithm given task we define algorithms model class representation language uses express generalization examples supervised learning algorithms differ underlying model class search good generalization given characterization surprising algorithms find better generalizations tasks therefore order find best generalization task automated learning system must search appropriate model class addition searching best generalization within chosen class this thesis proposal investigates issues involved automating selection appropriate model class the presented approach two facets firstly approach combines different model classes form model combination decision tree allows best representation found subconcept learning task secondly model class appropriate determined dynamically using set heuristic rules explicit rule conditions particular model class appropriate done next in addition describing approach proposal describes approach evaluated order demonstrate efficient effective method automatic model selection
the development nervous system involves many cases interactions local scale rather execution fully specified genetic blueprint the problem discover nature interactions factors depend the withdrawal polyinnervation developing muscle example competitive interactions play important role we examine possible types competition formal
this paper presents new approach inductive learning combines aspects instancebased learning rule induction single simple algorithm the rise system searches rules specifictogeneral fashion starting one rule per training example avoids difficulties separateandconquer approaches evaluating proposed induction step globally ie efficient procedure equivalent checking accuracy rule set whole every training example classification performed using bestmatch strategy reduces nearestneighbor generalizations instances rejected an extensive empirical study shows rise consistently achieves higher accuracies stateoftheart representatives parent paradigms pebls cn also outperforms decisiontree learner c test domains
most research machine learning focused scenarios learner faces single isolated learning task the lifelong learning framework assumes instead learner encounters multitude related learning tasks lifetime providing opportunity transfer knowledge this paper studies lifelong learning context binary classification it presents invariance approach knowledge transferred via learned model invariances domain results learning recognize objects color images demonstrate superior generalization capabilities invariances learned used bias subsequent learning this research sponsored part national science foundation award iri wright laboratory aeronautical systems center air force materiel command usaf advanced research projects agency arpa grant number f views conclusions contained document authors interpreted necessarily representing official policies endorsements either expressed implied nsf wright laboratory united states government
previous bias shift approaches predicate invention applicable learning positive examples complete hypothesis found given language negative examples required determine whether new predicates invented one approach problem presented merlin successor system predicate invention guided sequences input clauses sldrefutations positive negative examples wrt overly general theory in contrast predecessor searches minimal finitestate automaton generate positive negative sequences merlin uses technique inducing hidden markov models positive sequences this enables system invent new predicates without triggered negative examples another advantage using induction technique allows incremental learning experimental results presented comparing merlin positive learning framework progol comparing original induction technique new version produces deterministic hidden markov models the results show predicate invention may indeed necessary possible learning positive examples well beneficial keep induced model deterministic
we present algorithms learn certain classes functionfree recursive logic programs polynomial time equivalence queries in particular show single kary recursive constantdepth determinate clause learnable twoclause programs consisting one learnable recursive clause one constantdepth determinate nonrecursive clause also learnable additional basecase oracle assumed these results immediately imply paclearnability classes although classes learnable recursive programs constrained shown companion paper maximally general generalizing either class natural way leads computationally difficult learning problem thus taken together companion paper paper establishes boundary efficient learnability recursive logic programs
we present evaluate two methods improving performance ilp systems one discretization numerical attributes based fayyad iranis text adapted extended way cope aspects discretization occur relational learning problems indeterminate literals occur the second technique lookahead it wellknown problem ilp learner always assess quality refinement without knowing refinements enabled afterwards ie without looking ahead refinement lattice we present simple method specifying lookahead used kind lookahead interesting both discretization lookahead techniques evaluated experimentally the results show techniques improve quality induced theory computational costs acceptable
this paper analyses recently suggested particle approach filtering time series we suggest algorithm robust outliers two reasons design simulators use discrete support represent sequentially updating prior distribution both problems tackled paper we believe largely solved first problem reduced order magnitude second in addition introduce idea stratification particle filter allows us perform online bayesian calculations parameters index models maximum likelihood estimation the new methods illustrated using stochastic volatility model time series model angles
in paper suggest determinations representation knowledge easy understand we briefly review determinations displayed tabular format use prediction involves simple matching process we describe condet algorithm uses feature selection construct determinations training data augmented condensation process collapses rows produce simpler structures we report experiments show condensation reduces complexity loss accuracy discuss condets relation work outline directions future studies
recurrent neural networks complex parametric dynamic systems exhibit wide range different behavior we consider task grammatical inference recurrent neural networks specifically consider task classifying natural language sentences grammatical ungrammatical recurrent neural network made exhibit kind discriminatory power provided principles parameters linguistic framework government binding theory we attempt train network without bifurcation learned vs innate components assumed chomsky produce judgments native speakers sharply grammaticalungrammatical data we consider recurrent neural network could possess linguistic capability investigate properties elman narendra parthasarathy np williams zipser wz recurrent networks frasconigorisoda fgs locally recurrent networks setting we show
working paper is leonard n stern school business new york university in journal computational intelligence finance special issue improving generalization nonlinear financial forecasting models httpwwwsternnyueduaweigendresearchpapersinteractionlayer abstract predictive models financial data often based large number plausible inputs potentially nonlinearly combined yield conditional expectation target daily return asset this paper introduces new architecture task on output side predict dynamical variables first derivatives curvatures different time spans these subsequently combined interaction output layer form several estimates variable interest those estimates averaged yield final prediction independently idea input side propose new internal preprocessing layer connected diagonal matrix positive weights layer squashing functions these weights adapt input individually learn squash outliers input we apply two ideas real world example daily predictions german stock index dax deutscher aktien index compare results network single output the new six layer architecture stable training due two facts more information flowing back outputs input backward pass the constraint predicting first second derivatives focuses learning relevant variables dynamics the architectures compared training perspective squared errors robust errors trading perspective annualized returns percent correct sharpe ratio
we propose new method estimation linear models the lasso minimizes residual sum squares subject sum absolute value coefficients less constant because nature constraint tends produce coefficients exactly zero hence gives interpretable models our simulation studies suggest lasso enjoys favourable properties subset selection ridge regression it produces interpretable models like subset selection exhibits stability ridge regression there also interesting relationship recent work adaptive function estimation donoho johnstone the lasso idea quite general applied variety statistical models extensions generalized regression models treebased models briefly described
instancebased learning techniques typically handle continuous linear input values well often handle nominal input attributes appropriately the value difference metric vdm designed find reasonable distance values nominal attribute values largely ignores continuous attributes requiring discretization map continuous values nominal values this paper proposes three new heterogeneous distance functions called heterogeneous value difference metric hvdm interpolated value difference metric ivdm windowed value difference metric wvdm these new distance functions designed handle applications nominal attributes continuous attributes in experiments applications new distance metrics achieve higher classification accuracy average three previous distance functions datasets nominal continuous attributes
research utility noncoding segments introns geneticbased encodings shown expedite evolution solutions domains protecting building blocks destructive crossover we consider genetic programming system noncoding segments removed resultant chromosomes returned population this parsimonious repair leads premature convergence since remove naturally occurring noncoding segments strip away protective backup feature we duplicate coding segments repaired chromosomes place modified chromosomes population the duplication method significantly improves learning rate domain considered we also show method applied domains
this paper introduces new operation restricted iteration creation automatically genetic programming extends hollands genetic algorithm task automatic programming early work genetic programming demonstrated possible evolve sequence workperforming steps single resultproducing branch onepart main program the book genetic programming on programming computers means natural selection koza describes extension hollands genetic algorithm genetic population consists computer programs compositions primitive functions terminals see also koza rice in basic form genetic programming single resultproducing branch evolved genetic programming demonstrated capability discover sequence length content workperforming steps sufficient produce satisfactory solution several problems including many problems used years benchmarks machine learning artificial intelligence before applying genetic programming problem user must perform five major preparatory steps namely identifying terminals inputs tobeevolved programs identifying primitive functions operations contained tobeevolved programs creating fitness measure evaluating well given program solving problem hand choosing certain control parameters notably population size number generations run determining termination criterion method result designation typically bestsofar individual populations produced run creates restricted iterationperforming
inertia added continuoustime hopfield effectiveneuron system we explore effects stability fixed points system a two neuron system one two inertial terms added shown exhibit chaos the chaos confirmed lyapunov exponents power spectra phase space plots
this paper describes partialmemory incremental learning method based aqc inductive learning system the method maintains representative set past training examples used together new examples appropriately modify currently held hypotheses incremental learning evoked feedback environment user such method useful applications involving intelligent agents acting changing environment active vision dynamic knowledgebases for study method applied problem computer intrusion detection symbolic profiles learned computer systems users in experiments proposed method yielded significant gains terms learning time memory requirements expense slightly lower predictive accuracy higher concept complexity compared batch learning examples given
the genetic algorithm ga problem solving method modelled process natural selection we interested studying specific aspect ga effect noncoding segments ga performance noncoding segments segments bits individual provide contribution positive negative fitness individual previous research noncoding segments suggests including structures ga may improve ga performance understanding improvement occurs help us use ga full potential in article discuss hypotheses noncoding segments describe results experiments the experiments may separated two categories testing program problems previous related studies testing new hypotheses effect noncoding segments
we previously introduced exemplar model named gcmisw exploits highly flexible weighting scheme our simulations showed records faster learning rates higher asymptotic accuracies several artificial categorization tasks models limited abilities warp input spaces this paper extends previous work describes experimental results suggest human subjects also invoke highly flexible schemes in particular model provides significantly better fits models less flexibility hypothesize humans selectively weight attributes depending items location input space we need flexible models many theories human concept learning posit concepts represented prototypes reed exemplars medin schaffer prototype models represent concepts best example central tendency concept a new item belongs category c relatively similar cs prototype prototype models relatively inflexible discard great deal information people use concept learning eg number exemplars concept homa cultice variability features fried holyoak correlations features medin et al particular exemplars used whittlesea concept learning
current inductive logic programming systems limited handling noise employ greedy covering approach constructing hypothesis one clause time this approach also causes difficulty learning recursive predicates additionally many current systems implicit expectation cardinality positive negative examples reflect proportion concept instance space a framework learning noisy data fixed example size presented a bayesian heuristic finding probable hypothesis general framework derived this approach evaluates hypothesis whole rather one clause time the heuristic nice theoretical properties incorporated ilp system lime experimental results show lime handles noise better foil progol it able learn recursive definitions noisy data systems perform well lime also capable learning positive data also negative data
we present general formulation network stochastic directional units this formulation extension boltzmann machine units binary take values cyclic range radians this measure appropriate many domains representing cyclic angular values eg wind direction days week phases moon the state unit directionalunit boltzmann machine dubm described complex variable phase component specifies direction weights also complex variables we associate quadratic energy function corresponding probability dubm configuration the conditional distribution units stochastic state circular version gaussian probability distribution known von mises distribution in meanfield approximation stochastic dubm phase component units state represents mean direction magnitude component specifies degree certainty associated direction this combination value certainty provides additional representational power unit we present proof settling dynamics meanfield dubm cause convergence free energy minimum finally describe learning algorithm simulations demonstrate meanfield dubms ability learn interesting mappings fl to appear neural networks
for century known damage right hemisphere brain cause patients unaware contralesional side space this condition known unilateral neglect represents collection clinically related spatial disorders characterized failure free vision respond explore orient stimuli predominantly located side space opposite damaged hemisphere recent studies using simple task line bisection conventional diagnostic test proved surprisingly revealing respect spatial attentional impairments involved neglect in line bisection patient asked mark midpoint thin horizontal line sheet paper neglect patients generally transect far right center extensive studies line bisection conducted manipulatingamong factorsline length orientation position we simulated pattern results using existing computational model visual perception selective attention called morsel mozer morsel already used model data related disorder neglect dyslexia mozer behrmann in earlier work morsel lesioned accordance damage suppose occurred brains
this paper overviews proposed architecture adaptive parallel logic referred asocs adaptive selforganizing concurrent system the asocs approach based adaptive network composed many simple computing elements operate parallel asynchronous fashion problem specification given system presenting ifthen rules form boolean conjunctions rules added incrementally system adapts changing rulebase adaptation data processing form two separate phases operation during processing system acts parallel hardware circuit the adaptation process distributed amongst computing elements efficiently exploits parallelism adaptation done selforganizing fashion takes place time linear depth network this paper summarizes overall asocs concept overviews three specific architectures
we describe design tuning controller enforcing compliance prescribed velocity profile railbased transportation system this requires following trajectory rather fixed setpoints automobiles we synthesize fuzzy controller tracking velocity profile providing smooth ride staying within prescribed speed limits we use genetic algorithm tune fuzzy controllers performance adjusting parameters scaling factors membership functions sequential order significance we show approach results controller superior manually designed one modest computational effort this makes possible customize automated tuning variety different configurations route terrain power configuration cargo
the patientadaptive classifier compared wellestablished baseline algorithm six major databases consisting million heartbeats when trained initial records tested additional records patientadaptive algorithm found reduce number vn errors one channel factor number nv errors factor we conclude patient adaptation provides significant advance classifying normal vs ventricular beats ecg patient monitoring
this paper presents experiment comparing new namepronunciation system anapron seven existing systems three stateoftheart commercial systems bellcore bell labs dec two variants machinelearning system nettalk two humans anapron works combining rulebased casebased reasoning it based idea much easier improve rulebased system adding casebased reasoning tuning rules deal every exception in experiment described anapron used set rules adapted mitalk elementary foreignlanguage textbooks case library names with components required relatively little knowledge engineering anapron found perform almost level commercial systems significantly better two versions nettalk this work may copied reproduced whole part commercial purpose permission copy whole part without payment fee granted nonprofit educational research purposes provided whole partial copies include following notice copying permission mitsubishi electric research laboratories cambridge massachusetts acknowledgment authors individual contributions work applicable portions copyright notice copying reproduction republishing purpose shall require license payment fee mitsubishi electric research laboratories all rights reserved
this paper devoted problem learning predict ordinal ie ordered discrete classes ilp setting we start relational regression algorithm named srt structural regression trees study various ways transforming firstorder learner ordinal classification tasks combinations algorithm variants several data preprocessing methods compared two ilp benchmark data sets verify relative strengths weaknesses strategies study tradeoff optimal categorical classification accuracy hit rate minimum distancebased error preliminary results indicate promising avenue towards algorithms combine aspects classification regression relational learning
learning problems text processing domain often map text space whose dimensions measured features text eg words three characteristic properties domain high dimensionality b learned concepts instances reside sparsely feature space c high variation number active features instance in work study three mistakedriven learning algorithms typical task nature text categorization we argue algorithms categorize documents learning linear separator feature space properties make ideal domain we show quantum leap performance achieved modify algorithms better address specific characteristics domain in particular demonstrate variation document length tolerated either normalizing feature weights using negative weights positive effect applying threshold range training alternatives considering feature frequency benefits discarding features training overall present algorithm variation littlestones winnow performs significantly better algorithm tested task using similar feature set
we show networks relatively realistic mathematical models biological neurons principle simulate arbitrary feedforward sigmoidal neural nets way previously considered this new approach based temporal coding single spikes respectively timing synchronous firing pools neurons rather traditional interpretation analog variables terms firing rates the resulting new simulation substantially faster hence consistent experimental results maximal speed information processing cortical neural systems as consequence show networks noisy spiking neurons universal approximators sense approximate regard temporal coding given continuous function several variables this result holds fairly large class schemes coding analog variables firing times spiking neurons our new proposal possible organization computations networks spiking neurons systems interesting consequences type learning rules would needed explain selforganization networks finally fast noiserobust implementation sigmoidal neural nets via temporal coding points possible new ways implementing feedforward recurrent sigmoidal neural nets pulse stream vlsi
in framework functional response model ie regression model feedforward neural network estimator nonlinear response function constructed set functional units the parameters defining functional units estimated using bayesian approach a sample representing bayesian posterior distribution obtained applying markov chain monte carlo procedure namely combination gibbs metropolishastings algorithms the method described histogram bspline radial basis function estimators response function in general proposed approach suitable finding bayesoptimal values parameters complicated parameter space we illustrate method numerical examples
selecting set features optimal given classification task one central problems machine learning we address problem using flexible robust filter technique eubafes eubafes based feature weighting approach computes binary feature weights therefore solution feature selection sense also gives detailed information feature relevance continuous weights moreover user gets one several potentially optimal feature subsets important filterbased feature selection algorithms since gives flexibility use even complex classifiers application combined filterwrapper approach we applied eubafes number artificial real world data sets used radial basis function networks examine impact feature subsets classifier accuracy complexity
a machine learn biased way typically bias supplied hand example choice appropriate set features however learning machine embedded within environment related tasks learn bias learning sufficiently many tasks environment in paper two models bias learning equivalently learning learn introduced main theoretical results presented the first model pactype model based empirical process theory second hierarchical bayes model
this paper compares efficiency two encoding schemes artificial neural networks optimized evolutionary algorithms direct encoding encodes weights priori fixed neural network architecture cellular encoding encodes weights architecture neural network in previous studies direct encoding cellular encoding used create neural networks balancing poles attached cart fixed track the poles balanced controller pushes cart left right in cases velocity information pole cart provided input cases network must learn balance single pole without velocity information a careful study behavior systems suggests possible balance single pole velocity information input without learning compute velocity a new fitness function introduced forces neural network compute velocity by using new fitness function tuning syntactic constraints used cellular encoding achieve tenfold speedup previous study solve difficult problem balancing two poles information velocity provided input
demands applications requiring massive parallelism symbolic environments given rebirth research models labeled neura l networks these models made many simple nodes highly interconnected computation takes place data flows amongst nodes network to present models proposed nodes based simple analog functions inputs multiplied weights summed total optionally transformed arbitrary function node learning systems accomplished adjusting weights input lines this paper discusses use digital boolean nodes primitive building block connectionist systems digital nodes naturally engender new paradigms mechanisms learning processing connectionist networks the digital nodes used basic building block class models called asocs adaptive selforganizing concurrent systems these models combine massive parallelism ability adapt selforganizing fashion basic features standard neural network learning algorithms proposed using digital nodes compared contrasted the latter mechanisms lead vastly improved efficiency many applications
many abductive understanding systems explain novel situations chaining process neutral explainer needs beyond generating plausible explanation event explained this paper examines relationship standard models abductive understanding casebased explanation model in casebased explanation construction selection abductive hypotheses focused specific explanations prior episodes goalbased criteria reflecting current information needs the casebased method inspired observations human explanation anomalous events everyday understanding paper focuses methods contributions problems building good explanations everyday domains we identify five central issues compare issues addressed traditional casebased explanation models discuss motivations using casebased approach facilitate generation plausible useful explanations domains complex imperfectly un derstood
previous research shown technique called errorcorrecting output coding ecoc dramatically improve classification accuracy supervised learning algorithms learn classify data points one k classes in paper extend technique ecoc also provide class probability information ecoc method converting kclass supervised learning problem large number l twoclass supervised learning problems combining results l evaluations the underlying twoclass supervised learning algorithms assumed provide l probability estimates the problem computing class probabilities formulated overconstrained system l linear equations least squares methods applied solve equations accuracy reliability probability estimates demonstrated
in paper propose reactive critic able respond changing situations we explain usefull reinforcement learning critic used improve control strategy we take problem derive solution analytically this enables us investigate relation parameters resulting approximations critic we also demonstrate reactive critic reponds changing situations
quadratic dynamical systems qds whose definition extends markov chains used model phenomena variety fields like statistical physics natural evolution such systems also play role genetic algorithms widelyused class heuristics notoriously hard analyze recently rabinovich et al took important step study qdss showing technical assumptions systems converge stationary distribution similar theorems markov chains wellknown we show however following sampling problem qdss pspacehard given initial distribution produce random sample tth generation the hardness result continues hold restricted classes qdss simple initial distributions thus suggesting qdss intrinsically complicated markov chains
hublers technique using aperiodic forces drive nonlinear oscillators resonance analyzed the oscillators examined effective neurons model hopfield neural networks the method shown valid several different circumstances it verified analysis power spectrum force resonance energy transfer system
an extended version dual constraint model motor endplate morphogenesis presented includes activity dependent independent competition it supported wide range recent neurophysiological evidence indicates strong relationship synaptic efficacy survival the computational model justified molecular level predictions match developmental regenerative behaviour real synapses
we consider problem learning functions fixed distribution an algorithm kushilevitz mansour learns boolean function f g n time polynomial l norm fourier transform function we show kmalgorithm special case general class learning algorithms this achieved extending ideas using representations finite groups we introduce new classes functions learned using generalized km algorithm
standard statistical practice ignores model uncertainty data analysts typically select model class models proceed selected model generated data this approach ignores uncertainty model selection leading overconfident inferences decisions risky one thinks bayesian model averaging bma provides coherent mechanism accounting model uncertainty several methods implementing bma recently emerged we discuss methods present number examples in examples bma provides improved outofsample predictive performance we also provide catalogue currently available bma software
with rapid expansion machine learning methods applications strong need computerbased interactive tools support education area the emerald system developed provide handson experience interactive demonstration several machine learning discovery capabilities students ai cognitive science ai professionals the current version emerald integrates five programs exhibit different types machine learning discovery learning rules examples determining structural descriptions object classes inventing conceptual clusterings entities predicting sequences objects discovering equations characterizing collections quantitative qualitative data emerald extensively uses color graphic capabilities voice synthesis natural language representation knowledge acquired learning programs each program presented learning robot personality expressed icon voice comments generates learning process results learning presented natural language text andor voice output users learn capabilities robot challenged perform learning tasks creating similar tasks challenge robot emerald extension illian initial much smaller version toured eight major us museums science seen half million visitors emeralds architecture allows incorporate new programs new capabilities the system runs sun workstations available universities educational institutions
the nozzle design associate nda computational environment design jet engine exhaust nozzles supersonic aircraft nda may used either design new aircraft design new nozzles adapt existing aircraft may reutilized new missions nda developed collaboration computer scientists rutgers university exhaust nozzle designers general electric aircraft engines general electric corporate research development the nda project two principal goals provide useful engineering tool exhaust nozzle design explore fundamental research issues arise application automated design optimization methods realistic engineering problems
the learning many visual perceptual tasks motion discrimination shown specific practiced stimulus new stimuli require relearning scratch this specificity found many different tasks supports hypothesis perceptual learning takes place early visual cortical areas in contrast using novel paradigm motion discrimination learning shown specific found generalization we trained subjects discriminate directions moving dots verified learning transfer trained direction new one however tracking subjects performance across time new direction found rate learning doubled moreover mastering task easy stimulus subjects practiced briefly discriminate easy stimulus new direction generalized difficult stimulus direction this generalization demanded mastering brief practice thus learning motion discrimination always generalizes new stimuli learning manifested various forms acceleration learning rate indirect transfer direct transfer these results challenge existing theories perceptual learning suggest complex picture learning takes place multiple levels learning biological systems great importance but cognitive learning problem solving abrupt generalizes analogous problems appear acquire perceptual skills gradually specifically human subjects generalize perceptual discrimination skill solve similar problems different attributes for example discrimination task described fig subject trained discriminate motion directions ffi ffi use skill discriminate ffi ffi such specificity supports hypothesis perceptual learning embodies neuronal modifications brains stimulusspecific cortical areas eg visual area mt in contrast previous results specificity show three experiments learning motion discrimination always generalizes when task easy generalizes directions training
this paper addresses problem learning evolving concepts concepts whose meaning gradually evolves time solving problem important many applications example building intelligent agents helping users internet search active vision automatically updating knowledgebases acquiring profiles users telecommunication networks requirements learning architecture supporting applications include ability incrementally modify concept definitions accommodate new information fast learning recognition rates low memory needs understandability computercreated concept descriptions to address requirements propose learning architecture based variablevalued logic star methodology aq algorithm the method uses partialmemory approach means step learning system remembers current concept descriptions specially selected representative examples past experience the developed method experimentally applied problem computer system intrusion detection the results show significant advantages method learning speed memory requirements slight decreases predictive accuracy concept simplicity compared traditional batchstyle learning training examples provided
we use simulated evolution search genetic programming automatic synthesis small iterative machinelanguage programs for integer register machine addition instruction sole arithmetic operator show genetic programming produce exact general multiplication routines synthesizing necessary iterative control structures primitive machinelanguage instructions our program representation virtual register machine admits arbitrary control flow our evolution strategy furthermore artificially restrict synthesis control structure place upper bound program evaluation time a programs fitness distance output produced test case desired output multiplication the test cases exhaustively cover multiplication finite subset natural numbers n yet derived solutions constitute general multiplication positive integers for problem simulated evolution twopoint crossover operator examines significantly fewer individuals finding solution random search introduction small rate mutation fur ther increases number solutions
technical report csrp abstract in paper present gpmusic system interactive system allows users evolve short musical sequences using interactive genetic programming extensions aimed making system fully automated the basic gpsystem works using genetic programming algorithm small set functions creating musical sequences user interface allows user rate individual sequences with user interactive technique possible generate pleasant tunes runs individuals generations as user bottleneck interactive systems system takes rating data users run uses train neural network based automatic rater auto rater replace user bigger runs using auto rater able make runs generations individuals per generation the best run pieces generated auto raters pleasant general nice generated user interactive runs
most concept induction algorithms process concept instances described terms properties remain constant time in temporal domains instances best described terms properties whose values vary time data engineering called upon temporal domains transform raw data appropriate form concept induction i investigate method inducing features suitable classifying finite univariate time series governed unknown deterministic processes contaminated noise in supervised setting i induce piecewise polynomials appropriate complexity characterize data class using bayesian model induction principles in study i evaluate proposed method empirically semideterministic domain waveform classification problem originally presented cart book i compared classification accuracy proposed algorithm accuracy attained c various noise levels feature induction improved classification accuracy noisy situations degraded noise the results demonstrate value proposed method presence noise reveal weakness shared classifiers using generative rather discriminative models sensitivity model inaccuracies
in article present casebased approach flexible query answering systems two different application areas the experiencebook supports technical diagnosis field system administration in fallq project use cbr system document retrieval industrial setting the objective systems manage knowledge stored less structured documents the internal case memory implemented case retrieval net this allows handle large case bases efficient retrieval process in order provide multi user access chose client server model combined web interface
dynamic programming provides methodology develop planners controllers nonlinear systems however general dynamic programming computationally intractable we developed procedures allow complex planning control problems solved we use second order local trajectory optimization generate locally optimal plans local models value function derivatives we maintain global consistency local models value function guaranteeing locally optimal plans actually globally optimal resolution search procedures
this paper discusses three techniques useful relaxing constraints imposed control flow parallelism control dependence analysis executing multiple flows control simultaneously speculative execution we evaluate techniques using trace simulations find limits parallelism machines employ different combinations techniques we three major results first local regions code limited parallelism control dependence analysis useful extracting global parallelism different parts program second superscalar processor fundamentally limited execute independent regions code concurrently higher performance obtained machines multiprocessors dataflow machines simultaneously follow multiple flows control finally without speculative execution allow instructions execute control dependences resolved modest amounts parallelism obtained programs complex control flow
this paper presents general framework learning searchcontrol heuristics logic programs used improve efficiency accuracy knowledgebased systems expressed definiteclause logic programs the approach combines techniques explanationbased learning recent advances inductive logic programming learn clauseselection heuristics guide program execution two specific applications framework detailed dynamic optimization prolog programs improving efficiency natural language acquisition improving accuracy in area program optimization prototype system dolphin able transform intractable specifications polynomialtime algorithms outperforms competing approaches several benchmark speedup domains a prototype language acquisition system chill also described it capable automatically acquiring semantic grammars uniformly incorprate syntactic semantic constraints parse sentences caserole representations initial experiments show approach able construct accurate parsers generalize well novel sentences significantly outperform previous approaches learning caserole mapping based connectionist techniques planned extensions general framework specific applications well plans evaluation also discussed
we study online generalized linear regression multidimensional outputs ie neural networks multiple output nodes hidden nodes we allow final layer transfer functions softmax function need consider linear activations output neurons we use distance functions certain kind two completely independent roles deriving analyzing online learning algorithms tasks we use one distance function define matching loss function possibly multidimensional transfer function allows us generalize earlier results onedimensional multidimensional outputs we use another distance function tool measuring progress made online updates this shows previously studied algorithms gradient descent exponentiated gradient fit common framework we evaluate performance algorithms using relative loss bounds compare loss online algoritm best offline predictor relevant model class thus completely eliminating probabilistic assumptions data
systems automated design optimization complex realworld objects principle constructed combining domainindependent numerical routines existing domainspecific analysis simulation programs unfortunately legacy analysis codes frequently unsuitable use automated design they may crash large classes input numerically unstable locally nonsmooth highly sensitive control parameters to useful analysis programs must modified reduce eliminate undesired behaviors without altering desired computation to direct modification programs laborintensive necessitates costly revalidation we implemented highlevel language runtime environment allow failurehandling strategies incorporated existing fortran c analysis programs preserving computational integrity our approach relies globally managing execution programs level discretely callable functions computation affected problems detected problem handling procedures constructed knowledge base generic problem management strategies we show approach effective improving analysis program robustness design optimization performance domain conceptual design jet engine nozzles
in paper study sample complexity weak learning that ask much data must collected unknown distribution order extract small significant advantage prediction we show important distinguish learning algorithms output deterministic hypotheses output randomized hypotheses we prove weak learning model algorithm using deterministic hypotheses weakly learn class vapnikchervonenkis dimension dn requires dn examples in contrast randomized hypotheses allowed show fi examples suffice cases we show exists efficient algorithm using deterministic hypotheses weakly learns distribution set size dn odn examples thus class symmetric boolean functions n variables strong learning sample complexity fin sample complexity weak learning using deterministic hypotheses n on sample complexity weak learning using randomized hypotheses fi next prove existence classes distributionfree sample size required obtain slight advantage prediction random guessing essentially equal required obtain arbitrary accuracy finally class small circuits namely parity functions subsets n boolean variables prove weak learning sample complexity fin this bound holds even weak learning algorithm allowed replace random sampling membership queries target distribution uniform f g n p
convergence results em approach abstract the expectationmaximization em algorithm iterative approach maximum likelihood parameter estimation jordan jacobs recently proposed em algorithm mixture experts architecture jacobs jordan nowlan hinton hierarchical mixture experts architecture jordan jacobs they showed empirically em algorithm architectures yields significantly faster convergence gradient ascent in current paper provide theoretical analysis algorithm we show algorithm regarded variable metric algorithm searching direction positive projection gradient log likelihood we also analyze convergence algorithm provide explicit expression convergence rate in addition describe acceleration technique yields significant speedup simulation experiments this report describes research done dept brain cognitive sciences center biological computational learning artificial intelligence laboratory massachusetts institute technology support cbcl provided part grant nsf asc support laboratorys artificial intelligence research provided part advanced research projects agency dept defense the authors supported grant mcdonnellpew foundation grant atr human information processing research laboratories grant siemens corporation grant iri national science foundation grant nj office naval research nsf grant ecs support initiative intelligent control mit michael i jordan nsf presidential young investigator
an agent must learn act world trial error faces reinforcement learning problem quite different standard concept learning although good algorithms exist problem general case often quite inefficient exhibit generalization one strategy find restricted classes action policies learned efficiently this paper pursues strategy developing algorithm performans online search space action mappings expressed boolean formulae the algorithm compared existing methods empirical trials shown good performance
the aim paper describe adapter system diagnostic architecture combining casebased reasoning abductive reasoning exploiting adaptation solution old episodes order focus reasoning process domain knowledge represented via logical model basic mechanisms based abductive reasoning consistency constraints defined solving complex diagnostic problems involving multiple faults the modelbased component supplemented case memory adaptation mechanisms developed order make diagnostic system able exploit past experience solving new cases a heuristic function proposed able rank solutions associated retrieved cases respect adaptation effort needed transform solutions possible solutions current case we discuss preliminary experiments showing validity heuristic convenience solving new case adapting retrieved solution rather solving new problem scratch
the commonly used neural network models well suited direct digital implementations node needs perform large number operations floating point values fortunately ability learn examples generalize restricted networks type indeed networks node implements simple boolean function boolean networks designed way exhibit similar properties two algorithms generate boolean networks examples presented the results show algorithms generalize well class problems accept compact boolean network descriptions the techniques described general applied tasks known characteristic two examples applications presented image reconstruction handwritten character recognition
this paper explores issues involved implementing robot learning challenging dynamic task using case study robot juggling we use memorybased local model ing approach locally weighted regression represent learned model task performed statistical tests given examine uncertainty model optimize pre diction quality deal noisy corrupted data we develop exploration algorithm explicitly deals prediction accuracy requirements dur ing explo ration using ingredients combination methods optimal control robot achieves fast real time learning task within trials address authors massachusetts institute technology the artificial intelligence laboratory the department brain cognitive sciences technology square cambride ma usa email sschaalaimitedu cgaaimitedu support provided air force office sci entific research siemens cor pora tion support first author provided ger man scholar ship foundation alexander von hum boldt founda tion support second author provided na tional sci ence foundation pre sidential young investigator award we thank gideon stein im ple ment ing first version lwr microprocessor gerrie van zyl build ing devil stick robot implementing first version devil stick learning
genetic algorithms extensively used different domains means global optimization simple yet reliable manner however realistic engineering design optimization domains observed simple classical implementation ga based binary encoding bit mutation crossover sometimes inefficient unable reach global optimum using floating point representation alone eliminate problem in paper describe way augmenting ga new operators strategies take advantage structure properties engineering design domains empirical results initially domain conceptual design supersonic transport aircraft domain high performance supersonic missile inlet design demonstrate newly formulated ga significantly better classical ga terms efficiency reliability httpwwwcsrutgersedushehatapapershtml
consider estimating mean vector data n n i l q norm loss q known lie ndimensional l p ball p for large n ratio minimax linear risk minimax risk arbitrarily large p lt q obvious exceptions aside limiting ratio equals p q our arguments mostly indirect involving reduction univariate bayes minimax problem when p lt q simple nonlinear coordinatewise threshold rules asymptotically minimax small signaltonoise ratios within bounded factor asymptotic minimaxity general our results basic theory estimation besov spaces
searching objects scenes natural task people extensively studied psychologists in paper examine task connectionist perspective computational complexity arguments suggest parallel feedforward networks perform task efficiently one difficulty order distinguish target distractors combination features must associated single object often called binding problem requirement presents serious hurdle connectionist models visual processing multiple objects present psychophysical experiments suggest people use covert visual attention get around problem in paper describe psychologically plausible system uses focus attention mechanism locate target objects a strategy combines topdown bottomup information used minimize search time the behavior resulting system matches reaction time behavior people several interesting tasks
we present algorithm inducing recursive clauses using inverse implication rather inverse resolution underlying generalization method our approach applies class logic programs similar class primitive recursive functions induction performed using small number positive examples need along resolution path our algorithm implemented system named crustacean locates matched lists generating terms determine pattern decomposition exhibited target recursive clause our theoretical analysis defines class logic programs approach complete described terms characteristic ilp approaches our current implementation considerably faster previously reported we present evidence demonstrating given randomly selected inputs increasing number positive examples increases accuracy reduces number outputs we relate approach similar recent work inducing recursive clauses
the pursuerevader pe game recognized important domain study coevolution robust adaptive behavior protean behavior miller cliff nevertheless potential game largely unrealized due methodological hurdles coevolutionary simulation raised pe versions game optimal solutions isaacs closedended formulations opaque respect solution space lack rigorous metric agent behavior this inability characterize behavior turn obfuscates coevolutionary dynamics we present new formulation pe affords rigorous measure agent behavior system dynamics the game moved twodimensional plane onedimensional bitstring time step evader generates bit pursuer must simultaneously predict because behavior expressed time series employ information theory provide quantitative analysis agent activity further version pe opens vistas onto communicative component pursuit evasion behavior providing openended serial communications channel open world via coevolution results show subtle changes game determine whether openended profoundly affect viability armsrace dynamics
the aim paper understand involved parametric design problem solving in order achieve goal paper identify detail conceptual elements defining parametric design task specification ii illustrate elements interpreted operationalised design process iii formulate generic model parametric design problem solving we redescribe number problem solving methods terms proposed generic model show redescription enables us provide precise account different competence behaviours expressed methods design constructing artifacts this means broadly speaking design process creative sense design process produces new solution opposed selecting solution predefined set while recognizing essential creative elements present design process researchers eg gero restrict use term creative design design applications design elements target artifact selected predefined set for instance designing new car model normally case design innovations included present previous car designs in words always possible characterise process designing new car one components assembled configured predefined set nevertheless large number realworld applications possible assume target artifact going designed terms predefined design elements in scenario design process consists assembling configuring preexisting design elements way satisfies design requirements constraints approximates typically costrelated optimization criterion this class design tasks takes name configuration design stefik in many cases typically problem hand exhibit complex spatial requirements possible solutions adhere common solution template possible simplify configuration design problem even modelling target artifact set parameters characterizing design problem solving process assigning values parameters accordance given design requirements constraints optimization criterion when assumption true particular task say parametric design task the vt application marcus et al yost rothenfluh provides wellknown example parametric design task the aim paper understand involved parametric design problem solving in order achieve goal paper identify detail conceptual elements defining parametric design task specification ii illustrate elements interpreted operationalised design process iii produce generic model parametric design problem solving characterised knowledge level generalizes existing methods parametric design we redescribe number problem solving methods terms question
in paper describe selfadjusting algorithm packet routing reinforcement learning method embedded node network only local information used node keep accurate statistics routing policies lead minimal routing times in simple experiments involving node irregularlyconnected network learning approach proves superior routing based precomputed shortest paths
modern compilers employ sophisticated instruction scheduling techniques shorten number cycles taken execute instruction stream in addition correctness instruction scheduler must also ensure hardware resources oversubscribed cycle for contemporary processor implementation multiple pipelines complex resource usage restrictions easy task the complexity involved reasoning resource hazards one primary factors constrain instruction scheduler performing many aggressive transformations for example ability code motion instruction replacement middle already scheduled block would powerful transformation could performed efficiently we extend technique detecting pipeline resource hazards based finite state automata support efficient implementation transformations essential aggressive instruction scheduling beyond basic blocks although similar code transformations supported schemes reservation tables scheme superior terms space time a global instruction scheduler used techniques implemented ksr compiler
we propose new method variable selection estimation coxs proportional hazards model our proposal minimizes log partial likelihood subject sum absolute values parameters bounded constant because nature constraint tends produce coefficients exactly zero hence gives interpretable models the method variation lasso proposal tibshirani designed linear regression context simulations indicate lasso accurate stepwise selection setting
gmd report abstract many current artificial neural network systems serious limitations concerning accessibility flexibility scaling reliability in order go way removing suggest reflective neural network architecture in architecture modular structure important element the buildingblock elements called minos modules they perform selfobservation inform current level development scope expertise within module a pandemonium system integrates submodules work together handle mapping tasks network complexity limitations attacked way pandemonium problem decomposition paradigm static dynamic unreliability whole pandemonium system effectively eliminated generation interpretation confidence ambiguity measures every moment development system two problem domains used test demonstrate various aspects architecture reliability quality measures defined systems answer part time our system achieves better quality values single networks larger size handwritten digit problem when second third best answers accepted system left error test set better best single net it also shown system elegantly learn handle garbage patterns with parity problem demonstrated complexity problems may decomposed automatically system solving networks size smaller single net required even system find solution parity problem networks small size used reliability remains around our pandemonium architecture gives power flexibility higher levels large hybrid system single net system offering useful information higherlevel feedback loops reliability answers may intelligently traded less reliable important intuitional answers in providing weighted alternatives possible generalizations architecture gives best possible service larger system form part
the demands rapid response complexity many environments make difficult decompose tune coordinate reactive behaviors ensuring consistency we hypothesize complex behaviors decomposed separate behaviors resident separate networks coordinated higher level controller to explore issues implemented neural network architecture reactive component two layer control system simulated race car by varying architecture tested whether decomposing reactivity separate behaviors leads superior overall performance learning convergence based results modified architecture produce race car competitive publicly available solutions
supervised classification problems received considerable attention machine learning community we propose novel genetic algorithm based prototype learning system please class problems given set prototypes possible classes class input instance determined prototype nearest instance we assume ordinal attributes prototypes represented sets featurevalue pairs a genetic algorithm used evolve number prototypes per class positions input space determined corresponding featurevalue pairs comparisons c set artificial problems controlled complexity demonstrate effectiveness pro posed system
this paper compares two methods refining uncertain knowledge bases using propositional certaintyfactor rules the first method implemented rapture system employs neuralnetwork training refine certainties existing rules uses symbolic technique add new rules the second method based one used kbann system initially adds complete set potential new rules low certainty allows neuralnetwork training filter adjust rules experimental results indicate former method results significantly faster training produces much simpler refined rule bases slightly greater accuracy
this paper discusses approach constructing new attributes based decision trees production rules it improve concepts learned form decision trees simplifying improving predictive accuracy in addition approach distinguish relevant primitive attributes irrelevant primitive attributes
performance human subjects wide variety early visual processing tasks improves practice hyperbf networks poggio girosi constitute mathematically wellfounded framework understanding improvement performance perceptual learning class tasks known visual hyperacuity the present article concentrates two issues raised recent psychophysical computational findings reported poggio et al b fahle edelman first develop biologically plausible extension hyperbf model takes account basic features functional architecture early vision second explore various learning modes coexist within hyperbf framework focus two unsupervised learning rules may involved hyperacuity learning finally report results psychophysical experiments consistent hypothesis activitydependent presynaptic amplification may involved perceptual learning hyperacuity
proceedings nd international conference knowledge discovery data mining kdd the official version paper published american association artificial intelligence httpwwwaaaiorg c fl american association artificial intelligence all rights reserved abstract we describe results performing data mining challenging medical diagnosis domain acute abdominal pain this domain well known difficult yielding little predictive accuracy human machine diagnosticians moreover many researchers argue one simplest approaches naive bayesian classifier optimal by comparing performance naive bayesian classifier general cousin bayesian network classifier selective bayesian classifiers total attributes show simplest models perform least well complex models we argue simple models like selective naive bayesian classifier perform well complicated models similarly complex domains relatively small data sets thereby calling question extra expense necessary induce complex models
this report describes statistical research development work hospital quality monitor data sets nationwide va hospital system the project covers statistical analysis exploration modelling data several quality monitors primary goals understanding patterns variability time hospitallevel monitor area specific quality monitor measures b understanding patterns dependencies sets monitors we present discussion basic perspectives data structure preliminary data exploration three monitors followed developments several classes formal models we identify classes hierarchical random effects time series models relevance modelling single multiple monitor time series we summarise basic model features results analyses three monitor data sets single multiple monitor frameworks present variety summary inferences graphical displays our discussion includes summary conclusions related two key goals discussions questions comparisons across hospitals recommendations potential substantive statistical investigations
adaptive ridge special form ridge regression balancing quadratic penalization parameter model this paper shows equivalence adaptive ridge lasso least absolute shrinkage selection operator this equivalence states procedures produce estimate least absolute shrinkage thus viewed particular quadratic penalization from observation derive em algorithm compute lasso solution we finally present series applications type algorithm regres sion problems kernel regression additive modeling neural net training
technical report ncrg available httpwwwncrgastonacuk to appear advances neural information processing systems eds m i jordan m j kearns s a solla lawrence erlbaum abstract gaussian processes provide natural nonparametric prior distributions regression functions in paper consider regression problems noise output variance noise depends inputs if assume noise smooth function inputs natural model noise variance using second gaussian process addition gaussian process governing noisefree output value we show prior uncertainty parameters controlling processes handled posterior distribution noise rate sampled using markov chain monte carlo methods our results synthetic data set give posterior noise variance wellapproximates true variance
technical report no department statistics university toronto abstract simulated annealing moving tractable distribution distribution interest via sequence intermediate distributions traditionally used inexact method handling isolated modes markov chain samplers here shown one use markov chain transitions annealing sequence define importance sampler the markov chain aspect allows method perform acceptably even highdimensional problems finding good importance sampling distributions would otherwise difficult use importance weights ensures estimates found converge correct values number annealing runs increases this annealed importance sampling procedure resembles second half previouslystudied tempered transitions seen generalization recentlyproposed variant sequential importance sampling it also related thermodynamic integration methods estimating ratios normalizing constants annealed importance sampling attractive isolated modes present estimates normalizing constants required may also generally useful since independent sampling allows one bypass problems assessing convergence autocorrelation markov chain samplers
inversion multilayer synchronous networks method tries answer questions like what kind input give desired output is possible get desired output special inputoutput constraints we describe two methods inverting connectionist network firstly extend inversion via backpropagation lindenkindermann williams recurrent elman jordan mozer williamszipser timedelayed waibel al discrete versions continuous networks pineda pearlmutter the result inversion input vector the corresponding output vector equal target vector except small remainder the knowledge attractors may help understand function generalization qualities connectionist systems kind secondly introduce new inversion method proving nonexistence input combination special constraints eg subspace input space this method works iterative exclusion invalid activation values it might helpful way judge properties trained network we conclude simulation results three different tasks xor morse signal decoding handwritten digit recognition
in paper study learning algorithms environments changing time unlike previous work interested case changes might rapid direction relatively constant we model type change assuming target distribution changing continuously constant rate one extreme distribution another we show case use simple weighting scheme estimate error hypothesis using estimate minimize error prediction
chaque parametre du modele est penalise individuellement le reglage de ces penalisations se fait automatiquement partir de la definition dun hyperparametre de regularisation globale cet hyperparametre qui controle la complexite du regresseur peut etre estime par des techniques de reechantillonnage nous montrons experimentalement les performances et la stabilite de la penalisation multiple adaptative dans le cadre de la regression lineaire nous avons choisi des problemes pour lesquels le probleme du controle de la complexite est particulierement crucial comme dans le cadre plus general de lestimation fonctionnelle les comparaisons avec les moindres carres regularises et la selection de variables nous permettent de deduire les conditions dapplication de chaque algorithme de penalisation lors des simulations nous testons egalement plusieurs techniques de reechantillonnage ces techniques sont utilisees pour selectionner la complexite optimale des estimateurs de la fonction de regression nous comparons les pertes occasionnees par chacune dentre elles lors de la selection de modeles sousoptimaux nous regardons egalement si elles permettent de determiner lestimateur de la fonction de regression minimisant lerreur en generalisation parmi les differentes methodes de penalisation en competition
the crossover operator common implementations genetic programming gp another usually unavoidable factor form restriction size trees gp population this paper concentrates interaction crossover operator restriction tree depth demonstrated max problem involves returning largest possible value given function terminal sets
we propose model efficient online reinforcement learning based expected mistake bound framework introduced haussler littlestone warmuth the measure performance use expected difference total reward received learning agent received agent behaving optimally start we call expected difference cumulative mistake agent require levels reasonably fast rate learning progresses we show model polynomially equivalent pac model offline reinforcement learning introduced fiechter in particular show offline pac reinforcement learning algorithm transformed efficient online algorithm simple practical way an immediate consequence result pac algorithm general finite statespace reinforcement learning problem described fiechter transformed polynomial online al gorithm guaranteed performances
we investigate space protein sequences we combine standard measures similarity sw fasta blast associate sequence exhaustive list neighboring sequences these lists induce weighted directed graph whose vertices sequences the weight edge connecting two sequences represents degree similarity this graph encodes much fundamental properties sequence space we look clusters related proteins graph these clusters correspond strongly connected sets vertices two main ideas underlie work interesting homologies among proteins deduced transitivity ii transitivity applied restrictively order prevent unrelated proteins clustering together our analysis starts conservative classification based significant similarities many classes subsequently classes merged include less significant similarities merging performed via novel two phase algorithm first algorithm identifies groups possibly related clusters based transitivity strong connectivity using local considerations merges then global test applied identify nuclei strong relationships within groups clusters classification refined accordingly this process takes place varying thresholds statistical significance step algorithm applied classes previous classification obtain next one permissive threshold consequently hierarchical organization proteins obtained the resulting classification splits space protein sequences well defined groups proteins the results show automatically induced sets proteins closely correlated natural biological families super families the hierarchical organization reveals finer subfamilies make known families proteins well many interesting relations protein families the hierarchical organization proposed may considered first map space protein sequences an interactive web site including results analysis constructed accessible httpwwwprotomapcshujiacil
this paper presents system why learns updates diagnostic knowledge base using domain knowledge set examples the apriori knowledge consists causal model domain stating relationships among basic phenomena body phenomenological theory describing links abstract concepts possible manifestations world the phenomenological knowledge used deductively causal model used abductively examples used inductively the problems imperfection intractability theory handled allowing system make assumptions reasoning in way robust knowledge learned limited complexity limited number examples the system works first order logic environment applied real domain
this paper investigates behaviour random walk metropolis algorithm high dimensional problems here concentrate case components target density spatially homogeneous gibbs distribution finite range the performance algorithm strongly linked presence absence phase transition gibbs distribution convergence time approximately linear dimension problems phase transition present related optimal way scale variance proposal distribution order maximise speed convergence algorithm this turns involve scaling variance proposal reciprocal dimension least phase transition free case moreover actual optimal scaling characterised terms overall acceptance rate algorithm maximising value value predicted studies simpler classes target density the results proved framework weak convergence result shows algorithm actually behaves like infinite dimensional diffusion process high dimensions introduction discussion results
this paper develops probabilistic bounds outofsample error rates several classifiers using single set insample data the bounds based probabilities partitions union insample outofsample data insample outofsample data sets the bounds apply insample outofsample data drawn distribution partitionbased bounds stronger vctype bounds require computation
we present framework learning dfa simple examples we show efficient pac learning dfa possible class distributions restricted simple distributions teacher might choose examples based knowledge target concept this answers open research question posed pitts seminal paper are dfas pacidentifiable examples drawn uniform distribution known simple distribution our approach uses rpni algorithm learning dfa labeled examples in particular describe efficient learning algorithm exact learning target dfa high probability bound number states n target dfa known advance when n known show algorithm used efficient pac learning dfas
the simple synchrony network ssn new connectionist architecture incorporating insights temporal synchrony variable binding tsvb simple recurrent networks the use tsvb means ssns output representations structures learn generalisations constituents structures required systematicity this paper describes ssn associated training algorithm demonstrates ssns generalisation abilities results training
this paper deals combination evolutionary algorithms artificial neural networks ann a new method presented find good buildingblocks architectures artificial neural networks the method based cellular encoding representation scheme f gruau genetic programming j koza first shown modified cellular encoding technique able find good architectures even nonboolean networks with help graphdatabase new graphrewriting method secondly possible build architectures modular structures the information buildingblocks architectures obtained statistically analyzing data graphdatabase simulation results two real world problems given
we attempted obtain stronger correlation relationship g g performance this included studying variance fitnesses members population well observing rate convergence gp respect g population evolved g unfortunately yet able obtain significant correlation in future work plan track genetic diversity considered phenotypic variance far populations order shed light underlying mechanism priming one factor made analysis difficult far use genetic programming space genotypes large ie many redundant solutions neighborhood structure less easily intuited standard genetic algorithm since every reason believe underlying mechanism incremental evolution largely independent peculiarities genetic programming currently investigating incremental evolution mechanism using genetic algorithms fixedlength genotypes this enable better understanding mechanism ultimately scale research effort analyze incremental evolution one transition test cases this involve many open issues regarding optimization transition schedule test cases we performed following experiment let f iti g fitness value genetic program i according evaluation function g best ofp op g member i fl population p op time highest fitness according g words i fl best of p op g maximizes f iti g i p op a population p op evolved usual manner using evaluation function g generations however generation also evaluated current population using evaluation function g recorded value f itbest of p op g g in words evolved population using g evaluation function every generation also computed fitness best individual population according g saved value using random seed control parameters evolved population p op generations using g evaluation function note generation p op identical p op for values compared f itbest of p op g g f itbest of p op g g order better formalize exploit notion domain difficulty
genetic programming computationally expensive for applications vast majority time spent evaluating candidate solutions desirable make individual evaluation efficient possible we describe genome compiler compiles sexpressions machine code resulting significant speedup individual evaluations standard gp systems based performance results symbolic regression show execution genome compiler system comparable fastest alternative gp systems we also demonstrate utility compilation realworld problem lossless image compression a somewhat surprising result test domains overhead compilation negligible
the crossover operator common implementations genetic programming gp another usually unavoidable factor form restriction size trees gp population this paper concentrates interaction crossover operator restriction tree depth demonstrated max problem involves returning largest possible value given function terminal sets some characteristics inadequacies crossover normal use highlighted discussed subtree discovery movement takes place mostly near leaf nodes nodes near root left untouched diversity drops quickly zero near root node tree population gp unable create fitter trees via crossover operator leaving mutation operator common ineffective route discovery fitter trees
design rationale record design activity alternatives available choices made reasons explanations proposed design intended work we describe representation called functional representation fr used represent devices functions arise causally functions components interconnections we propose fr provide basis capturing causal aspects design rationale we briefly discuss use fr number tasks would expect design rationale useful generation diagnostic knowledge design verification redesign
we present neural networkbased face detection system a retinally connected neural network examines small windows image decides whether window contains face the system arbitrates multiple networks improve performance single network we use bootstrap algorithm training networks adds false detections training set training progresses this eliminates difficult task manually selecting nonface training examples must chosen span entire space nonface images comparisons stateoftheart face detection systems presented system better performance terms detection falsepositive rates this work partially supported grant siemens corporate research inc department army army research office grant number daahg office naval research grant number n this work started shumeet baluja supported national science foundation graduate fellowship he currently supported graduate student fellowship national aeronautics space administration administered lyndon b johnson space center the views conclusions contained document authors interpreted necessarily representing official policies endorsements either expressed implied sponsoring agencies
